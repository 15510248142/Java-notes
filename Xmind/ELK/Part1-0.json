






#查看插件
bin/elasticsearch-plugin list
#查看安装的插件
GET http://localhost:9200/_cat/plugins?v


--------

#查看集群
GET http://localhost:9200
#查看nodes
GET _cat/nodes
GET _cluster/health


----------


## 基本概念(1)- 索引，文档和 REST API

# 查看索引相关信息
# 索引是文档的容器 
# index体现了逻辑空间的概念 每个索引都有自己的Mapping定义，用于定义包含的文档的字段名和字段类型
# setting体现了物理空间的概念 索引中的数据分散在Shard上
GET kibana_sample_data_ecommerce

#查看索引的文档总数
GET kibana_sample_data_ecommerce/_count

#查看文档，了解文档格式
POST kibana_sample_data_ecommerce/_search
{
}

#_cat indices API
#查看indices
GET /_cat/indices/kibana*?v&s=index

#查看状态为绿的索引
GET /_cat/indices?v&health=green

#按照文档个数排序
GET /_cat/indices?v&s=docs.count:desc

#查看具体的字段（指定输出的列）
GET /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt

#How much memory is used per index?s:(Sort)
GET /_cat/indices?v&h=i,tm&s=tm:desc
GET /_cat/indices?v&h=i,tm&s

# 1.（无）开头表示当前目录下的
# 2.（/）开头的目录表示该目录为根目录的一个子目录



--------


## 基本概念（2）：节点，集群，分片及副本

# 获取节点
GET _cat/nodes?v
GET /_nodes/es7_01,es7_02
GET /_cat/nodes?v
GET /_cat/nodes?v&h=id,ip,port,v,m

# 获取分片
GET _cat/shards

GET _cat/shards?h=index,shard,prirep,state,unassigned.reason


# 集群健康状态
GET _cluster/health
GET _cluster/health?level=shards
GET /_cluster/health/kibana_sample_data_ecommerce
GET /_cluster/health/kibana_sample_data_ecommerce?level=shards

# 集群状态
GET /_cluster/state

# 获取集群settings
GET /_cluster/settings
GET /_cluster/settings?include_defaults=true


--------------

# index 不存在就索引新的文档 存在则覆盖 并且版本号+1
PUT users/_doc/1
# create 指定id 若存在则失败  
PUT users/_create/1
# create 不指定id 自动生成
POST users/_doc/1
# read 找到文档200 找不到404
GET users/_doc/1
# update 不会删除原来的文档 实现真正的数据更新
POST users/_update/1


## 文档的基本 CRUD 与批量操作

############Create Document############
#create document. 自动生成 _id
POST users/_doc
{
	"user" : "Mike",
    "post_date" : "2019-04-15T14:12:12",
    "message" : "trying out Kibana"
}

#create document. 指定Id。如果id已经存在，报错
PUT users/_doc/1?op_tye=crpeate
{
  "user": "Jack",
  "post_date": "2019-05-15T14:12:12",
  "message": "trying out Elasticsearch"
}

#create document. 指定 ID 如果已经存在，就报错
PUT users/_create/1
{
    "user" : "Jack",
    "post_date" : "2019-05-15T14:12:12",
    "message" : "trying out Elasticsearch"
}





###  Index & Update
#Update 指定 ID  (先删除，在写入)
GET users/_doc/1

PUT users/_doc/1
{
	"user" : "Mike"
}

#在原文档上增加字段 增加的字段要包含在doc中
POST users/_update/1/
{
    "doc":{
        "post_date" : "2019-05-15T14:12:12",
        "message" : "trying out Elasticsearch"
    }
}



### Delete by Id
# 删除文档
DELETE users/_doc/1



### Bulk 操作 支持在一次API调用中，对不同的索引进行操作 支持4种类型：index,create,update,delete 可以在url种指定index 也可以在请求的payload中指定
# （1）index 和 create  第二行是source数据体
# （2）delete 没有第二行
# （3）update 第二行可以是partial doc，upsert或者是script

#执行两次，查看每次的结果


#执行第1次
POST _bulk
{"index":{"_index":"test","_id":"1"}}
{"field1":"value1"}
{"delete":{"_index":"test","_id":"2"}}
{"create":{"_index":"test2","_id":"3"}}
{"field1":"value3"}
{"update":{"_index":"test"},"_id":"1"}
{"doc":{"field2":"value2"}}


#执行第2次
POST _bulk
{ "index" : { "_index" : "test", "_id" : "1" } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_id" : "2" } }
{ "create" : { "_index" : "test2", "_id" : "3" } }
{ "field1" : "value3" }
{ "update" : {"_id" : "1", "_index" : "test"} }
{ "doc" : {"field2" : "value2"} }



### mget 批量获取操作
GET /_mget
{
  "docs": [ 
    {
      "_index": "test",
      "_id": "1"
    },
    {
      "_index": "test",
      "_id": "2"
    }
  ]
}


#URI中指定index
GET /test/_mget
{
  "docs": [
    {
      "_id": "1"
    },
    {
      "_id": "2"
    }
  ]
}


GET /_mget
{
  "docs": [
    {
      "_index": "test",
      "_id": "1",
      "_source": false
    },
    {
      "_index": "test",
      "_id": "2",
      "_source": [
        "field3",
        "field4"
      ]
    },
    {
      "_index": "test",
      "_id": "3",
      "_source": {
        "include": [
          "user"
        ],
        "exclude": [
          "user.location"
        ]
      }
    }
  ]
}


### msearch 批量查询操作
POST kibana_sample_data_ecommerce/_msearch
{}
{"query" : {"match_all" : {}},"size":1}
{"index" : "kibana_sample_data_flights"}
{"query" : {"match_all" : {}},"size":2}


### 清除测试数据
#清除数据
DELETE users
DELETE test
DELETE test2


-----------


## 倒排索引

# 正排索引：文档id到文档内容和单词的关联
# 倒排索引：单词到文档id的关联                            
 包含两个部分：        
 	单词词典：记录单词到倒排列表的关联关系          
 		单词词典一般比较大 使用b+树或哈希拉链法实现        
 	倒排列表：记录了单词到对应文档的集合，由倒排索引项组成           
  		文档id，词频TF，位置，偏移


POST _analyze
{
  "analyzer": "standard",
  "text": "Mastering Elasticsearch"
}

POST _analyze
{
  "analyzer": "standard",
  "text": "Elasticsearch Server"
}

POST _analyze
{
  "analyzer": "standard",
  "text": "Elasticsearch Essentials"
}



---------

## 使用分析器进行分词

# Analysis也叫分词，就是将文本转换成一系列单词的过程
# Analysis是通过Analyzer来实现的
# 除了在数据写入时转换词条，匹配Query语句时候也需要用相同的分析器对查询语句进行分析

# Analyzer的组成
	分词器是专门处理分词的组件，由三部分组成：
		Character Filter： 针对原始文本处理，例如去除html
		Tokenizer： 按照规则切分为单词
		Token Filter : 将切分的单词进行加工，小写，删除stopwords，增加同义词

# ES的内置分词器
	#standard Analyzer - 默认分词器，按词切分，小写处理
	#Simple Analyzer – 按照非字母切分（符号被过滤），小写处理
	#Stop Analyzer – 小写处理，停用词过滤（the，a，is）
	#Whitespace Analyzer – 按照空格切分，不转小写
	#Keyword Analyzer – 不分词，直接将输入当作输出
	#Patter Analyzer – 正则表达式，默认 \W+ (非字符分隔)
	#Language – 提供了30多种常见语言的分词器
	#Cusromer Analyzer - 自定义分词器
#2 running Quick brown-foxes leap over lazy dogs in the summer evening

#查看不同的analyzer的效果
#standard
GET _analyze
{
  "analyzer": "standard",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}

#simpe
GET _analyze
{
  "analyzer": "simple",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}


GET _analyze
{
  "analyzer": "stop",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}


#stop
GET _analyze
{
  "analyzer": "whitespace",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}

#keyword
GET _analyze
{
  "analyzer": "keyword",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}

GET _analyze
{
  "analyzer": "pattern",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}


#english
GET _analyze
{
  "analyzer": "english",
  "text": "2 running Quick brown-foxes leap over lazy dogs in the summer evening."
}


POST _analyze
{
  "analyzer": "icu_analyzer",
  "text": "他说的确实在理”"
}


POST _analyze
{
  "analyzer": "standard",
  "text": "他说的确实在理”"
}


POST _analyze
{
  "analyzer": "icu_analyzer",
  "text": "这个苹果不大好吃"
}





-----------

## URI Search 概览


#URI Query
# q用来表示查询内容 KV
# 键值对
GET kibana_sample_data_ecommerce/_search?q=customer_first_name:Eddie
GET kibana*/_search?q=customer_first_name:Eddie
GET /_all/_search?q=customer_first_name:Eddie


#REQUEST Body
POST kibana_sample_data_ecommerce/_search
{
	"profile": true,
	"query": {
		"match_all": {}
	}
}

# Response
	took：花费的时间
	total：符合条件的总数
	hints：结果集，默认前十个文档
	_index：索引名
	_id：文档的id
	_score：相关度评分
	_source：文档原始信息

# 搜索的相关性
	- 是否可以找到所有相关的内容
	- 有多少不相关的内容被返回
	- 文档的打分是否合理
	- 结合业务去修，平衡结果排名

# 衡量相关性
	Information Retrieval
		Precision（查准率）：尽可能返回较少的无关文档	True Positive/全部返回的结果（True Positives and false Positives）
		Recall（查全率）：尽量返回较多的相关文档	True Positive/所有应该返回的结果（True Positives and false Negtives）
		Ranking：是否能够按照相关度进行排序



--------



## URL Search详解


#基本查询
# q指定查询语句 df默认字段，不指定时 sort排序 from/size分页 profile可以查看查询是如何被执行的
GET /movies/_search?q=2012&df=title&sort=year:desc&from=0&size=10&timeout=1s

#带profile
GET /movies/_search?q=2012&df=title
{
	"profile":"true"
}

# 泛字段q=2012 指定查询q=title:2012

#泛查询，正对_all,所有字段
GET /movies/_search?q=2012
{
	"profile":"true"
}

#指定字段
GET /movies/_search?q=title:2012&sort=year:desc&from=0&size=10&timeout=1s
{
	"profile":"true"
}


# 查找美丽心灵, Mind为泛查询
# 等效于Beautiful OR Mind
GET /movies/_search?q=title:Beautiful Mind
{
	"profile":"true"
}


#使用引号，Phrase查询
# 等效于Beautiful AND Mind
GET /movies/_search?q=title:"Beautiful Mind"
{
	"profile":"true"
}

#分组，Bool查询
# +表示must -表示must_not
GET /movies/_search?q=title:(+Beautiful -Mind)
{
	"profile":"true"
}


#布尔操作符
# 查找美丽心灵
GET /movies/_search?q=title:(Beautiful AND Mind)
{
	"profile":"true"
}

# 查找美丽心灵
GET /movies/_search?q=title:(Beautiful NOT Mind)
{
	"profile":"true"
}

# 查找美丽心灵
GET /movies/_search?q=title:(Beautiful %2BMind)
{
	"profile":"true"
}


/*

#范围查询 ,区间写法
# [] 闭区间 {}开区间
GET /movies/_search?q=title:beautiful AND year:[2002 TO 2018%7D
{
	"profile":"true"
}

# 算数符号
# year:>2010
# year:(>2010 && <=2018)
# year:(+>2010 +<=2018)


#通配符查询
# ? 一个字符 * 0或多个字符
GET /movies/_search?q=title:b*
{
	"profile":"true"
}

#模糊匹配
GET /movies/_search?q=title:beautifl~1
{
	"profile":"true"
}

#近似度匹配
GET /movies/_search?q=title:"Lord Rings"~2
{
	"profile":"true"
}

*/



--------------------


## Request Body 与 Query DSL
# 将查询语句通过HTTP Request Body 发送给ES

# Query DSL
#ignore_unavailable=true，可以忽略尝试访问不存在的索引“404_idx”导致的报错
#查询movies分页
POST /movies,404_idx/_search?ignore_unavailable=true
{
  "profile": true,
	"query": {
		"match_all": {}
	}
}

POST /kibana_sample_data_ecommerce/_search
{
  "from":10,
  "size":20,
  "query":{
    "match_all": {}
  }
}


#对日期排序
# 最好在日期型或数字型字段上排序
POST kibana_sample_data_ecommerce/_search
{
  "sort":[{"order_date":"desc"}],
  "query":{
    "match_all": {}
  }

}

#source filtering
# 如果_source没有存储，那就只返回匹配的文档的元数据
# _source支持使用通配符 _source["name*","desc*"]
POST kibana_sample_data_ecommerce/_search
{
  "_source":["order_date"],
  "query":{
    "match_all": {}
  }
}


#脚本字段
# 用例：订单中有不同的汇率，需要结合汇率对，订单价格进行排序
GET kibana_sample_data_ecommerce/_search
{
  "script_fields": {
    "new_field": {
      "script": {
        "lang": "painless",
        "source": "doc['order_date'].value+'hello'"
      }
    }
  },
  "query": {
    "match_all": {}
  }
}



# 使用查询表达式 Match
# 会匹配以上所有文档，但是它无法告诉我们这两个词是否表达了部分原文的部分意义，或者是表达了完整的意义
POST movies/_search
{
  "query": {
    "match": {
      "title": "last christmas"
    }
  }
}

POST movies/_search
{
  "query": {
    "match": {
      "title": {
        "query": "last christmas",
        "operator": "and"
      }
    }
  }
}



# 短语搜索 Match Phrase
# 真实的文档也许比上面几个例子要长的多：last和christmas也许相隔了几个段落。也许我们仍然希望包含这样的文档，
# 但是我们会给那些last和christmas出现的较近的文档更高的相关度分值。
# 这就是短语匹配(Phrase Matching)，或者邻近度匹配(Proximity Matching)。
# match_phrase查询首先解析查询字符串来产生一个词条列表。
# 然后会搜索所有的词条，但只保留含有了所有搜索词条的文档，并且词条的位置要邻接。
	one和love必须全部出现在某个字段中。
	love的位置必须比one的位置大1。

POST movies/_search
{
  "query": {
    "match_phrase": {
      "title":{
        "query": "one love"

      }
    }
  }
}


# 精确短语(Exact-phrase)匹配也许太过于严格了。也许我们希望含有"quick brown fox"的文档也能够匹配"quick fox"查询，即使位置并不是完全相等的。
# 我们可以在短语匹配使用slop参数来引入一些灵活性 slop参数告诉match_phrase查询词条能够相隔多远时仍然将文档视为匹配

POST movies/_search
{
  "query": {
    "match_phrase": {
      "title":{
        "query": "one love",
        "slop": 1
      }
    }
  }
}




--------


## Query & Simple Query String Query
	类似URL Query



PUT /users/_doc/1
{
  "name":"Ruan Yiming",
  "about":"java, golang, node, swift, elasticsearch"
}

PUT /users/_doc/2
{
  "name":"Li Yiming",
  "about":"Hadoop"
}


POST users/_search
{
  "query": {
    "query_string": {
      "default_field": "name",
      "query": "Ruan AND Yiming"
    }
  }
}


POST users/_search
{
  "query": {
    "query_string": {
      "fields":["name","about"],
      "query": "(Ruan AND Yiming) OR (Java AND Elasticsearch)"
    }
  }
}


#Simple Query 默认的operator是 Or
	 类似Query String ，但是会忽略错误的语法，同时只支持部分查询语法
	 不支持AND OR NOT ，会当作字符串处理
	 Term之间默认的关系是OR，可以指定Operator
	 支持部分逻辑 +替代AND |替代OR -替代NOT

POST users/_search
{
  "query": {
    "simple_query_string": {
      "query": "Ruan AND Yiming",
      "fields": ["name"]
    }
  }
}


POST users/_search
{
  "query": {
    "simple_query_string": {
      "query": "Ruan Yiming",
      "fields": ["name"],
      "default_operator": "AND"
    }
  }
}


GET /movies/_search
{
	"profile": true,
	"query":{
		"query_string":{
			"default_field": "title",
			"query": "Beafiful AND Mind"
		}
	}
}


# 多fields
GET /movies/_search
{
	"profile": true,
	"query":{
		"query_string":{
			"fields":[
				"title",
				"year"
			],
			"query": "2012"
		}
	}
}



GET /movies/_search
{
	"profile":true,
	"query":{
		"simple_query_string":{
			"query":"Beautiful +mind",
			"fields":["title"]
		}
	}
}







-----------



## Dynamic Mapping 和常见字段类型
	
Mapping类似数据库中的schema的定义
Mpaping会把JSON文档映射为Lucene所需要的扁平格式
一个Mapping属于一个索引的Type
	每个文档都属于一个Type
	一个Type有一个Mapping定义
	7.0开始，不需要在Mapping定义中指定type信息
Mapping中的字段一旦设定后，禁止直接修改。因为倒排索引生成后不允许直接修改。需要重新建立新的索引，做reindex操作。
类似数据库中的表结构定义，主要作用
	- 定义字段名字
	- 定义字段的类型
		简单类型
			Text/Keyword
			Date
			Interger/Floating
			Boolean
		复杂类型
			对象类型/嵌套类型
		特殊类型
			geo_point & geo_shape /percolator
	- 定义倒排索引相关的配置（是否被索引？采用的Analyzer）

对新增字段的处理
	true 一旦有新增字段的加入，Mapping也同时被更新
	false Mapping不会被更新，新增字段的数据无法被索引，但信息会出现在_source中
	strict 文档写入失败
对已有字段，一旦已经有数据写入，就不再支持修改字段定义
	Lucene实现的倒排索引，一旦生成后，就不允许修改



在object下，支持做dynamic的属性的定义

Dynamic Mappring
	在写入文档时候，如果索引不存在，会自动创建索引
	无需手动定义Mappings，es会自动根据文档信息，推算出字段的类型
	当类型如果设置不对时，会导致一些功能无法正常运行，例如Range查询

类型的自动识别
	字符串 
		匹配日期格式Date 
		配置数字设置为float或者long，该选项默认关闭
		设置为Text，并且增加keyword字字段
	布尔值 boolean
	浮点数 float
	整数 long
	对象 Object
	数组 由第一个非空数值的类型所决定
	空置 忽略


#写入文档，查看 Mapping
PUT mapping_test/_doc/1
{
  "firstName":"Chan",
  "lastName": "Jackie",
  "loginDate":"2018-07-24T10:29:48.103Z"
}

#查看 Mapping文件
GET mapping_test/_mapping


#Delete index
DELETE mapping_test

#dynamic mapping，推断字段的类型
PUT mapping_test/_doc/1
{
    "uid" : "123",
    "isVip" : false,
    "isAdmin": "true",
    "age":19,
    "heigh":180
}

#查看 Dynamic
GET mapping_test/_mapping


#默认Mapping支持dynamic，写入的文档中加入新的字段
PUT dynamic_mapping_test/_doc/1
{
  "newField":"someValue"
}

#该字段可以被搜索，数据也在_source中出现
POST dynamic_mapping_test/_search
{
  "query":{
    "match":{
      "newField":"someValue"
    }
  }
}


#修改为dynamic false
PUT dynamic_mapping_test/_mapping
{
  "dynamic": false
}

#新增 anotherField
PUT dynamic_mapping_test/_doc/10
{
  "anotherField":"someValue"
}


#该字段不可以被搜索，因为dynamic已经被设置为false
POST dynamic_mapping_test/_search
{
  "query":{
    "match":{
      "anotherField":"someValue"
    }
  }
}

get dynamic_mapping_test/_doc/10


#修改为strict
PUT dynamic_mapping_test/_mapping
{
  "dynamic": "strict"
}



#写入数据出错，HTTP Code 400
PUT dynamic_mapping_test/_doc/12
{
  "lastField":"value"
}

DELETE dynamic_mapping_test



--------------


## 显式Mapping设置与常见参数介绍
依照以下步骤：
	创建一个临时的index，写入一些样本数据
	通过访问Mapping API获得该临时文件的动态Mapping定义
	修改后使用该配置创建你的索引
	删除临时索引



#设置 index 为 false 该字段不可被搜索
DELETE users
PUT users
{
    "mappings" : {
      "properties" : {
        "firstName" : {
          "type" : "text"
        },
        "lastName" : {
          "type" : "text"
        },
        "mobile" : {
          "type" : "text",
          "index": false
        }
      }
    }
}

PUT users/_doc/1
{
  "firstName":"Ruan",
  "lastName": "Yiming",
  "mobile": "12345678"
}

POST /users/_search
{
  "query": {
    "match": {
      "mobile":"12345678"
    }
  }
}





#设定Null_value 需要对Null值实现搜索
	只有keyword类型支持设定Null_value
DELETE users
PUT users
{
    "mappings" : {
      "properties" : {
        "firstName" : {
          "type" : "text"
        },
        "lastName" : {
          "type" : "text"
        },
        "mobile" : {
          "type" : "keyword",
          "null_value": "NULL"
        }

      }
    }
}

PUT users/_doc/1
{
  "firstName":"Ruan",
  "lastName": "Yiming",
  "mobile": null
}


PUT users/_doc/2
{
  "firstName":"Ruan2",
  "lastName": "Yiming2"

}

GET users/_search
{
  "query": {
    "match": {
      "mobile":"NULL"
    }
  }

}



#设置 Copy to 
	_all在7中被copy_to取带
	满足一些特定的搜索需求
	copy_to将字段的数值拷贝到目标字段，实现类似_all的作用
	copy_to的目标字段不出现在_source中

DELETE users
PUT users
{
  "mappings": {
    "properties": {
      "firstName":{
        "type": "text",
        "copy_to": "fullName"
      },
      "lastName":{
        "type": "text",
        "copy_to": "fullName"
      }
    }
  }
}
PUT users/_doc/1
{
  "firstName":"Ruan",
  "lastName": "Yiming"
}

GET users/_search?q=fullName:(Ruan Yiming)

POST users/_search
{
  "query": {
    "match": {
       "fullName":{
        "query": "Ruan Yiming",
        "operator": "and"
      }
    }
  }
}



#数组类型
	es中不提供专门的数组类型。但是任何字段，都可以包含多个相同类型的数值

PUT users/_doc/1
{
  "name":"onebird",
  "interests":"reading"
}

PUT users/_doc/1
{
  "name":"twobirds",
  "interests":["reading","music"]
}

POST users/_search
{
  "query": {
		"match_all": {}
	}
}

GET users/_mapping




------------------



## 多字段特性及Mapping中配置自定义Analyzer
	



多字段特性：
	厂商实现精确匹配
		增加一个keyword字段
使用不同的analyzer
	不同语言
	pinyin字段的搜索
	还支持为搜索和索引指定不同的analyzer
PUT products
{
  "mappings": {
    "properties": {
      "company": {
        "type": "text",
        "fields": {
          "keyword": {
            "type": "keyword",
            "ignore_above": 256
          }
        }
      },
      "comment": {
        "type": "text",
        "fields": {
          "english_comment": {
            "type": "text",
            "analyzer": "english",
            "search_analyzer": "english"
          }
        }
      }
    }
  }
}


Exact Values
	包括数字/日期/具体一个字符串
	es中的keyword
Full Text
	全文本，非结构化的文本数据
	es中的text



# Exact Values不需要被分词
	es为每一个字段创建一个倒排索引
		Exact Values在索引时，不需要做特殊的分词处理
PUT logs/_doc/1
{"level":"DEBUG"}

GET /logs/_mapping



# 自定义分词器
	Character Filter
		在Tokenizer之前对文本进行处理，例如增加删除及替换字符。可以配置多个Character Filter。会影响Tokenizer的position和offset信息。
		一些自带的Character Filter
			HTML strip 除去html标签
			Mapping 字符串替换
			Pattern replace 正则匹配替换
	Tokenizer
		将原始的文本按照一定的规则，且分为词
		es内置的Tokenizer
			whitespace/standard/uax_url_email/pattern/keyword/path_hierarchy
		可以用java开发插件，实现自己的Tokenizer
	Token Filter
		将Tokenizer输出的单词（term），进行增加，修改，删除
		自带的Token Filters
			lowercase/STOP/synonym



POST _analyze
{
  "tokenizer":"keyword",
  "char_filter":["html_strip"],
  "text": "<b>hello world</b>"
}


POST _analyze
{
  "tokenizer":"path_hierarchy",
  "text":"/user/ymruan/a/b/c/d/e"
}



# 使用char filter进行替换
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
      {
        "type" : "mapping",
        "mappings" : [ "- => _"]
      }
    ],
  "text": "123-456, I-test! test-990 650-555-1234"
}

# char filter 替换表情符号
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [
      {
        "type" : "mapping",
        "mappings" : [ ":) => happy", ":( => sad"]
      }
    ],
    "text": ["I am felling :)", "Feeling :( today"]
}

# white space and snowball
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop","snowball"],
  "text": ["The gilrs in China are playing this game!"]
}


# whitespace与stop
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["stop","snowball"],
  "text": ["The rain in Spain falls mainly on the plain."]
}


# remove 加入lowercase后，The被当成 stopword删除
GET _analyze
{
  "tokenizer": "whitespace",
  "filter": ["lowercase","stop","snowball"],
  "text": ["The gilrs in China are playing this game!"]
}

//正则表达式
GET _analyze
{
  "tokenizer": "standard",
  "char_filter": [
      {
        "type" : "pattern_replace",
        "pattern" : "http://(.*)",
        "replacement" : "$1"
      }
    ],
    "text" : "http://www.elastic.co"
}







-----------------



## Dynamic Template和Index Template

Index Template帮助你设定Mapping和Setting，并按照一定的规则，自动匹配到新创建的索引之上
	模板仅在一个索引被新创建时，才会产生作用。
	你可以设定多个索引模板，这些设置会被“merge”在一起
	你可以指定“order”的数值，控制“merging”的过程

# 当一个索引被新创建时
	应用es默认的settings和mappings
	应用order数值低的index template中的设定
	应用order高的index Template中的设定，之前的设定会被覆盖
	应用创建索引时，用户指定的settings和mappings覆盖之前模板中的设定


#数字字符串被映射成text，日期字符串被映射成日期
PUT ttemplate/_doc/1
{
	"someNumber":"1",
	"someDate":"2019/01/01"
}
GET ttemplate/_mapping




# 两个Index Template
PUT _template/template_default
{
  "index_patterns": ["*"],
  "order" : 0,
  "version": 1,
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas":1
  }
}

PUT /_template/template_test
{
    "index_patterns" : ["test*"],
    "order" : 1,
    "settings" : {
    	"number_of_shards": 1,
        "number_of_replicas" : 2
    },
    "mappings" : {
    	"date_detection": false,
    	"numeric_detection": true
    }
}

#查看template信息
GET /_template/template_default
GET /_template/temp*


#写入新的数据，index以test开头
PUT testtemplate/_doc/1
{
	"someNumber":"1",
	"someDate":"2019/01/01"
}
GET testtemplate/_mapping
get testtemplate/_settings

PUT testmy
{
	"settings":{
		"number_of_replicas":5
	}
}

put testmy/_doc/1
{
  "key":"value"
}

get testmy/_settings
DELETE testmy
DELETE /_template/template_default
DELETE /_template/template_test



#Dynaminc Mapping 根据es识别的数据类型，结合字段名称，来动态设定字段类型

DELETE my_index

PUT my_index/_doc/1
{
  "firstName":"Ruan",
  "isVIP":"true"
}

GET my_index/_mapping
DELETE my_index


所有的字符串类型都设定成keyword，或者关闭keyword字段
is开头的字段都设置成boolean
long_开头的都设置成long类型

dynamic template定义在某个mapping中
ttemplate有一个名称
匹配规则是一个数组
为匹配到的字段设置mapping
match_mapping_type：匹配自动识别的字段类型，如string，Boolean
match、unmatch：匹配字段名

PUT my_index
{
  "mappings": {
    "dynamic_templates": [
            {
        "strings_as_boolean": {
          "match_mapping_type":   "string",
          "match":"is*",
          "mapping": {
            "type": "boolean"
          }
        }
      },
      {
        "strings_as_keywords": {
          "match_mapping_type":   "string",
          "mapping": {
            "type": "keyword"
          }
        }
      }
    ]
  }
}


DELETE my_index
#结合路径
PUT my_index
{
  "mappings": {
    "dynamic_templates": [
      {
        "full_name": {
          "path_match":   "name.*",
          "path_unmatch": "*.middle",
          "mapping": {
            "type":       "text",
            "copy_to":    "full_name"
          }
        }
      }
    ]
  }
}


PUT my_index/_doc/1
{
  "name": {
    "first":  "John",
    "middle": "Winston",
    "last":   "Lennon"
  }
}

GET my_index/_search?q=full_name:John




-------------


## Elasticsearch聚合分析简介

es除搜索外，提供针对es数据进行统计分析的功能
	实时性高
	Hadoop
通过聚合，我们会得到一个数据的概览，是分析和总结全套的数据，而不是寻找单个文档
	上海的客房数量
	不同的价格区间，可预订的五星级和经济型酒店的数量
高性能
	无需在客户端自己去实现分析逻辑


集合的分类
	Bukcet Aggregation：一些列满足特定条件的文档的集合
	Metric Aggregation：一些数学运算，可以对文档字段进行统计分析
	Pipeline Aggregation：对其他的聚合结果进行二次聚合
	Matrix Aggregation：支持对多个字段的操作并提供一个结果矩阵

Bukcet & Metrix
	select count(brand) -Metric
	from cars
	group by brand - Bukcet

Bukcet:
	es提供了很多类型的Bucket，帮助你用多种方式划分文档
		Term & Range （时间/年龄区间/地理位置） 
	高档、中档、低档
	高档：好评、中评、差评

Metrix:
	基于数据集计算结果，支持在字段上进行计算或者在脚本产生的结果之上进行计算
	大多数Metrix是数学计算，仅输出一个值
		min/max/sum/avg/cardinality
	部分Metrix支持输出多个数值
		stats/percentiles/percentiles_ranks




#按照目的地进行分桶统计
GET kibana_sample_data_flights/_search
{
	"size": 0,
	"aggs":{
		"flight_dest":{
			"terms":{
				"field":"DestCountry"
			}
		}
	}
}


关键词：aggs
	自定义名字
		不同类型的分析

#查看航班目的地的统计信息，增加平均，最高最低价格
GET kibana_sample_data_flights/_search
{
	"size": 0,
	"aggs":{
		"flight_dest":{
			"terms":{
				"field":"DestCountry"
			},
			"aggs":{
				"avg_price":{
					"avg":{
						"field":"AvgTicketPrice"
					}
				},
				"max_price":{
					"max":{
						"field":"AvgTicketPrice"
					}
				},
				"min_price":{
					"min":{
						"field":"AvgTicketPrice"
					}
				}
			}
		}
	}
}



嵌套

#查看航班目的地的统计信息，平均票价以及天气信息
GET kibana_sample_data_flights/_search
{
	"size": 0,
	"aggs":{
		"flight_dest":{
			"terms":{
				"field":"DestCountry"
			},
			"aggs":{
				"stats_price":{
					"stats":{
						"field":"AvgTicketPrice"
					}
				},
				"wather":{
				  "terms": {
				    "field": "DestWeather",
				    "size": 5
				  }
				}

			}
		}
	}
}



































































