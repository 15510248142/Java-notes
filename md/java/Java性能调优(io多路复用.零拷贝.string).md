## 如何制定性能调优标准

- CPU
  - CPU利用率则代表单位时间内一个线程或进程实时占用CPU的百分比
  - 系统负载代表单位时间内正在运行或等待的进程或线程数
  - CPU密集型
  - I/O密集型
- 内存
  - Java 程序一般通过 JVM 对内存进行分配管理，主要是用 JVM 中的堆内存来存储Java 创建的对象。系统堆内存的读写速度非常快，所以基本不存在读写性能瓶颈。但是由 于内存成本要比磁盘高，相比磁盘，内存的存储空间又非常有限。所以当内存空间被占满，对象无法回收时，就会导致内存溢出、内存泄露等问题。
- 磁盘 I/O
  - 磁盘相比内存来说，存储空间要大很多，但磁盘 I/O 读写的速度要比内存慢
- 网络
  - 网络带宽
- 异常
  - Java 应用中，抛出异常需要构建异常栈，对异常进行捕获和处理，这个过程非常消耗系统性能。
- 数据库
  - 数据库的操作往往是涉及到磁盘 I/O 的读写
- 锁竞争
  - 锁的使用可能会带来上下文切换，从而给系统带来性能开销。
- TPS
  - TPS(transaction per second)是单位时间内处理事务的数量
  - TPS代表一个事务的处理，可以包含了多次请求
  - TPS的一次事务代表一次用户操作到服务器返回结果
- QPS
  - QPS(query per second)是单位时间内请求的数量
  - QPS的一次请求代表一个接口的一次请求到服务器返回结果。当一次用户操作只包含一个请求接口时，TPS和QPS没有区别。当用户的一次操作包含了多个服务请求时，这个时候TPS作为这次用户操作的性能指标就更具有代表性了。
- 端口被CLOSE_WAIT占用
  - 可以通过tcpdump抓包看看连接状态，分析是否是服务端的FIN packet没有发出去。
  -   正常的关闭流程是：服务端在接收到客户端发送的关闭请求FIN后，会进入CLOSE_WAIT状态，同时发送ACK回去。在完成与客户端直接的通信操作之后，再向客户端发送FIN，进入LAST_ACK状态。
  - 如果连接是CLOSE_WAIT状态，而不是LAST_ACK状态，说明还没有发FIN给Client，那么可能是在关闭连接之前还有许多数据要发送或者其他事要做，导致没有发这个FIN packet。  





## 字符串

- String 对象是如何实现的
  -  从 Java9 版本开始，工程师将 char[] 字段改为了 byte[] 字段，又维护了一个新的属性coder，它是一个编码格式的标识
  - 从 Java7 版本开始到 Java8 版本，Java 对 String 类做了一些改变。String 类中不再有offset 和 count 两个变量了。这样的好处是 String 对象占用的内存稍微少了些，同时，String.substring 方法也不再共享 char[]，从而解决了使用该方法可能导致的内存泄漏问题。
    - 在Java6中substring方法会调用new string构造函数，此时会复用原来的char数组
- String 对象的不可变性
  -  String 类被 final 关键字修饰了，char[] 被 final+private 修饰，代表了String 对象不可被更改
  - 保证 String 对象的安全性
  -  可以实现字符串常量池
     - 当代码中使用String str= "abcdef"方式创建字符串对象时，JVM 首先会检查该对象是否在字符串常量池中，如果在，就返回该对象引用，否则新的字符串将在常量池中被创建。这种方式可以减少同一个值的字符串对象的重复创建，节约内存
     -  String str = new String(“abc”) 这种方式，首先在编译类文件时，"abc"常量字符串将会放入到常量结构中，在类加载时，“abc"将会在常量池中创建；其次，在调用 new 时，JVM 命令将会调用 String 的构造函数，同时引用常量池中的"abc” 字符串，在堆内存中创建一个 String 对象；最后，str 将引用 String 对象
        - 具体的复制过程是先将常量池中的字符串压入栈中，在使用string的构造方法时，会拿到栈中的字符串作为构造方法的参数
        - 如果调用 intern 方法，会去查看字符串常量池中是否有等于该对象的字符串，如果没有，就在常量池中新增该对象，并返回该对象引用；如果有，就返回常量池中的字符串引用。堆内存中原有的对象由于没有引用指向它，将会通过垃圾回收器回收。
        - intern方法生成的引用或对象是在运行时常量池中。
  -  对象在内存中是一块内存地址，str 则是一个指向该内存地址的引用
-  静态常量池和运行时常量池，
  - 静态常量池是存放字符串字面量、符号引用以及类和方法的信息，而运行时常量池存放的是运行时一些直接引用。
  - 运行时常量池是在类加载完成之后，将静态常量池中的符号引用值转存到运行时常量池中，类在解析之后，将符号引用替换成直接引用。
  - 这两个常量池在JDK1.7版本之后，就移到堆内存中了，这里指的是物理空间，而逻辑上还是属于方法区（方法区是逻辑分区）。  





## ArrayList还是LinkedList

- ArrayList
  - 数组是一块连续的内存空间，并且实现了RandomAccess 接口标志，意味着 ArrayList 可以实现快速随机访问，所以 for 循环效率非常高。
  - ArrayList 的实现类源码中对象数组 elementData 使用了transient 修饰，防止对象数组被其他外部方法序列化。
    - 由于 ArrayList 的数组是基于动态扩增的，所以并不是所有被分配的内存空间都存储了数据。
    - 如果采用外部序列化法实现数组的序列化，会序列化整个数组。ArrayList 为了避免这些没有存储数据的内存空间被序列化，内部提供了两个私有方法 writeObject 以及 readObject来自我完成序列化与反序列化，从而在序列化与反序列化数组时节省了空间和时间。
  - 如果我们在初始化时就比较清楚存储数据的大小，就可以在 ArrayList 初始化时指定数组容量大小，并且在添加元素时，只在数组末尾添加元素，那么 ArrayList 在大量新增元素的场景下，性能并不会变差
  - 两个方法也有不同之处，添加元素到任意位置，会导致在该位置后的所有元素都需要重新排列，而将元素添加到数组的末尾，在没有发生扩容的前提下，是不会有元素复制排序过程的。
  - ArrayList 在每一次有效的删除元素操作之后，都要进行数组的重组，并且删除的元素位置越前，数组重组的开销就越大。
- LinkedList
  - LinkedList 的两个重要属性 first/last 属性，其实还有一个 size 属性。我们可以看到这三个属性都被 transient 修饰了，原因很简单，我们在序列化的时候不会只对头尾进行序列化，所以 LinkedList 也是自行实现 readObject 和writeObject 进行序列化与反序列化
  - LinkedList 的获取元素操作实现跟 LinkedList 的删除元素操作基本类似，通过分前后半段来循环查找到对应的元素。但是通过这种方式来查询元素是非常低效的，特别是在 for 循环遍历的情况下，每一次循环都会去遍历半个 List。以使用 iterator 方式迭代循环，直接拿到我们的元素，而不需要通过循环查找 List。
- 在添加元素到尾部的操作中，我们发现，在没有扩容的情况下，ArrayList 的效率要高于LinkedList。这是因为 ArrayList 在添加元素到尾部的时候，不需要复制重排数据，效率非常高。而 LinkedList 虽然也不用循环查找元素，但 LinkedList 中多了 new 对象以及变换指针指向对象的过程，所以效率要低于 ArrayList。
- for迭代遍历
  - 当再次遍历时，会先调用内部类iteator中的hasNext(),再调用next(),在调用next()方法时，会对modCount和expectedModCount进行比较（checkForComodification()），此时两者不一致，就抛出了ConcurrentModificationException异常
  - 在foreach循环中调用list中的remove()方法，会走到fastRemove()方法，该方法不是iterator中的方法，而是ArrayList中的方法，在该方法只做了modCount++，而没有同步到expectedModCount。
  - 在iterator中的remove方法中，删除元素实际上调用的就是list.remove()方法，但是它多了一个操作：expectedModCount = modCount;
- 为什么 ArrayList 不像 HashMap 一样在扩容时需要一个负载因子
  - HashMap有负载因子是既要考虑数组太短，因哈希冲突导致链表过长而导致查询性能下降，也考虑了数组过长，新增数据时性能下降。这个负载因子是综合了数组和链表两者的长度，不能太大也不能太小。而ArrayList不需要这种考虑。





## HashMap

- 存储键值对（x，“aa”）时，哈希表会通过哈希函数 f(x) 得到"aa"的实现存储位置。

- 实际上一个链表被放满8个节点的概率非常小，实际上链表转红黑树是非常耗性能的，而链表在8个节点以内的平均查询时间复杂度与红黑树相差无几。

- hash() 方法

  - 如果我们没有使用 hash() 方法计算 hashCode，而是直接使用对象的 hashCode 值，会出现什么问题呢？

    - 假设要添加两个对象 a 和 b，如果数组长度是 16，这时对象 a 和 b 通过公式 (n - 1) &hash 运算，也就是 (16-1)＆a.hashCode 和 (16-1)＆b.hashCode。你会发现上述与运算结果都是 0。

  - 如果我们将 hashCode 值右移 16 位（h >>> 16），也就是取int 类型的一半，刚好可以将该二进制数对半切开，并且使用位异或运算（如果两个数对应的位置相反，则结果为 1，反之为 0），这样的话，就能避免上面的情况发生

    - ```java
      h = key.hashCode()) ^ (h >>> 16)
      ```

  -  (n - 1) & hash 是怎么设计的，这里的 n 代表哈希表的长度，哈希表习惯将长度设置为 2 的 n 次方，这样恰好可以保证 (n - 1) & hash 的计算得到的索引值总是位于table 数组的索引之内。例如：hash=15，n=16 时，结果为 15；hash=17，n=16 时，结果为 1。

- 在获得 Node 的存储位置后，如果判断 Node 不在哈希表中，就新增一个 Node

  - 如果存在tab[i]节点，分三种情况
    - 存在和Key相等的节点
    - 红黑树节点
    - 链表节点
      - 长度是否超过阈值
        - 转红黑树（链表长度大于8而且整个map中的键值对大于等于MIN_TREEIFY_CAPACITY (64)时，才进行链表到红黑树的转换）
      - 链表尾部新增节点

- HashMap 扩容优化

  - JDK1.7 中
    - 分别取出数组元素，一般该元素是最后一个放入链表中的元素，然后遍历以该元素为头的单向链表元素，依据每个被遍历元素的 hash 值计算其在新数组中的下标
  -  JDK 1.8 中
    - HashMap 对扩容操作做了优化。由于扩容数组的长度是 2 倍关系，所以对于假设初始 tableSize = 4 要扩容到 8 来说就是 0100 到 1000 的变化
    - 在扩容中只用判断原来的 hash 值和左移动的一位（newtable 的值）按位与操作是 0 或 1 就行，0 的话索引不变，1 的话索引变成原索引加上扩容前数组。

- 实际应用中，我们设置初始容量，一般得是 2 的整数次幂。

  - 2的幂次方减1后每一位都是1，让数组每一个位置都能添加到元素。
  - 例如十进制8，对应二进制1000，减1是0111，这样在&hash值使数组每个位置都是可以添加到元素的，如果有一个位置为0，那么无论hash值是多少那一位总是0，例如0101，&hash后第二位总是0，也就是说数组中下标为2的位置总是空的。
  - 减少哈希冲突，均匀分布元素





## 序列化

- 两个服务之间要共享一个数据对象，就需要从对象转换成二进制流，通过网络传输，传送到对方服务，再转换回对象，供服务方法调用。这个编码和解码过程我们称之为序列化与反序列化。
- 在大量并发请求的情况下，如果序列化的速度慢，会导致请求响应时间增加；而序列化后的传输数据体积大，会导致网络吞吐量下降。所以一个优秀的序列化框架可以提高系统的整体性能
- 一个使用单例模式实现的类，如果我们将该类实现 Java 的 Serializable 接口，它还是单例吗？如果要你来写一个实现了 Java 的 Serializable 接口的单例，你会怎么写呢？
  - 序列化会通过反射调用无参构造器返回一个新对象，破坏单例模式。反序列化得到的对象，和序列化之前的对象，不是同一个对象
  - 解决方法是重写readResolve，返回单例对象的方式来避免这个问题
    - Java的序列化机制提供了一个钩子方法，即私有的readresolve方法，允许我们来控制反序列化时得到的对象。
    - 序列化时，先执行了writeReplace方法，后执行了writeObject方法；在反序列化的时候，先执行了readObject方法，最后执行了readResolve方法
-  Protobuf序列化
  - Protobuf 以一个 .proto 后缀的文件为基础，这个文件描述了字段以及字段类型，在序列化该数据对象的时候，Protobuf 通过.proto文件描述来生成 Protocol Buffers 格式的编码。 
  - 它使用 T-L-V（标识 - 长度 - 字段值）的数据格式来存储数据





## 通信协议

- 微服务的核心是远程通信和服务治理

- RPC 通信包括了建立通信、实现报文、传输协议以及传输数据编解码等操作

  - 选择合适的通信协议

    - 网络传输协议有 TCP、UDP 协议，这两个协议都是基于 Socket 编程接口之上
    - TCP：socket()->bind()->listen()->accept()阻塞等待客户端连接—>connect()->read()、write()->close()
      - 当有一个客户端连接到服务端之后，服务端就会调用 fork 创建一个子进程，通过系统调用read 监听客户端发来的消息，再通过 write 向客户端返回信息。
    - UDP：socket()->bind()->recvfrom()、sendto()->close()

  - 使用单一长连接

    - 基于长连接实现，就可以省去大量的 TCP 建立和关闭连接的操作

  - 优化 Socket 通信

    - 传统的Socket 通信主要存在 I/O 阻塞、线程模型缺陷以及内存拷贝等问题。我们可以使用比较成熟的通信框架，比如 Netty
    - 高效的 Reactor 线程模型
      - 服务端接收客户端请求连接是用了一个主线程
      - 监听到事件后会创建一个链路请求。链路请求将会注册到负责 I/O 操作的 I/O 工作线程上，由 I/O 工作线程负责后续的 I/O 操作。
    - 串行设计
      - Netty 采用了串行无锁化完成链路操作，Netty 提供了 Pipeline 实现链路的各个操作在运行期间不进行线程切换。
    - 零拷贝
      - NIO 提供的ByteBuffer 可以使用 Direct Buffers 模式，直接开辟一个非堆物理内存，不需要进行字节缓冲区的二次拷贝，可以直接将数据写入到内核空间。

  - 量身定做报文格式

  - 编码、解码

  - 调整 Linux 的 TCP 参数设置选项

    

    



## IO多路复用

- 复用是指一个线程可以服务多条IO流，I/O multiplexing 这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Socket(I/O流)的状态来同时管理多个I/O流。
  - IO多路复用的优势在于，当处理的消耗对比IO几乎可以忽略不计时，可以处理大量的并发IO，而不用消耗太多CPU/内存
  - IO多路复用 + 单进（线）程有个额外的好处，就不会有并发编程的各种坑问题，比如在nginx里，redis里，编程实现都会很简单很多。
  - Java世界里，因为JDBC这个东西是BIO的，所以在我们常见的Java服务里没有办法做到全部NIO化，必须得弄成多线程模型。如果要在做Java web服务这个大场景下享受IO多路复用的好处，要不就是不需要DB的，要不就是得用Vert.X一类的纯NIO框架把DB IO访问也得框进来。
- I/O 复用
  - Linux 提供了 I/O 复用函数 select/poll/epoll，进程将一个或多个读操作通过系统调用函数，阻塞在函数操作上。这样，系统内核就可以帮我们侦测多个读操作是否处于就绪状态。
  - select() 函数调用后会阻塞，直到有描述符就绪或者超时，函数返回。当 select 函数返回后，可以通过函数 FD_ISSET 遍历 fdset，来找到就绪的描述符
  - select() 函数监视的文件描述符分 3 类，分别是 writefds（写文件描述符）、readfds（读文件描述符）以及 exceptfds（异常事件文件描述符）
  - poll() 和 select() 存在一个相同的缺点，那就是包含大量文件描述符的数组被整体复制到用户态和内核的地址空间之间，而无论这些文件描述符是否就绪，他们的开销都会随着文件描述符数量的增加而线性增大
  - epoll 使用事件驱动的方式代替轮询扫描fd。
  - 一旦某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知，之后进程将完成相关 I/O 操作
  - 进\线程并不是在socket上阻塞  而是在select/epoll上阻塞   socket是否阻塞是交给内核来判断然后给进程发送信号
- Epoll代理
  - 没有最大并发连接的限制，能打开的fd上限远大于1024（1G的内存能监听约10万个端口）
  - 采用回调的方式，效率提升。只有活跃可用的fd才会调用callback函数，也就是说 epoll 只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，epoll的效率就会远远高于select和poll。只轮询发出了事件的流，哪个流发生了怎样的IO事件会通知处理线程，因此对这些流的操作都是有意义的，复杂度降低到了O(1)
  - 内存拷贝。使用mmap()文件映射内存来加速与内核空间的消息传递，减少复制开销
- CPU本来就是线性的不论什么都需要顺序处理并行只能是多核CPU
- io多路复用本来就是用来解决对多个I/O监听时,一个I/O阻塞影响其他I/O的问题,跟多线程没关系.
- 跟多线程相比较,线程切换需要切换到内核进行线程切换,需要消耗时间和资源.而I/O多路复用不需要切换线/进程,效率相对较高,特别是对高并发的应用nginx就是用I/O多路复用,故而性能极佳.但多线程编程逻辑和处理上比I/O多路复用简单.而I/O多路复用处理起来较为复杂



## NIO

- 同步异步、阻塞非阻塞
  - 同步或异步（synchronous/asynchronous）。简单来说，同步是一种可靠的有序运行机制，当我们进行同步操作时，后续的任务是等待当前调用返回，才会进行下一步；而异步则相反，其他任务不需要等待当前调用返回，通常依靠事件、回调等机制来实现任务间次序关系。
  - 区分阻塞与非阻塞（blocking/non-blocking）。在进行阻塞操作时，当前线程会处于阻塞状态，无法从事其他任务，只有当条件就绪才能继续，比如 ServerSocket 新连接建立完毕，或数据读取、写入操作完成；而非阻塞则是不管 IO 操作是否结束，直接返回，相应操作在后台继续处理

- io
  - 首先，传统的 java.io 包，它基于流模型实现，提供了我们最熟知的一些 IO 功能，比如File 抽象、输入输出流等。交互方式是同步、阻塞的方式，也就是说，在读取输入流或者写入输出流时，在读、写动作完成之前，线程会一直阻塞在那里，它们之间的调用是可靠的线性顺序
  - 线程上下文切换开销会在高并发时变得很明显，这是同步阻塞方式的低扩展性劣势
  
- 传统 I/O 的性能问题

  - 磁盘 I/O 操作和网络 I/O 操作
  - 多次内存复制，导致不必要的数据拷贝和上下文切换，从而降低 I/O的性能
  - 大量连接请求时，创建大量监听线程，这时如果线程没有数据就绪就会被挂起，然后进入阻塞状态。
    - 阻塞线程在阻塞状态是不会占用CPU资源的，但是会被唤醒争夺CPU资源。操作系统将CPU轮流分配给线程任务，当线程数量越多的时候，当某个线程在规定的时间片运行完之后，会被其他线程抢夺CPU资源，此时会导致上下文切换。抢夺越激烈，上下文切换就越频繁
  - 在传统 I/O 中，InputStream 的 read() 是一个 while 循环操作，它会一直等待数据读取直到数据就绪才会返回。这就意味着如果没有数据就绪，这个读取操作将会一直被挂起，用线程将会处于阻塞状态。

- 如何优化 I/O 操作

  - NIO 的发布优化了内存复制以及阻塞导致的严重性能问题
  - 使用缓冲区优化读写流操作
    - 传统 I/O 和 NIO 的最大区别就是传统 I/O 是面向流，NIO 是面向 Buffer。buffer 可以文件一次性读入内存再做后续处理，而传统的方式是边读文件边处理数据
    - 缓冲区（Buffer）和通道（Channel）
      - Buffer 是一块连续的块，是 NIO 读写数据的中转地。
        - 在没有bytebuff缓存的情况下，一旦读取数据的SO_RCVBUF满了，将会通知对端TCP协议中的窗口关闭（滑动窗口），将影响TCP发送端，这也就影响到了整个TCP通信的速度。而有了bytebuff，我们可以先将读取的数据缓存在bytebuff中，提高TCP的通信能力。
      - Channel 表示缓冲数据的源头或者目的地，它用于读取缓冲或者写入数据，是访问缓冲的接口。
  - 使用 DirectBuffer 减少内存复制
    - NIO 的 Buffer 除了做了缓冲块优化之外，还提供了一个可以直接访问物理内存的类DirectBuffer。普通的 Buffer 分配的是 JVM 堆内存，而 DirectBuffer 是直接分配物理内存。
    - 直接将步骤简化为从内核空间复制到外部设备，减少了数据拷贝
    - 由于 DirectBuffer 申请的是非 JVM 的物理内存，所以创建和销毁的代价很小。DirectBuffer 申请的内存并不是直接由 JVM 负责垃圾回收，但在 DirectBuffer 包类被回收时，会通过 Java Reference 机制来释放该内存块

- NIO主要组成部分
  - Buffer，高效的数据容器，除了布尔类型，所有原始数据类型都有相应的 Buffer 实现。
  - Channel，类似在 Linux 之类操作系统上看到的文件描述符，是 NIO 中被用来支持批量式 IO 操作的一种抽象。
    - 外部设备(磁盘)通过DMA控制器（DMAC），向CPU提出接管总线控制权的总线请求
  - CPU对某个设备接口响应DMA请求时，会让出总线控制权。于是在DMA控制器的管理下，磁盘和存储器直接进行数据交换，而不需CPU干预。 
    - 通道则是在DMA的基础上增加了能执行有限通道指令的I/O控制器，代替CPU管理控制外设
  - File 或者 Socket，通常被认为是比较高层次的抽象，而 Channel 则是更加操作系统底层的一种抽象，这也使得 NIO 得以充分利用现代操作系统底层机制，获得特定场景的性能优化。
  - Selector，是 NIO 实现多路复用的基础，它提供了一种高效的机制，可以检测到注册在Selector 上的多个 Channel 中，是否有 Channel 处于就绪状态，进而实现了单线程对多 Channel 的高效管理
    - selector 是 Java NIO 编程的基础。用于检查一个或多个 NIO Channel 的状态是否处于可读、可写。
    - selector 是基于事件驱动实现的，一个线程使用一个 Selector，通过轮询的方式，可以监听多个 Channel 上的事件。
    - 目前操作系统的 I/O 多路复用机制都使用了 epoll，相比传统的 select 机制，epoll 没有最大连接句柄 1024 的限制。所以 Selector 在理论上可以轮询成千上万的客户端。
  
- NIO 多路复用

  - 首先，通过 Selector.open() 创建一个 Selector，作为类似调度员的角色。
  - 然后，创建一个 ServerSocketChannel，并且向 Selector 注册，通过指定SelectionKey.OP_ACCEPT，告诉调度员，它关注的是新的连接请求。注意，为什么我们要明确配置非阻塞模式呢？这是因为阻塞模式下，注册操作是不允许的，会抛出 IllegalBlockingModeException 异常。
  - Selector 阻塞在 select 操作，当有 Channel 发生接入请求，就会被唤醒

- 对于多路复用IO，当出现有的IO请求在数据拷贝阶段，会出现由于资源类型过份庞大而导致线程长期阻塞，最后造成性能瓶颈的情况

- 网络 I/O 模型优化
  
  - B和N通常是针对数据是否就绪的处理方式来
    sync和async是对阻塞进行更深一层次的阐释，区别在于数据拷贝由用户线程完成还是内核完成，讨论范围一定是两个线程及以上了。
    
    > 同步阻塞，从数据是否准备就绪到数据拷贝都是由用户线程完成
    >
    > 同步非阻塞，数据是否准备就绪由内核判断，数据拷贝还是用户线程完成
    >
    > 异步非阻塞，数据是否准备就绪到数据拷贝都是内核来完成
    >
    > 所以真正的异步IO一定是非阻塞的。
    >
    > 多路复用IO即使有Reactor通知用户线程也是同步IO范畴，因为数据拷贝期间仍然是用户线程完成。
    >
    > 所以假如我们没有内核支持数据拷贝的情况下，讨论的非阻塞并不是彻底的非阻塞，也就没有引入sync和async讨论的必要了  
    
  
- 线程模型优化
  - Reactor 模型是同步 I/O 事件处理的一种常见模型，其核心思想是将 I/O 事件注册到多路复用器上，一旦有 I/O 事件触发，多路复用器就会将事件分发到事件处理器中，执行就绪的 I/O 事件操作。
  - 单线程 Reactor 线程模型
    - 最开始 NIO 是基于单线程实现的，所有的 I/O 操作都是在一个 NIO 线程上完成。由于NIO 是非阻塞 I/O，理论上一个线程可以完成所有的 I/O 操作。
  - 多线程 Reactor 线程模型
    - Acceptor 线程来监听连接请求事件，当连接成功之后，会将建立的连接注册到多路复用器中，一旦监听到事件，将交给 Worker 线程池来负责处理
  - 主从 Reactor 线程模型
    - Acceptor 不再是一个单独的 NIO 线程，而是一个线程池。Acceptor 接收到客户端的 TCP 连接请求，建立连接之后，后续的 I/O 操作将交给 Worker I/O 线程。
- 基于线程模型的 Tomcat 参数调优
  - 在 NIO 中，Tomcat 新增了一个 Poller 线程池，Acceptor 监听到连接后，不是直接使用Worker 中的线程处理请求，而是先将请求发送给了 Poller 缓冲队列。在 Poller 中，维护了一个 Selector 对象，通过遍历 Selector，找出其中就绪的 I/O 操作，并使用 Worker 中的线程处理相应的请求
    - acceptorThreadCount：该参数代表 Acceptor 的线程数量
    - maxThreads：专门处理 I/O 操作的 Worker 线程数量
    - acceptCount ：这里的 acceptCount 指的是 accept 队列的大小。
      - Tomcat 的 Acceptor 线程是负责从 accept 队列中取出该 connection，然后交给工作线程去执行相关操作
      - 当 Http 关闭 keep alive，在并发量比较大时，可以适当地调大这个值。而在 Http 开启keep alive 时，因为 Worker 线程数量有限，Worker 线程就可能因长时间被占用，而连接在 accept 队列中等待超时。如果 accept 队列过大，就容易浪费连接。
    - maxConnections：表示有多少个 socket 连接到 Tomcat 上。





## 零拷贝

- "零拷贝"中的"拷贝"是操作系统在I/O操作中,将数据从一个内存区域复制到另外一个内存区域. 而"零"并不是指0次复制, 更多的是指在用户态和内核态之间的复制是0次.
  
  - CPU COPY
    - 在"拷贝"发生的时候,往往需要CPU暂停现有的处理逻辑,来协助内存的读写.这种我们称为CPU COPY
  - DMA COPY
    - 当需要与外设进行数据交换时, CPU只需要初始化这个动作便可以继续执行其他指令,剩下的数据传输的动作完全由DMA来完成
    - DMA COPY是可以避免大量的CPU中断的
  - 存在多次拷贝的原因
    - 操作系统为了保护系统不被应用程序有意或无意地破坏,为操作系统设置了用户态和内核态两种状态.用户态想要获取系统资源(例如访问硬盘), 必须通过系统调用进入到内核态, 由内核态获取到系统资源,再切换回用户态返回应用程序.
    - 操作系统在内核态中也增加了一个"内核缓冲区"(kernel buffer). 读取数据时并不是直接把数据读取到应用程序的buffer, 而先读取到kernel buffer, 再由kernel buffer复制到应用程序的buffer. 因此,数据在被应用程序使用之前,可能需要被多次拷贝
  - 从硬盘上读取文件数据, 发送到网络上去.
    - ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190807155340609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppODI4MTk1NzAyNA==,size_16,color_FFFFFF,t_70)
    - 一次read-send涉及到了四次拷贝
      - 其中涉及到2次cpu中断, 还有4次的上下文切换
      - 很明显,第2次和第3次的的copy只是把数据复制到app buffer又原封不动的复制回来, 为此带来了两次的cpu copy和两次上下文切换, 是完全没有必要的
      - linux的零拷贝技术就是为了优化掉这两次不必要的拷贝
  - sendFile
    - ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190807155602428.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppODI4MTk1NzAyNA==,size_16,color_FFFFFF,t_70)
    - 这个系统调用可以在内核态内把数据从内核缓冲区直接复制到套接字(SOCKET)缓冲区内, 从而可以减少上下文的切换和不必要数据的复制
  - mmap和sendFile
    - 虚拟内存
      - 一个用户虚拟地址和内核虚拟地址可以指向同一个物理内存地址
      - 虚拟内存空间可大于实际可用的物理地址；
    - 利用第一条特性可以把内核空间地址和用户空间的虚拟地址映射到同一个物理地址，这样DMA就可以填充对内核和用户空间进程同时可见的缓冲区了
    - java也利用操作系统的此特性来提升性能
    - MMAP(内存映射文件), 是指将文件映射到进程的地址空间去, 实现硬盘上的物理地址跟进程空间的虚拟地址的一一对应关系.
    - MMAP是另外一个用于实现零拷贝的系统调用.跟sendFile不一样的地方是, 它是利用共享内存空间的方式, 避免app buf和kernel buf之间的数据拷贝(两个buf共享同一段内存)
  - 在 Linux 中零拷贝技术主要有 3 个实现思路：用户态直接 I/O、减少数据拷贝次数以及写时复制技术。
    - 用户态直接 I/O：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。
    - 减少数据拷贝次数：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝，这也是当前主流零拷贝技术的实现思路。
    - 写时复制技术：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。
  - Java零拷贝
    - ByteBuffer byteBuffer = ByteBuffer.allocate(512);
      - 堆内存：返回 HeapByteBuffer
      - HeapByteBuffer是在jvm的内存范围之内，然后在调io的操作时会将数据区域拷贝一份到os的内存区域
      - 只能拷贝jvm的那一份到os的内存空间，即使jvm那边的数据区域被改变，但是os里边的不会受到影响，等os使用io结束后会对这块区域进行回收，因为这是os的管理范围之内
    - DirectByteBuffer
      - 开辟了一段直接的内存，并不会占用jvm的内存空间
      - DirectByteBuffer使用的是直接的堆外内存，这块内存直接与io设备进行交互
  
  
  





## epoll

- 进程阻塞为什么不占用cpu资源

  - 了解epoll本质的**第三步**，要从**操作系统进程调度**的角度来看数据接收。阻塞是进程调度的关键一环，指的是进程在等待某事件（如接收到网络数据）发生之前的等待状态，recv、select和epoll都是阻塞方法。

  - 为简单起见，我们从普通的recv接收开始分析，先看看下面代码：

    ```c++
    //创建socket
    int s = socket(AF_INET, SOCK_STREAM, 0);   
    //绑定
    bind(s, ...)
    //监听
    listen(s, ...)
    //接受客户端连接
    int c = accept(s, ...)
    //接收客户端数据
    recv(c, ...);
    //将数据打印出来
    printf(...)
    ```

    这是一段最基础的网络编程代码，先新建socket对象，依次调用bind、listen、accept，最后调用recv接收数据。recv是个阻塞方法，当程序运行到recv时，它会一直等待，直到接收到数据才往下执行。

- 阻塞的原理

  - 操作系统为了支持多任务，实现了进程调度的功能，会把进程分为“运行”和“等待”等几种状态。运行状态是进程获得cpu使用权，正在执行代码的状态；等待状态是阻塞状态，比如上述程序运行到recv时，程序会从运行状态变为等待状态，接收到数据后又变回运行状态。操作系统会分时执行各个运行状态的进程，由于速度很快，看上去就像是同时执行多个任务。
  - 一开始，这3个进程都被操作系统的**工作队列**所引用，处于运行状态，会分时执行。
  - 当进程A执行到创建socket的语句时，操作系统会创建一个由文件系统管理的**socket对象**。这个socket对象包含了发送缓冲区、接收缓冲区、等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该socket事件的进程。
  - 当程序执行到recv时，操作系统会将进程A从工作队列移动到该socket的**等待队列**中。由于工作队列只剩下了进程B和C，依据进程调度，cpu会轮流执行这两个进程的程序，不会执行进程A的程序。**所以进程A被阻塞，不会往下执行代码，也不会占用cpu资源**。
  - 当socket接收到数据后，操作系统将该socket等待队列上的进程重新放回到工作队列，该进程变成运行状态，继续执行代码。也由于socket的接收缓冲区已经有了数据，recv可以返回接收到的数据。

- 内核接收网络数据全过程

  - 进程在recv阻塞期间，计算机收到了对端传送的数据（步骤①）。数据经由网卡传送到内存（步骤②），然后网卡通过中断信号通知cpu有数据到达，cpu执行中断程序（步骤③）。此处的中断程序主要有两项功能，先将网络数据写入到对应socket的接收缓冲区里面（步骤④），再唤醒进程A（步骤⑤），重新将进程A放入工作队列中。

- 操作系统如何知道网络数据对应于哪个socket？

  - 因为一个socket对应着一个端口号，而网络数据包中包含了ip和端口的信息，内核可以通过端口号找到对应的socket。当然，为了提高处理速度，操作系统会维护端口号到socket的索引结构，以快速读取。

- 如何同时监视多个socket的数据？

  - 服务端需要管理多个客户端连接，而recv只能监视单个socket，这种矛盾下，人们开始寻找监视多个socket的方法。epoll的要义是**高效**的监视多个socket。**从历史发展角度看，必然先出现一种不太高效的方法**，人们再加以改进。只有先理解了不太高效的方法，才能够理解epoll的本质。

  - 如能够预先传入一个socket列表，**如果列表中的socket都没有数据，挂起进程，直到有一个socket收到数据，唤醒进程**。这种方法很直接，也是**select**的设计思想。

  - 为方便理解，我们先复习select的用法。在如下的代码中，先准备一个数组（下面代码中的fds），让fds存放着所有需要监视的socket。然后调用select，如果fds中的所有socket都没有数据，select会阻塞，直到有一个socket接收到数据，select返回，唤醒进程。用户可以遍历fds，通过FD_ISSET判断具体哪个socket收到数据，然后做出处理。

    ```c++
    int s = socket(AF_INET, SOCK_STREAM, 0);  
    bind(s, ...)
    listen(s, ...)
    
    int fds[] =  存放需要监听的socket
    
    while(1){
        int n = select(..., fds, ...)
        for(int i=0; i < fds.count; i++){
            if(FD_ISSET(fds[i], ...)){
                //fds[i]的数据处理
            }
        }
    }
    ```

  - select的实现思路很直接。假如程序同时监视sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中
  - 当任何一个socket收到数据后，中断程序将唤起进程。谓唤起进程，就是将进程从所有的等待队列中移除，加入到工作队列里面。
  - 经由这些步骤，当进程A被唤醒后，它知道至少有一个socket接收了数据。程序只需遍历一遍socket列表，就可以得到就绪的socket。

- 缺点

  - 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。
  - 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。
  - 本节只解释了select的一种情形。当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞。

- epoll的设计思路

  - epoll通过以下一些措施来改进效率。

  - select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。

    - 如下图所示，每次调用select都需要这两步操作，然而大多数应用场景中，需要监视的socket相对固定，并不需要每次都修改。epoll将这两个操作分开，先用epoll_ctl维护等待队列，再调用epoll_wait阻塞进程。显而易见的，效率就能得到提升。

    - ![img](https://pic2.zhimg.com/80/v2-5ce040484bbe61df5b484730c4cf56cd_720w.jpg)

    - 为方便理解后续的内容，我们先复习下epoll的用法。如下的代码中，先用epoll_create创建一个epoll对象epfd，再通过epoll_ctl将需要监视的socket添加到epfd中，最后调用epoll_wait等待数据。

      ```c++
      int s = socket(AF_INET, SOCK_STREAM, 0);   
      bind(s, ...)
      listen(s, ...)
      
      int epfd = epoll_create(...);
      epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中
      
      while(1){
          int n = epoll_wait(...)
          for(接收到数据的socket){
              //处理
          }
      }
      ```

      功能分离，使得epoll有了优化的可能。

  - **措施二：就绪列表**

    select低效的另一个原因在于程序不知道哪些socket收到数据，只能一个个遍历。如果内核维护一个“就绪列表”，引用收到数据的socket，就能避免遍历。假设计算机共有三个socket，收到数据的sock2和sock3被rdlist（就绪列表）所引用。当进程被唤醒后，只要获取rdlist的内容，就能够知道哪些socket收到数据。

- epoll的原理和流程

  - **创建epoll对象**
    - 当某个进程调用epoll_create方法时，内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）。eventpoll对象也是文件系统中的一员，和socket一样，它也会有等待队列。
  - **维护监视列表**
    - 创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket。以添加socket为例，如果通过epoll_ctl添加sock1、sock2和sock3的监视，内核会将eventpoll添加到这三个socket的等待队列中。
    - 当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程。
  - **接收数据**
    - 当socket收到数据后，中断程序会给eventpoll的“就绪列表”添加socket引用
    - eventpoll对象相当于是socket和进程之间的中介，socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。
    - 当程序执行到epoll_wait时，如果rdlist已经引用了socket，那么epoll_wait直接返回，如果rdlist为空，阻塞进程。
  - **阻塞和唤醒进程**
    - 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。内核会将进程A放入eventpoll的等待队列中，阻塞进程。
    - 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。

- eventpoll的数据结构

  - **就绪列表的数据结构**

    就绪列表引用着就绪的socket，所以它应能够快速的插入数据。

    程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。

    所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist）。

  - **索引结构**

    既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll使用了红黑树作为索引结构（对应上图的rbr）。

    > ps：因为操作系统要兼顾多种功能，以及由更多需要保存的数据，rdlist并非直接引用socket，而是通过epitem间接引用，红黑树的节点也是epitem对象。同样，文件系统也并非直接引用着socket。

- select、poll和epoll的区别
  
  - ![img](https://pic2.zhimg.com/80/v2-14e0536d872474b0851b62572b732e39_720w.jpg)



一、ET模式（边沿触发）的文件描述符(fd)：

​         当epoll_wait检测到fd上有事件发生并将此事件通知应用程序后，应用程序必须立即处理该事件，因为后续的epoll_wait调用将不再向应用程序通知这一事件。

​         epoll_wait只有在客户端第一次发数据是才会返回,以后即使缓冲区里还有数据，也不会返回了。epoll_wait是否返回，是看客户端是否发数据，客户端发数据了就会返回，且只返回一次。

​         eg：客户端发送数据，I/O函数只会提醒一次服务端fd上有数据，以后将不会再提醒

所以要求服务端必须一次把数据读完--->循环读数据 (读完数据后，可能会阻塞)  --->将描述符设置成非阻塞模式

二、LT模式（水平触发）的文件描述符(fd)：

​         当epoll_wait检测到fd上有事件发生并将此事件通知应用程序后，应用程序可以不立即处理该事件，这样，当应用程序下一次调用epoll_wait时，epoll_wait还会再次向应用程序通知此事件，直到此事件被处理。

eg：客户端发送数据，I/O函数会提醒描述符fd有数据---->recv读数据，若一次没有读完，I/O函数会一直提醒服务端fd上有数据，直到recv缓冲区里的数据读完

 

三、可见ET模式在很大程度上降低了同一个epoll事件被重复触发的次数，因此ET模式效率比LT模式高

​         原因：ET模式下事件被触发的次数比LT模式下少很多

注意：每个使用ET模式的文件描述符都应该是非阻塞的。 如果描述符是阻塞的，那么读或写操作将会因没有后续事件而一直处于阻塞状态 ( 饥渴状态 )。



