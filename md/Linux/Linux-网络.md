

## **怎么评估系统的网络性能**

- **性能指标回顾**

  - 首先，**带宽**，表示链路的最大传输速率，单位是 b/s（比特 / 秒）。在你为服务器选购网卡 时，带宽就是最核心的参考指标。常用的带宽有 1000M、10G、40G、100G 等。 
  - 第二，**吞吐量**，表示没有丢包时的最大数据传输速率，单位通常为 b/s （比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽的限制，吞吐量 / 带宽也就是该网络链路的使用率。 
  - 第三，**延时**，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。这个指标 在不同场景中可能会有不同的含义。它可以表示建立连接需要的时间（比如 TCP 握手延时），或者一个数据包往返所需时间（比如 RTT）。 
  - 最后，**PPS**，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速 率。PPS 通常用来评估网络的转发能力，而基于 Linux 服务器的转发，很容易受到网络包 大小的影响（交换机通常不会受到太大影响，即交换机可以线性转发）。 

- **网络基准测试**

  - 基于 HTTP 或者 HTTPS 的 Web 应用程序，显然属于应用层，需要我们测试 HTTP/HTTPS 的性能；
  - 而对大多数游戏服务器来说，为了支持更大的同时在线人数，通常会基于 TCP 或 UDP ，与客户端进行交互，这时就需要我们测试 TCP/UDP 的性能

- **各协议层的性能测试**

  - 转发性能

    - hping3 作为一个 SYN 攻击的工具来使用

    - Linux 内核自带的高性能网络测试工具 pktgen。 pktgen 支持丰富的自定义选项，方便你根据实际需要构造所需网络包，从而更准确地测试 出目标服务器的性能。

    - 不过，在 Linux 系统中，你并不能直接找到 pktgen 命令。因为 pktgen 作为一个内核线 程来运行，需要你加载 pktgen 内核模块后，再通过 /proc 文件系统来交互。

    - ```shell
      1 $ modprobe pktgen
      2 $ ps -ef | grep pktgen | grep -v grep
      
      5 $ ls /proc/net/pktgen/
      
      
      ```

    - pktgen 在每个 CPU 上启动一个内核线程，并可以通过 /proc/net/pktgen 下面的同名文 件，跟这些线程交互；而 pgctrl 则主要用来控制这次测试的开启和停止。

  - $ cat /proc/net/pktgen/eth0

    - 你可以看到，测试报告主要分为三个部分： 
      - 第一部分的 Params 是测试选项； 
      - 第二部分的 Current 是测试进度，其中， packts so far（pkts-sofar）表示已经发送了 100 万个包，也就表明测试已完成。 
      - 第三部分的 Result 是测试结果，包含测试所用时间、网络包数量和分片、PPS、吞吐量 以及错误数。
      - PPS 为 12 万
        - 作为对比，你可以计算一下千兆交换机的 PPS。交换机可以达到线速（满负载时，无差错 转发），它的 PPS 就是 1000Mbit 除以以太网帧的大小，即 1000Mbps/((64+20)*8bit) = 1.5 Mpps（其中，20B 为以太网帧前导和帧间距的大小）。 你看，即使是千兆交换机的 PPS，也可以达到 150 万 PPS，比我们测试得到的 12 万大多 了。

  - TCP/UDP 性能

    - 相应的测试工具，比如 iperf 或者 netperf

      - 特别是现在的云计算时代，在你刚拿到一批虚拟机时，首先要做的，应该就是用 iperf ，测 试一下网络性能是否符合预期。 iperf 和 netperf 都是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量。它们都以 客户端和服务器通信的方式，测试一段时间内的平均吞吐量。

    - ```shell
      1 # -s 表示启动服务端，-i 表示汇报间隔，-p 表示监听端口
      2 $ iperf3 -s -i 1 -p 10000
      ```

    - ```shell
      # 接着，在另一台机器上运行 iperf 客户端，运行测试
      1 # -c 表示启动客户端，192.168.0.30 为目标服务器的 IP
      2 # -b 表示目标带宽 (单位是 bits/s)
      3 # -t 表示测试时间
      4 # -P 表示并发数，-p 表示目标服务器监听端口
      5 $ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000
      ```

    - 稍等一会儿（15 秒）测试结束后，回到目标服务器，查看 iperf 的报告

      - 最后的 SUM 行就是测试的汇总结果，包括测试时间、数据传输量以及带宽等。按照发送和 接收，这一部分又分为了 sender 和 receiver 两行。 从测试结果你可以看到，这台机器 TCP 接收的带宽（吞吐量）为 860 Mb/s， 跟目标的 1Gb/s 相比，还是有些差距的。 

  - HTTP 性能

    - 从传输层再往上，到了应用层。有的应用程序，会直接基于 TCP 或 UDP 构建服务。当 然，也有大量的应用，基于应用层的协议来构建服务，HTTP 就是最常用的一个应用层协 议。比如，常用的 Apache、Nginx 等各种 Web 服务，都是基于 HTTP。

    - 要测试 HTTP 的性能，也有大量的工具可以使用，比如 ab、webbench 等，都是常用的 HTTP 压力测试工具。其中，ab 是 Apache 自带的 HTTP 压测工具，主要测试 HTTP 服务 的每秒请求数、请求延迟、吞吐量以及请求延迟的分布情况等。

      - ```shell
        1 # -c 表示并发请求数为 1000，-n 表示总的请求数为 10000
        2 $ ab -c 1000 -n 10000 http://192.168.0.30/
        ```

      - 可以看到，ab 的测试结果分为三个部分，分别是请求汇总、连接时间汇总还有请求延迟汇 总。以上面的结果为例，我们具体来看。

        - **Requests per second** 为 1074； 

          每个请求的延迟（Time per request）分为两行，第一行的 927 ms 表示平均延迟，包 括了线程运行的调度时间和网络请求响应时间，而下一行的 0.927ms ，则表示实际请求 的响应时间； 

        - **Transfer rate 表示吞吐量（BPS）**为 890 KB/s。 

          连接时间汇总部分，则是分别展示了建立连接、请求、等待以及汇总等的各类时间，包括最 小、最大、平均以及中值处理时间。 

        - 最后的**请求延迟汇总部分**，则给出了不同时间段内处理请求的百分比，比如， 90% 的请 求，都可以在 274ms 内完成。

  - 应用负载性能

    - 当你用 iperf 或者 ab 等测试工具，得到 TCP、HTTP 等的性能数据后，这些数据是否就能 表示应用程序的实际性能呢？我想，你的答案应该是否定的。

      - 比如，你的应用程序基于 HTTP 协议，为最终用户提供一个 Web 服务。这时，使用 ab 工 具，可以得到某个页面的访问性能，但这个结果跟用户的实际请求，很可能不一致。因为用户请求往往会附带着各种各种的负载（payload），而这些负载会影响 Web 应用程序内部 的处理逻辑，从而影响最终性能。

    - 幸运的是，我们还可以用 wrk、TCPCopy、Jmeter 或 者 LoadRunner 等实现这个目标。

      - wrk 的命令行参数比较简单。比如，我们可以用 wrk ，来重新测一下前面已经启动的 Nginx 的性能。

        - ```shell
          1 # -c 表示并发连接数 1000，-t 表示线程数为 2
          2 $ wrk -c 1000 -t 2 http://192.168.0.30/
          ```

        - 你可以看到，每秒请求 数为 9641，吞吐量为 7.82MB，平均延迟为 65ms，比前面 ab 的测试结果要好很多。

        



## 网络性能优化的几个思路

- **确定优化目标**

  - 实际上，虽然网络性能优化的整体目标，是降低网络延迟（如 RTT）和提高吞吐量（如 BPS 和 PPS），但具体到不同应用中，每个指标的优化标准可能会不同，优先级顺序也大 相径庭。 
    - NAT 网关来说，由于其直接影响整个数据中心的网络出入性能，所以 NAT 网关通常需要达到或接近线性转发，也就是说， PPS 是最主要的性能目标。 
    - 再如，对于数据库、缓存等系统，快速完成网络收发，即低延迟，是主要的性能目标。
    - 而对于我们经常访问的 Web 服务来说，则需要同时兼顾

- Linux 网络协议栈

  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105407329-759102076.png)
  - **首先是网络接口层和网络层**，它们主要负责网络包的封装、寻址、路由，以及发送和接收。 每秒可处理的网络包数 PPS，就是它们最重要的性能指标（特别是在小包的情况下）。你可以用内核自带的发包工具 pktgen ，来测试 PPS 的性能。
  - **再向上到传输层的 TCP 和 UDP**，它们主要负责网络传输。对它们而言，吞吐量（BPS）、 连接数以及延迟，就是最重要的性能指标。你可以用 iperf 或 netperf ，来测试传输层的性 能。 
    - 不过要注意，网络包的大小，会直接影响这些指标的值。所以，通常，你需要测试一系列不 同大小网络包的性能。
  - 最后，再往上到了应用层，最需要关注的是吞吐量（BPS）、每秒请求数以及延迟等指标。 你可以用 wrk、ab 等工具，来测试应用程序的性能

- 网络性能工具

  - 网络性能指标的工具
    - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105438150-732021018.png)
    - 性能工具
      - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105457114-2056652101.png)

- 应用程序

  - 从网络 I/O 的角度来说，主要有下面两种优化思路

    - 从网络 I/O 的角度来说，主要有下面两种优化思路。 第一种是最常用的 I/O 多路复用技术 epoll，主要用来取代 select 和 poll。这其实是解决 C10K 问题的关键，也是目前很多网络应用默认使用的机制。
    - 第二种是使用异步 I/O（Asynchronous I/O，AIO）。AIO 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。等到 I/O 完成后，系统会用事件通知的方式，告诉应 用程序结果。不过，AIO 的使用比较复杂，你需要小心处理很多边缘情况

  - 而从进程的工作模型来说，也有两种不同的模型用来优化

    - 第一种，主进程 + 多个 worker 子进程。其中，主进程负责管理网络连接，而子进程负责 实际的业务处理。这也是最常用的一种模型。 

      第二种，监听到相同端口的多进程模型。在这种模型下，所有进程都会监听相同接口，并且 开启 SO_REUSEPORT 选项，由内核负责，把请求负载均衡到这些监听进程中去。 

  - 应用层的网络协议优化

    - 使用长连接取代短连接，可以显著降低 TCP 建立连接的成本。在每秒请求次数较多时， 这样做的效果非常明显。 
    - 使用内存等方式，来缓存不常变化的数据，可以降低网络 I/O 次数，同时加快应用程序 的响应速度。 
    - 使用 Protocol Buffer 等序列化的方式，压缩网络 I/O 的数据量，可以提高应用程序的吞 吐。 
    - 使用 DNS 缓存、预取、HTTPDNS 等方式，减少 DNS 解析的延迟，也可以提升网络 I/O 的整体速度

- 套接字

  - 套接字可以屏蔽掉 Linux 内核中不同协议的差异，为应用程序提供统一的访问接口。每个 套接字，都有一个读写缓冲区。 

    - 读缓冲区，缓存了远端发过来的数据。如果读缓冲区已满，就不能再接收新的数据。 
    - 写缓冲区，缓存了要发出去的数据。如果写缓冲区已满，应用程序的写操作就会被阻塞。

  - > 增大每个套接字的缓冲区大小 net.core.optmem_max； 
    >
    > 增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 
    >
    > net.core.wmem_max； 
    >
    > 增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 
    >
    > net.ipv4.tcp_wmem。
    >
    > 
    >
    > 发送缓冲区大小，理想数值是吞吐量 * 延迟，这样才可以达到最大网络利用 率

- 传输层

  - 第一类，在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它 们会占用大量内存和端口资源。这时，我们可以优化与 TIME_WAIT 状态相关的内核选 项，比如采取下面几种措施。
    - 增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max。 
    - 减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源。 
    - 开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用 到新建的连接中。 
    - 增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整 体的并发能力。 
    - 增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和 
    - 系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数。
  - 第二类，为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，你可 以考虑优化与 SYN 状态相关的内核选项，比如采取下面几种措施。 
    - 增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项 不可同时使用）。 
    - 减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries。 
  - 第三类，在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接 断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法 满足应用程序的性能要求。所以，这时候你需要优化与 Keepalive 相关的内核选项，比 如
    - 缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time；缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl； 
    - 减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数net.ipv4.tcp_keepalive_probes。 
  - UDP 的优化
    - UDP 提供了面向数据报的网络协议，它不需要网络连接，也不提供可靠性保障。所以， UDP 优化，相对于 TCP 来说，要简单得多。这里我也总结了常见的几种优化方案。 
      - 跟上篇套接字部分提到的一样，增大套接字缓冲区大小以及 UDP 缓冲区范围； 
      - 跟前面 TCP 部分提到的一样，增大本地端口号的范围； 
      - 根据 MTU 大小，调整 UDP 数据包的大小，减少或者避免分片的发生。

-  网络层

  - 网络层，负责网络包的封装、寻址和路由，包括 IP、ICMP 等常见协议。在网络层，最主 要的优化，其实就是对路由、 IP 分片以及 ICMP 等进行调优。 
  - 第一种，从路由和转发的角度出发，你可以调整下面的内核选项。 
    - 在需要转发的服务器中，比如用作 NAT 网关的服务器或者使用 Docker 容器时，开启 IP 转发，即设置 net.ipv4.ip_forward = 1。 
    - 调整数据包的生存周期 TTL，比如设置 net.ipv4.ip_default_ttl = 64。注意，增大该值会 降低系统性能。 
    - 开启数据包的反向地址校验，比如设置 net.ipv4.conf.eth0.rp_filter = 1。这样可以防止 IP 欺骗，并减少伪造 IP 带来的 DDoS 问题。
  - 第二种，从分片的角度出发，最主要的是调整 MTU（Maximum Transmission Unit）的 大小。 
    - 通常，MTU 的大小应该根据以太网的标准来设置。以太网标准规定，一个网络帧最大为 1518B，那么去掉以太网头部的 18B 后，剩余的 1500 就是以太网 MTU 的大小。在使用 VXLAN、GRE 等叠加网络技术时，要注意，网络叠加会使原来的网络包变大，导致 MTU 也需要调整。 
    - 比如，就以 VXLAN 为例，它在原来报文的基础上，增加了 14B 的以太网头部、 8B 的 VXLAN 头部、8B 的 UDP 头部以及 20B 的 IP 头部。换句话说，每个包比原来增大了 50B。 
    - 所以，我们就需要把交换机、路由器等的 MTU，增大到 1550， 或者把 VXLAN 封包前 （比如虚拟化环境中的虚拟网卡）的 MTU 减小为 1450。

  - 第三种，从 ICMP 的角度出发，为了避免 ICMP 主机探测、ICMP Flood 等各种网络问 题，你可以通过内核选项，来限制 ICMP 的行为。 
    - 比如，你可以禁止 ICMP 协议，即设置 net.ipv4.icmp_echo_ignore_all = 1。这样，外 部主机就无法通过 ICMP 来探测主机。 
    - 或者，你还可以禁止广播 ICMP，即设置 net.ipv4.icmp_echo_ignore_broadcasts = 1。

## 服务吞吐量下降很厉害，怎么分析

- 测试一下，案例中 Nginx 的吞吐量。

  - ```shell
    1 # 默认测试时间为 10s，请求超时 2s
    2 $ wrk --latency -c 1000 http://192.168.0.30
    
     1910 requests in 10.10s, 573.56KB read
     Non-2xx or 3xx responses: 1910
    ```

  - 从 wrk 的结果中，你可以看到吞吐量（也就是每秒请求数）只有 189，并且所有 1910 个 请求收到的都是异常响应（非 2xx 或 3xx）。这些数据显然表明，吞吐量太低了，并且请 求处理都失败了。这是怎么回事呢？

  - 根据 wrk 输出的统计结果，我们可以看到，总共传输的数据量只有 573 KB，那就肯定不会 是带宽受限导致的。所以，我们应该从请求数的角度来分析。 分析请求数，特别是 HTTP 的请求数，有什么好思路吗？当然就要**从 TCP 连接数入手**。

- 连接数优化

  - 要查看 TCP 连接数的汇总情况，首选工具自然是 ss 命令。为了观察 wrk 测试时发生的问 题，我们在终端二中再次启动 wrk，并且把总的测试时间延长到 30 分钟

    - ```shell
      
      # 测试时间 30 分钟
      $ wrk --latency -c 1000 -d 1800 http://192.168.0.30
      
      #观察 TCP 连接数
      $ ss -s
      ```

    - 从这里看出，wrk 并发 1000 请求时，建立连接数只有 5，而 closed 和 timewait 状态的 连接则有 1100 多 。其实从这儿你就可以发现两个问题

      - 一个是建立连接数太少了； 
      - 另一个是 timewait 状态连接太多了

    - 分析问题，自然要先从相对简单的下手。我们先来看第二个关于 timewait 的问题。在之前 的 NAT 案例中，我已经提到过，内核中的连接跟踪模块，有可能会导致 timewait 问题。 我们今天的案例还是基于 Docker 运行，而 Docker 使用的 iptables ，就会使用**连接跟踪 模块**来管理 NAT。那么，怎么确认是不是连接跟踪导致的问题呢？

      - 其实，最简单的方法，就是通过 dmesg 查看系统日志，如果有连接跟踪出了问题，应该会 看到 nf_conntrack 相关的日志

        - $ dmesg | tail

        - 从日志中，你可以看到 nf_conntrack: table full, dropping packet 的错误日志。这说明， 正是连接跟踪导致的问题。 

        - 这种情况下，我们应该想起前面学过的两个内核选项——连接跟踪数的最大限制 nf_conntrack_max ，以及当前的连接跟踪数 nf_conntrack_count。

        - 这次的输出中，你可以看到最大的连接跟踪限制只有 200，并且全部被占用了。200 的限 制显然太小，不过相应的优化也很简单，调大就可以了。

        - ```shell
          # 我们执行下面的命令，将 nf_conntrack_max 增大：
          # 将连接跟踪限制增大到 1048576
           $ sysctl -w net.netfilter.nf_conntrack_max=1048576
          ```

      - 从 wrk 的输出中，你可以看到，连接跟踪的优化效果非常好，吞吐量已经从刚才的 189 增 大到了 5382。看起来性能提升了将近 30 倍

        - 别急，我们再来看看 wrk 汇报的其他数据。果然，10s 内的总请求数虽然增大到了 5 万， 但是有 4 万多响应异常，说白了，真正成功的只有 8000 多个（54221-45577=8644）

- 工作进程优化

  - 由于这些响应并非 Socket error，说明 Nginx 已经收到了请求，只不过，响应的状态码并 不是我们期望的 2xx （表示成功）或 3xx（表示重定向）。所以，这种情况下，搞清楚 Nginx 真正的响应就很重要了。 

    - ```shell
      $ docker logs nginx --tail 3
      ```

    - 从 Nginx 的日志中，我们可以看到，响应状态码为 499。499 并非标准的 HTTP 状态码，而是由 Nginx 扩展而来，表示服务器端还没来得及响应 时，客户端就已经关闭连接了。换句话说，问题在于服务器端处理太慢，客户端因为超时 （wrk 超时时间为 2s），主动断开了连接。

    - 既然问题出在了服务器端处理慢，而案例本身是 Nginx+PHP 的应用，那是不是可以猜 测，是因为 PHP 处理过慢呢

    - ```shell
      # 查询 PHP 容器日志
      $ docker logs phpfpm --tail 5
      ```

    - 从这个日志中，我们可以看到两条警告信息，server reached max_children setting (5)， 并建议增大 max_children。

      - 一般来说，每个 php-fpm 子进程可能会占用 20 MB 左右的内存。所以，你可以根据内存 和 CPU 个数，估算一个合理的值。这儿我把它设置成了 20，并将优化后的配置重新打包 成了 Docker 镜像。

      - ```shell
         # 停止旧的容器
         $ docker rm -f nginx phpfpm
         # 使用新镜像启动 Nginx 和 PHP
         $ docker run --name nginx --network host --privileged -itd feisky/nginx-tp:1
         $ docker run --name phpfpm --network host --privileged -itd feisky/php-fpm-tp:1
        ```

    - 从 wrk 的输出中，可以看到，虽然吞吐量只有 4683，比刚才的 5382 少了一些；但是测试 期间成功的请求数却多了不少，从原来的 8000，增长到了 15000（47210- 31692=15518）。 

- 套接字优化

  - 观察有没有发生套接字的丢包现象

    - ```shell
      1 # 只关注套接字统计
      2 $ netstat -s | grep socket
      ```

    - 根据两次统计结果中 socket overflowed 和 sockets dropped 的变化，你可以看到，有大 量的套接字丢包，并且丢包都是套接字队列溢出导致的。所以，接下来，我们应该分析连接 队列的大小是不是有异常。

  - 查看套接字的队列大小$ ss -ltnp

    - 这次可以看到，Nginx 和 php-fpm 的监听队列 （Send-Q）只有 10，而 nginx 的当前监 听队列长度 （Recv-Q）已经达到了最大值，php-fpm 也已经接近了最大值。很明显，套 接字监听队列的长度太小了，需要增大

  - 关于套接字监听队列长度的设置，既可以在应用程序中，通过套接字接口调整，也支持通过 内核选项来配置

    - ```shell
      1 # 查询 nginx 监听队列长度配置
      2 $ docker exec nginx cat /etc/nginx/nginx.conf | grep backlog
      3 listen 80 backlog=10;
      45 # 查询 php-fpm 监听队列长度
      6 $ docker exec phpfpm cat /opt/bitnami/php/etc/php-fpm.d/www.conf | grep backlog
      7 ; Set listen(2) backlog.
      8 ;listen.backlog = 511
      9
      10 # somaxconn 是系统级套接字监听队列上限
      11 $ sysctl net.core.somaxconn
      12 net.core.somaxconn = 10
      ```

    - 从输出中可以看到，Nginx 和 somaxconn 的配置都是 10，而 php-fpm 的配置也只有 511，显然都太小了。那么，优化方法就是增大这三个配置，比如，可以把 Nginx 和 phpfpm 的队列长度增大到 8192，而把 somaxconn 增大到 65536

    - 现在的吞吐量已经增大到了 6185，并且在测试的时候，如果你在终端一中重新执行 netstat -s | grep socket，还会发现，现在已经没有套接字丢包问题了

  - 不过，这次 Nginx 的响应，再一次全部失败了，都是 Non-2xx or 3xx

    - 你可以看到，Nginx 报出了无法连接 fastcgi 的错误，错误消息是 Connect 时， Cannot assign requested address。这个错误消息对应的错误代码为 EADDRNOTAVAIL，表示 IP 地址或者端口号不可用。 
    - 在这里，显然只能是端口号的问题

- 端口号优化

  - 我们执行下面的命令，就可以查询系统配置的临时端口号范围

    - ```shell
      1 $ sysctl net.ipv4.ip_local_port_range
      2 net.ipv4.ip_local_port_range=20000 20050
      ```

  - 你可以看到，临时端口的范围只有 50 个，显然太小了 。优化方法很容易想到，增大这个 范围就可以了。比如，你可以执行下面的命令，把端口号范围扩展为 “10000 65535”：

    - ```shell
      1 $ sysctl -w net.ipv4.ip_local_port_range="10000 65535"
      2 net.ipv4.ip_local_port_range = 10000 65535
      ```

  - 这次，异常的响应少多了 ，不过，吞吐量也下降到了 3208。并且，这次还出现了很多 Socket read errors。显然，还得进一步优化。

- 火焰图

  - 前面我们已经优化了很多配置。这些配置在优化网络的同时，却也会带来其他资源使用的上 升

  - 执行 top ，观察 CPU 和内存的使用

  - 从 top 的结果中可以看到，可用内存还是很充足的，但系统 CPU 使用率（sy）比较高，两 个 CPU 的系统 CPU 使用率都接近 50%，且空闲 CPU 使用率只有 2%。再看进程部分， CPU 主要被两个 Nginx 进程和两个 docker 相关的进程占用，使用率都是 30% 左右。

    - CPU 使用率上升了，该怎么进行分析呢？我想，你已经还记得我们多次用到的 perf，再配 合前两节讲过的火焰图，很容易就能找到系统中的热点函数。 

    - ```shell
      1 # 执行 perf 记录事件
      2 $ perf record -g
      34 # 切换到 FlameGraph 安装路径执行下面的命令生成火焰图
      ```

  - 根据我们讲过的火焰图原理，这个图应该从下往上、沿着调用栈中最宽的函数，来分析执行 次数最多的函数。 

  - 如果有大量连接 占用着端口，那么检查端口号可用的函数，不就会消耗更多的 CPU 吗

    -  $ ss -s

    - 这回可以看到，有大量连接（这儿是 32768）处于 timewait 状态，而 timewait 状态的连 接，本身会继续占用端口号。如果这些端口号可以重用，那么自然就可以缩短 __init_check_established 的过程。而 Linux 内核中，恰好有一个 tcp_tw_reuse 选项，用 来控制端口号的重用。

      ```shell
      # 我们在终端一中，运行下面的命令，查询它的配置：
      
      1 $ sysctl net.ipv4.tcp_tw_reuse
      2 net.ipv4.tcp_tw_reuse = 0
      # 你可以看到，tcp_tw_reuse 是 0，也就是禁止状态。其实看到这里，我们就能理解，为什么临时端口号的分配会是系统运行的热点了。当然，优化方法也很容易，把它设置成 1 就可以开启了。
      ```

      



## **分析性能问题的一般步骤**

- **系统资源瓶颈**
  - 使用率、饱和度以及错误数这三类指标来衡量。系统的资源，可以分为硬件资源和软件资源两 类。
    - 如 CPU、内存、磁盘和文件系统以及网络等，都是最常见的硬件资源。 
    - 而文件描述符数、连接跟踪数、套接字缓冲区大小等，则是典型的软件资源。
- **CPU** **性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925163759359-2129947612.png)
  - 比如说，当你收到系统的用户 CPU 使用率过高告警时，从监控系统中直接查询到，导致 CPU 使用率过高的进程；然后再登录到进程所在的 Linux 服务器中，分析该进程的行为。 
  - 你可以使用 strace，查看进程的系统调用汇总；也可以使用 perf 等工具，找出进程的热点 函数；甚至还可以使用动态追踪的方法，来观察进程的当前执行过程，直到确定瓶颈的根 源。

- **内存性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925164214302-575822803.png)
  - 比如说，当你收到内存不足的告警时，首先可以从监控系统中。找出占用内存最多的几个进 程。然后，再根据这些进程的内存占用历史，观察是否存在内存泄漏问题。
  - 确定出最可疑的 进程后，再登录到进程所在的 Linux 服务器中，分析该进程的内存空间或者内存分配，最 后弄清楚进程为什么会占用大量内存
- **磁盘和文件系统** **I/O** **性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925164238972-714799998.png)
  - 当你使用 iostat ，发现磁盘 I/O 存在性能瓶颈（比如 I/O 使用率过 高、响应时间过长或者等待队列长度突然增大等）后，可以再通过 pidstat、 vmstat 等， 确认 I/O 的来源。接着，再根据来源的不同，进一步分析文件系统和磁盘的使用率、缓存 以及进程的 I/O 等，从而揪出 I/O 问题的真凶
  - 比如说，当你发现某块磁盘的 I/O 使用率为 100% 时，首先可以从监控系统中，找出 I/O 最多的进程。然后，再登录到进程所在的 Linux 服务器中，借助 strace、lsof、perf 等工具，分析该进程的 I/O 行为。最后，再结合应用程序的原理，找出大量 I/O 的原因。 

- **网络性能分析**
  - 最后的网络性能，其实包含两类资源，即网络接口和内核资源
  - 而要分析网络的性能，自然也是要从这几个协议层入手，通过使用率、饱和度以及错误数这 几类性能指标，观察是否存在性能问题。比如 ： 
    - 在链路层，可以从网络接口的吞吐量、丢包、错误以及软中断和网络功能卸载等角度分 析； 
    - 在网络层，可以从路由、分片、叠加网络等角度进行分析； 
    - 在传输层，可以从 TCP、UDP 的协议原理出发，从连接数、吞吐量、延迟、重传等角度 进行分析；在应用层，可以从应用层协议（如 HTTP 和 DNS）、请求数（QPS）、套接字缓存等角 度进行分析。 
  - 比如，当你收到网络不通的告警时，就可以从监控系统中，查找各个协议层的丢包指标，确 认丢包所在的协议层。然后，从监控系统的数据中，确认网络带宽、缓冲区、连接跟踪数等 软硬件，是否存在性能瓶颈。最后，再登录到发生问题的 Linux 服务器中，借助 netstat、 tcpdump、bcc 等工具，分析网络的收发数据，并且结合内核中的网络选项以及 TCP 等网 络协议的原理，找出问题的来源

- **应用程序瓶颈**
  - 除了以上这些来自网络资源的瓶颈外，还有很多瓶颈，其实直接来自应用程序。比如，最典 型的应用程序性能问题，就是吞吐量（并发请求数）下降、错误率升高以及响应时间增大。 
  - 不过，在我看来，这些应用程序性能问题虽然各种各样，但就其本质来源，实际上只有三 种，也就是资源瓶颈、依赖服务瓶颈以及应用自身的瓶颈。 
    - 第一种**资源瓶颈**，其实还是指刚才提到的 CPU、内存、磁盘和文件系统 I/O、网络以及内 核资源等各类软硬件资源出现了瓶颈，从而导致应用程序的运行受限。对于这种情况，我们 就可以用前面系统资源瓶颈模块提到的各种方法来分析。 
    - 第二种**依赖服务的瓶颈**，也就是诸如数据库、分布式缓存、中间件等应用程序，直接或者间 接调用的服务出现了性能问题，从而导致应用程序的响应变慢，或者错误率升高。这说白了 就是跨应用的性能问题，使用全链路跟踪系统，就可以帮你快速定位这类问题的根源。 
    - 最后一种，**应用程序自身的性能问题**，包括了多线程处理不当、死锁、业务算法的复杂度过 高等等。对于这类问题，在我们前面讲过的应用程序指标监控以及日志监控中，观察关键环 节的耗时和内部执行过程中的错误，就可以帮你缩小问题的范围。 
  - 不过，由于这是应用程序内部的状态，外部通常不能直接获取详细的性能数据，所以就需要 应用程序在设计和开发时，就提供出这些指标，以便监控系统可以了解应用程序的内部运行状态。 
  - 如果这些手段过后还是无法找出瓶颈，你还可以用系统资源模块提到的各类进程分析工具， 来进行分析定位。比如： 
    - 你可以用 strace，观察系统调用； 
    - 使用 perf 和火焰图，分析热点函数； 
    - 甚至使用动态追踪技术，来分析进程的执行状态。
  - 当然，系统资源和应用程序本来就是相互影响、相辅相成的一个整体。实际上，很多资源瓶 颈，也是应用程序自身运行导致的。比如，进程的内存泄漏，会导致系统内存不足；进程过 多的 I/O 请求，会拖慢整个系统的 I/O 请求等。



## **优化性能问题的一般方法**

- 我们可以从系统资源瓶颈和应用程序瓶颈，这两个角度来分析性能问题的根源
  - 从系统资源瓶颈的角度来说，USE 法是最为有效的方法，即从使用率、饱和度以及错误数 这三个方面，来分析 CPU、内存、磁盘和文件系统 I/O、网络以及内核资源限制等各类软硬件资源。
  - 从应用程序瓶颈的角度来说，可以把性能问题的来源，分为资源瓶颈、依赖服务瓶颈以及应 用自身的瓶颈这三类。 
  - 资源瓶颈的分析思路，跟系统资源瓶颈是一样的。 
  - 依赖服务的瓶颈，可以使用全链路跟踪系统，进行快速定位。 
  - 而应用自身的问题，则可以通过系统调用、热点函数，或者应用自身的指标和日志等，进 行分析定位。 

- **CPU** **优化**
  - CPU 性能优化的核心，在于排除所有不必要的工作、充分利用 CPU 缓存并减少进程调度对性能的*影响
  - 最典型 的三种优化方法。
  - 第一种，把进程绑定到一个或者多个 CPU 上，充分利用 CPU 缓存的本地性，并减少进 程间的相互影响。 
  - 第二种，为中断处理程序开启多 CPU 负载均衡，以便在发生大量中断时，可以充分利用多 CPU 的优势分摊负载。 
  - 第三种，使用 Cgroups 等方法，为进程设置资源限制，避免个别进程消耗过多的 CPU。 同时，为核心应用程序设置更高的优先级，减少低优先级任务的影响。 
  
- **内存优化**
  - 内存问题，比如可用内存不足、内存泄漏、 Swap 过多、缺页异常过多以及缓存过多等等。所以，说白了，内存性能的优化，也就是要 解决这些内存使用的问题。
  - 你可以通过以下几种方法
    - 第一种，除非有必要，Swap 应该禁止掉。这样就可以避免 Swap 的额外 I/O ，带来内 存访问变慢的问题。 
    - 第二种，使用 Cgroups 等方法，为进程设置内存限制。这样就可以避免个别进程消耗过多内存，而影响了其他进程。对于核心应用，还应该降低 oom_score，避免被 OOM 杀 死。 
    - 第三种，使用大页、内存池等方法，减少内存的动态分配，从而减少缺页异常。
  
- **磁盘和文件系统** **I/O** **优化**
  - 三种最典型的方 法。
    - 第一种，也是最简单的方法，通过 SSD 替代 HDD、或者使用 RAID 等方法，提升 I/O性能。 
    - 第二种，针对磁盘和应用程序 I/O 模式的特征，选择最适合的 I/O 调度算法。比如， SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法；而数据库应用，更推荐使用deadline 算法。 
    - 第三，优化文件系统和磁盘的缓存、缓冲区，比如优化脏页的刷新频率、脏页限额，以及 内核回收目录项缓存和索引节点缓存的倾向等等。
  - 除此之外，使用不同磁盘隔离不同应用的数据、优化文件系统的配置选项、优化磁盘预读、 增大磁盘队列长度等，也都是常用的优化思路。
  
- **网络优化**
  - 针对每个协议层的工作原理 进行优化。这里，我同样强调一下，最典型的几种网络优化方法。
    - 首先，从内核资源和网络协议的角度来说，我们可以对内核选项进行优化，比如： 
      - 你可以增大套接字缓冲区、连接跟踪表、最大半连接数、最大文件描述符数、本地端口范 围等内核资源配额； 
      - 也可以减少 TIMEOUT 超时时间、SYN+ACK 重传数、Keepalive 探测时间等异常处理 参数； 
      - 还可以开启端口复用、反向地址校验，并调整 MTU 大小等降低内核的负担。 
    - 其次，从网络接口的角度来说，我们可以考虑对网络接口的功能进行优化，比如： 
      - 你可以将原来 CPU 上执行的工作，卸载到网卡中执行，即开启网卡的 GRO、GSO、 RSS、VXLAN 等卸载功能； 
      - 也可以开启网络接口的多队列功能，这样，每个队列就可以用不同的中断号，调度到不同 CPU 上执行； 
      - 还可以增大网络接口的缓冲区大小以及队列长度等，提升网络传输的吞吐量。
  
- **应用程序优化**

  - 第一个例子，是系统 CPU 使用率（sys%）过高的问题。有时候出现问题，虽然表面现象是系统 CPU 使用率过高，但待你分析过后，很可能会发现，应用程序的不合理系统调用才是罪魁祸首。这种情况下，优化应用程序内部系统调用的逻辑，显然要比优化内核要简单也有 用得多。 
  - 再比如说，数据库的 CPU 使用率高、I/O 响应慢，也是最常见的一种性能问题。这种问 题，一般来说，并不是因为数据库本身性能不好，而是应用程序不合理的表结构或者 SQL查询语句导致的。这时候，优化应用程序中数据库表结构的逻辑或者 SQL 语句，显然要比优化数据库本身，能带来更大的收益。
  - 所以，在观察性能指标时，你应该先查看**应用程序的响应时间、吞吐量以及错误率**等指标，因为它们才是性能优化要解决的终极问题。以终为始，从这些角度出发，你一定能想到很多优化方法，而我比较推荐下面几种方法。
    - 第一，从 CPU 使用的角度来说，简化代码、优化算法、异步处理以及编译器优化等，都 是常用的降低 CPU 使用率的方法，这样可以利用有限的 CPU 处理更多的请求。 
    - 第二，从数据访问的角度来说，使用缓存、写时复制、增加 I/O 尺寸等，都是常用的减 少磁盘 I/O 的方法，这样可以获得更快的数据处理速度。
    - 第三，从内存管理的角度来说，使用大页、内存池等方法，可以预先分配内存，减少内存的动态分配，从而更好地内存访问性能。 
    - 第四，从网络的角度来说，使用 I/O 多路复用、长连接代替短连接、DNS 缓存等方法，可以优化网络 I/O 并减少网络请求数，从而减少网络延时带来的性能问题。 
    - 第五，从进程的工作模型来说，异步处理、多线程或多进程等，可以充分利用每一个 CPU 的处理能力，从而提高应用程序的吞吐能力。 
    - 除此之外，你还可以使用消息队列、CDN、负载均衡等各种方法，来优化应用程序的架构，将原来单机要承担的任务，调度到多台服务器中并行处理。这样也往往能获得更好的整体性能

  























