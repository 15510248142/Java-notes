



## Term & Phrase Suggester

什么是搜索建议
	帮助⽤户在输⼊搜索的过程中，进⾏⾃动补全或者纠错。通过协助⽤户输⼊更加精准的关键词，提⾼后续搜索阶段⽂档匹配的程度
	在 google 上搜索，⼀开始会⾃动补全。当输⼊到⼀定⻓度，如因为单词拼写错误⽆法补全，就会开始提示相似的词或者句⼦

Elasticsearch Suggester API
	搜索引擎中类似的功能，在 Elasticsearch 中是通过 Suggester API 实现的
	原理：将输⼊的⽂本分解为 Token，然后在索引的字典⾥查找相似的 Term 并返回
	根据不同的使⽤场景，Elasticsearch 设计了 4 种类别的 Suggesters
		Term & Phrase Suggester
		Complete & Context Suggester	



DELETE articles
PUT articles
{
  "mappings": {
    "properties": {
      "title_completion":{
        "type": "completion"
      }
    }
  }
}


# ⼀些测试数据
	默认使⽤ standard 分词器
		⼤写转⼩写
		rocks 和 rock 是两个词

POST articles/_bulk
{ "index" : { } }
{ "title_completion": "lucene is very cool"}
{ "index" : { } }
{ "title_completion": "Elasticsearch builds on top of lucene"}
{ "index" : { } }
{ "title_completion": "Elasticsearch rocks"}
{ "index" : { } }
{ "title_completion": "elastic is the company behind ELK stack"}
{ "index" : { } }
{ "title_completion": "Elk stack rocks"}
{ "index" : {} }


POST articles/_search?pretty
{
  "size": 0,
  "suggest": {
    "article-suggester": {
      "prefix": "elk ",
      "completion": {
        "field": "title_completion"
      }
    }
  }
}

DELETE articles

POST articles/_bulk
{ "index" : { } }
{ "body": "lucene is very cool"}
{ "index" : { } }
{ "body": "Elasticsearch builds on top of lucene"}
{ "index" : { } }
{ "body": "Elasticsearch rocks"}
{ "index" : { } }
{ "body": "elastic is the company behind ELK stack"}
{ "index" : { } }
{ "body": "Elk stack rocks"}
{ "index" : {} }
{  "body": "elasticsearch is rock solid"}


POST _analyze
{
  "analyzer": "standard",
  "text": ["Elk stack  rocks rock"]
}


# Term Suggester
	Suggester 就是⼀种特殊类型的搜索。”text” ⾥是调⽤时候提供的⽂本，通常来⾃于⽤户界⾯上⽤户输⼊的内容
	⽤户输⼊的 “lucen” 是⼀个错误的拼写
	会到 指定的字段 “body” 上搜索，当⽆法搜索到结果时 (missing)，返回建议的词

	搜索 “lucen rock”：
		每个建议都包含了⼀个算分，相似性是通过 Levenshtein Edit Distance 的算法实现的。核⼼思想就是⼀个词改动多少字符就可以和另外⼀个词⼀致。 提供了很多可选参数来控制相似性的模糊程度。例如 “max_edits”
	⼏种 Suggestion Mode	
		Missing – 如索引中已经存在，就不提供建议
		Popular – 推荐出现频率更加⾼的词
		Always – ⽆论是否存在，都提供建议


POST /articles/_search
{
  "size": 1,
  "query": {
    "match": {
      "body": "lucen rock"
    }
  },
  "suggest": {
    "term-suggestion": {
      "text": "lucen rock",
      "term": {
        "suggest_mode": "missing",
        "field": "body"
      }
    }
  }
}

POST /articles/_search
{

  "suggest": {
    "term-suggestion": {
      "text": "lucen rock",
      "term": {
        "suggest_mode": "popular",
        "field": "body"
      }
    }
  }
}

POST /articles/_search
{

  "suggest": {
    "term-suggestion": {
      "text": "lucen rock",
      "term": {
        "suggest_mode": "always",
        "field": "body",
      }
    }
  }
}


# Sorting by Frequency & Prefix Length
	默认按照 score 排序，也可以按照“frequency”
	默认⾸字⺟不⼀致就不会匹配推荐，但是如果将 prefix_length 设置为 0，就会为 hock 建议 rock


POST /articles/_search
{

  "suggest": {
    "term-suggestion": {
      "text": "lucen hocks",
      "term": {
        "suggest_mode": "always",
        "field": "body",
        "prefix_length":0,
        "sort": "frequency"
      }
    }
  }
}


# Phrase Suggester

Phrase Suggester 在 Term Suggester 上增加了⼀些额外的逻辑
⼀些参数
	Suggest Mode ：missing, popular, always
	Max Errors：最多可以拼错的 Terms 数
	Confidence：限制返回结果数，默认为 1
direct_generator:候选生成器
phrase suggestion API接受关键字direct_generator下的生成器列表，该列表中的每个生成器在原始文本中均按术语被调用。
field	从中获取候选建议的字段。这是必需选项，需要全局设置或根据建议设置。
suggest_mode 建议模式

POST /articles/_search
{
  "suggest": {
    "my-suggestion": {
      "text": "lucne and elasticsear rock hello world ",
      "phrase": {
        "field": "body",
        "max_errors":2,
        "confidence":0,
        "direct_generator":[{
          "field":"body",
          "suggest_mode":"always"
        }],
        "highlight": {
          "pre_tag": "<em>",
          "post_tag": "</em>"
        }
      }
    }
  }
}





------------


## 自动补全与基于上下文的提示 

The Completion Suggester
	Completion Suggester 提供了“⾃动完成” (Auto Complete) 的功能。⽤户每输⼊⼀个字符，就需要即时发送⼀个查询请求到后段查找匹配项
	对性能要求⽐较苛刻。Elasticsearch 采⽤了不同的数据结构，并⾮通过倒排索引来完成。⽽是将 Analyze 的数据编码成 FST 和索引⼀起存放。FST 会被 ES 整个加载进内存，速度很快
	FST 只能⽤于前缀查找




# 使⽤ Completion Suggester 的⼀些步骤
	定义 Mapping，使⽤ “completion” type
	索引数据
	运⾏ “suggest” 查询，得到搜索建议

DELETE articles
PUT articles
{
  "mappings": {
    "properties": {
      "title_completion":{
        "type": "completion"
      }
    }
  }
}

POST articles/_bulk
{ "index" : { } }
{ "title_completion": "lucene is very cool"}
{ "index" : { } }
{ "title_completion": "Elasticsearch builds on top of lucene"}
{ "index" : { } }
{ "title_completion": "Elasticsearch rocks"}
{ "index" : { } }
{ "title_completion": "elastic is the company behind ELK stack"}
{ "index" : { } }
{ "title_completion": "Elk stack rocks"}
{ "index" : {} }


POST articles/_search
{
  "size": 0,
  "suggest": {
    "article-suggester": {
      "prefix": "elk ",
      "completion": {
        "field": "title_completion"
      }
    }
  }
}


# Context Suggester

Completion Suggester 的扩展
	可以在搜索中加⼊更多的上下⽂信息，例如，输⼊ “star”
		咖啡相关：建议 “Starbucks”
		电影相关：”star wars”

实现 Context Suggester
	可以定义两种类型的 Context
		Category – 任意的字符串
		Geo – 地理位置信息	

实现 Context Suggester 的具体步骤
	定制⼀个 Mapping
	索引数据，并且为每个⽂档加⼊ Context 信息
	结合 Context 进⾏ Suggestion 查询



# 定义 Mapping
	增加 Contexts
		Type
		name	

DELETE comments
PUT comments
PUT comments/_mapping
{
  "properties": {
    "comment_autocomplete":{
      "type": "completion",
      "contexts":[{
        "type":"category",
        "name":"comment_category"
      }]
    }
  }
}




POST comments/_doc
{
  "comment":"I love the star war movies",
  "comment_autocomplete":{
    "input":["star wars"],
    "contexts":{
      "comment_category":"movies"
    }
  }
}

POST comments/_doc
{
  "comment":"Where can I find a Starbucks",
  "comment_autocomplete":{
    "input":["starbucks"],
    "contexts":{
      "comment_category":"coffee"
    }
  }
}


POST comments/_search
{
  "suggest": {
    "MY_SUGGESTION": {
      "prefix": "sta",
      "completion":{
        "field":"comment_autocomplete",
        "contexts":{
          "comment_category":"coffee"
        }
      }
    }
  }
}


# 精准度和召回率 

精准度
	Completion > Phrase > Term
召回率
	Term > Phrase > Completion
性能
	Completion > Phrase > Term




-----------

## 跨集群搜索

⽔平扩展的痛点
	单集群 – 当⽔平扩展时，节点数不能⽆限增加
	当集群的 meta 信息（节点，索引，集群状态）过多，会导致更新压⼒变⼤，单个 Active Master 会成为性能瓶颈，导致整个集群⽆法正常⼯作

跨集群搜索 - Cross Cluster Search
	Elasticsearch 5.3 引⼊了跨集群搜索的功能(Cross Cluster Search)，推荐使⽤
	允许任何节点扮演 federated 节点，以轻量的⽅式，将搜索请求进⾏代理
	不需要以 Client Node 的形式加⼊其他集群


//启动3个集群

bin/elasticsearch -E node.name=cluster0node -E cluster.name=cluster0 -E path.data=cluster0_data -E discovery.type=single-node -E http.port=9200 -E transport.port=9300
bin/elasticsearch -E node.name=cluster1node -E cluster.name=cluster1 -E path.data=cluster1_data -E discovery.type=single-node -E http.port=9201 -E transport.port=9301
bin/elasticsearch -E node.name=cluster2node -E cluster.name=cluster2 -E path.data=cluster2_data -E discovery.type=single-node -E http.port=9202 -E transport.port=9302



配置跨集群搜索
每个集群创建相同的索引名，并写⼊数据
跨集群搜索
在 Kibana 的 Index Pattern 中配置跨集群搜索


# 配置及查询 
在每个集群上设置动态的设置

PUT _cluster/settings
{
  "persistent": {
    "cluster": {
      "remote": {
        "cluster0": {
          "seeds": [
            "127.0.0.1:9300"
          ],
          "transport.ping_schedule": "30s"
        },
        "cluster1": {
          "seeds": [
            "127.0.0.1:9301"
          ],
          "transport.compress": true,
          "skip_unavailable": true
        },
        "cluster2": {
          "seeds": [
            "127.0.0.1:9302"
          ]
        }
      }
    }
  }
}

#cURL
curl -XPUT "http://localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{"persistent":{"cluster":{"remote":{"cluster0":{"seeds":["127.0.0.1:9300"],"transport.ping_schedule":"30s"},"cluster1":{"seeds":["127.0.0.1:9301"],"transport.compress":true,"skip_unavailable":true},"cluster2":{"seeds":["127.0.0.1:9302"]}}}}}'

curl -XPUT "http://localhost:9201/_cluster/settings" -H 'Content-Type: application/json' -d'
{"persistent":{"cluster":{"remote":{"cluster0":{"seeds":["127.0.0.1:9300"],"transport.ping_schedule":"30s"},"cluster1":{"seeds":["127.0.0.1:9301"],"transport.compress":true,"skip_unavailable":true},"cluster2":{"seeds":["127.0.0.1:9302"]}}}}}'

curl -XPUT "http://localhost:9202/_cluster/settings" -H 'Content-Type: application/json' -d'
{"persistent":{"cluster":{"remote":{"cluster0":{"seeds":["127.0.0.1:9300"],"transport.ping_schedule":"30s"},"cluster1":{"seeds":["127.0.0.1:9301"],"transport.compress":true,"skip_unavailable":true},"cluster2":{"seeds":["127.0.0.1:9302"]}}}}}'


#创建测试数据
curl -XPOST "http://localhost:9200/users/_doc" -H 'Content-Type: application/json' -d'
{"name":"user1","age":10}'

curl -XPOST "http://localhost:9201/users/_doc" -H 'Content-Type: application/json' -d'
{"name":"user2","age":20}'

curl -XPOST "http://localhost:9202/users/_doc" -H 'Content-Type: application/json' -d'
{"name":"user3","age":30}'


#查询
GET /users,cluster1:users,cluster2:users/_search
{
  "query": {
    "range": {
      "age": {
        "gte": 20,
        "lte": 40
      }
    }
  }
}




------------

## 集群分布式模型及选主与脑裂问题

分布式特性

 Elasticsearch 的分布式架构带来的好处
	存储的⽔平扩容，⽀持 PB 级数据
	提⾼系统的可⽤性，部分节点停⽌服务，整个集群的服务不受影响
Elasticsearch 的分布式架构
	不同的集群通过不同的名字来区分，默认名字 “elasticsearch” 
	通过配置⽂件修改，或者在命令⾏中 -E cluster.name=geektime 进⾏设定


节点		
	节点是⼀个 Elasticsearch 的实例
		其本质上就是⼀个 JAVA 进程
		⼀台机器上可以运⾏多个 Elasticsearch 进程，但是⽣产环境⼀般建议⼀台机器上就运⾏⼀个 Elasticsearch 实例
	每⼀个节点都有名字，通过配置⽂件配置，或者启动时候 -E node.name=geektime 指定
	每⼀个节点在启动之后，会分配⼀个 UID，保存在 data ⽬录下

Coordinating Node
		
	处理请求的节点，叫 Coordinating Node
		路由请求到正确的节点，例如创建索引的请求，需要路由到 Master节点
	所有节点默认都是 Coordinating Node
	通过将其他类型设置成 False，使其成为 Dedicated Coordinating Node

	Demo – 启动节点
		启动⼀个节点的
		bin/elasticsearch -E node.name=node1 -E cluster.name=geektime -E path.data=node1_data -E 
		http.port=9200

	Demo - 创建⼀个新的索引	
		发送创建索引的请求
			Settings 3 Primary 和 1 个 Replica
			请求可以发送到任何的节点，处理你请求的节点，叫做 Coordinating Node
			创建 / 删除 索引的请求，只能被 Master 节点处理


Data Node
		
	可以保存数据的节点，叫做 Data Node
		节点启动后，默认就是数据节点。可以设置 node.data: false 禁⽌
	Data Node的职责
		保存分⽚数据。在数据扩展上起到了⾄关重要的作⽤（由 Master Node 决定如何把 分⽚分发到数据节点上）
	通过增加数据节点
		可以解决数据⽔平扩展和解决数据单点问题

Master Node

Master Node 的职责
	处理创建，删除索引等请求 /决定分⽚被分配到哪个节点 / 负责索引的创建与删除
	维护并且更新 Cluster State
Master Node 的最佳实践
	Master 节点⾮常重要，在部署上需要考虑解决单点的问题
	为⼀个集群设置多个 Master 节点 / 每个节点只承担 Master 的单⼀⻆⾊	


Master Eligible Nodes & 选主流程

	⼀个集群，⽀持配置多个 Master Eligible 节点。这些节点可以在必要时(如 Master 节点出现故障，⽹络故障时)参与选主流程，成为 Master 节点
	每个节点启动后，默认就是⼀个 Master eligible 节点
		可以设置 node.master: false 禁⽌
	当集群内第⼀个 Master eligible 节点启动时候，它会将⾃⼰选举成 Master 节点

集群状态

	集群状态信息（Cluster State），维护了⼀个集群中，必要的信息
		所有的节点信息
		所有的索引和其相关的 Mapping 与 Setting 信息
		分⽚的路由信息
	在每个节点上都保存了集群的状态信息
	但是，只有 Master 节点才能修改集群的状态信息，并负责同步给其他节点
		因为，任意节点都能修改信息会导致 Cluster State 信息的不⼀致

	Demo – 增加⼀个新的节点
		bin/elasticsearch -E node.name=node2 -E cluster.name=geektime -E path.data=node2_data -E http.port=9201
		Nodes API 看到新增节点
		发现 Replica 被分配


Master Eligible Nodes & 选主的过程
	互相 Ping 对⽅，Node Id 低的会成为被选举的节点
	其他节点会加⼊集群，但是不承担 Master 节点的⻆⾊。⼀旦发现被选中的主节点丢失，就会选举出新的 Master 节点


脑裂问题
	Split-Brain，分布式系统的经典⽹络问题，当出现⽹络问题，⼀个节点和其他节点⽆法连接
		Node1与Node2、Node3断开
			Node 2 和 Node 3 会重新选举 Master
			Node 1 ⾃⼰还是作为 Master，组成⼀个集群，同时更新 Cluster State
			导致 2 个 master，维护不同的 cluster state。当⽹络恢复时，⽆法选择正确恢复


如何避免脑裂问题
	限定⼀个选举条件，设置 quorum(仲裁)，从 7.0 开始，⽆需这个配置
		Quorum = （master 节点总数 /2）+ 1
		移除 minimum_master_nodes 参数，让Elasticsearch⾃⼰选择可以形成仲裁的节点。
		典型的主节点选举现在只需要很短的时间就可以完成。集群的伸缩变得更安全、更容易，并且可能造成丢失数据的系统配置选项更少了。
		节点更清楚地记录它们的状态，有助于诊断为什么它们不能加⼊集群或为什么⽆法选举出主节点

配置节点类型
	⼀个节点默认情况下是⼀个 Master eligible，data and ingest node:

节点类型 			配置参数 				默认值
maste 				eligible node.master 	true
data 				node.data 				true
ingest 				node.ingest 			ture
coordinating only 	⽆ 						设置上⾯三个参数全部为false
machine learning 	node.ml 				true (需要enable x-pack)




bin/elasticsearch -E node.name=node1 -E cluster.name=geektime -E path.data=node1_data
bin/elasticsearch -E node.name=node2 -E cluster.name=geektime -E path.data=node2_data
bin/elasticsearch -E node.name=node3 -E cluster.name=geektime -E path.data=node3_data






---------


## 分⽚与集群的故障转移

Primary Shard - 提升系统存储容量

	分⽚是 Elasticsearch 分布式存储的基⽯
		主分⽚ / 副本分⽚
	通过主分⽚，将数据分布在所有节点上	
		Primary Shard，可以将⼀份索引的数据，分散在多个 Data Node 上，实现存储的⽔平扩展
		主分⽚(Primary Shard)数在索引创建时候指定，后续默认不能修改，如要修改，需重建索引

Replica Shard - 提⾼数据可⽤性

	数据可⽤性
		通过引⼊副本分⽚ (Replica Shard) 提⾼数据的可⽤性。⼀旦主分⽚丢失，副本分⽚可以 Promote 成主分⽚。副本分⽚数可以动态调整。
		每个节点上都有完备的数据。如果不设置副本分⽚，⼀旦出现节点硬件故障，就有可能造成数据丢失
	提升系统的读取性能
		副本分⽚由主分⽚(Primary Shard)同步。通过⽀持增加 Replica 个数，⼀定程度可以提⾼读取的吞吐量


分⽚数的设定

	如何规划⼀个索引的主分⽚数和副本分⽚数
		主分⽚数过⼩：例如创建了 1 个 Primary Shard 的 Index
			如果该索引增⻓很快，集群⽆法通过增加节点实现对这个索引的数据扩展
	主分⽚数设置过⼤：引发⼀个节点上有过多分⽚，影响性能		
	副本分⽚数设置过多，会降低集群整体的写⼊性能


单节点集群
	Node1
	P0 P1 P1

	副本⽆法分⽚，集群状态⻩⾊（主分片和副本分片必须分配在不同节点上）
PUT tmdb
{
	"settings":{
		"number_of_shards":3,
		"number_of_replicas":1
	}
}

	增加⼀个数据节点
		Node1            Node2
		p0 p1 p1 		 r0 r1 r2

		集群状态转为绿⾊
		集群具备故障转移能⼒
		尝试着将 Replica 设置成 2 和 3，查看集群的状况

	再增加⼀个数据节点	
			Node1      Node2      Node3
			p0 r1 	   p1 r2	  p2 r1
		集群具备故障转移能⼒
		Master 节点会决定分⽚分配到哪个节点
		通过增加节点，提⾼集群的计算能⼒


故障转移
	需要集群具备故障转移的能⼒，必须将索引的副本分⽚数设置为 1，否则，一旦有节点丢失，就会造成数据丢失
	3 个节点共同组成。包含了 1 个索引，索引设置了 3 个 Primary Shard 和 1 个 Replica
	节点 1 是 Master 节点，节点意外出现故障。集群重新选举 Master 节点
	Node1宕机
		Node 3 上的 R0 提升成 P0，集群变⻩
		R0 和 R1 分配，集群变绿
			Node1   Node2    Node3
			p0 r1   p1 r2	 p2 r0

				 Node2     		Node3
				 p1 r2 r0		p2 p0 r1 
			

集群健康状态
	GET /_cluster/health
		Green：健康状态，所有的主分⽚和副本分⽚都可⽤
		Yellow：亚健康，所有的主分⽚可⽤，部分副本分⽚不可⽤
		Red：不健康状态，部分主分⽚不可⽤



---------


## ⽂档分布式存储

⽂档存储在分⽚上
	⽂档会存储在具体的某个主分⽚和副本分⽚上：例如 ⽂档 1， 会存储在 P0 和 R0 分⽚上
⽂档到分⽚的映射算法
	确保⽂档能均匀分布在所⽤分⽚上，充分利⽤硬件资源，避免部分机器空闲，部分机器繁忙
	实时计算，通过⽂档 1，⾃动算出，需要去那个分⽚上获取⽂档
⽂档到分⽚的路由算法
	shard = hash(_routing) % number_of_primary_shards
		Hash 算法确保⽂档均匀分散到分⽚中
		默认的 _routing 值是⽂档 id
		可以⾃⾏制定 routing数值，例如⽤相同国家的商品，都分配到指定的 shard
		设置 Index Settings 后， Primary 数，不能随意修改的根本原因


更新⼀个⽂档
	



删除⼀个⽂档



-----------


## 分⽚及其⽣命周期

分⽚的内部原理
	什么是 ES 的分⽚
		ES 中最⼩的⼯作单元 / 是⼀个 Lucene 的 Index	
	⼀些问题：
		为什么 ES 的搜索是近实时的(1 秒后被搜到)
		ES 如何保证在断电时数据也不会丢失
		为什么删除⽂档，并不会⽴刻释放空间


倒排索引不可变性
	倒排索引采⽤ Immutable Design，⼀旦⽣成，不可更改
	不可变性，带来了的好处如下
		⽆需考虑并发写⽂件的问题，避免了锁机制带来的性能问题
		⼀旦读⼊内核的⽂件系统缓存，便留在哪⾥。只要⽂件系统存有⾜够的空间，⼤部分请求就会直接请求内存，不会命中磁盘，提升了很⼤的性能
		缓存容易⽣成和维护 / 数据可以被压缩
	不可变更性，带来了的挑战：如果需要让⼀个新的⽂档可以被搜索，需要重建整个索引。


Lucene Index
	在 Lucene 中，单个倒排索引⽂件被称为Segment。Segment 是⾃包含的，不可变更的。多个 Segments 汇总在⼀起，称为 Lucene 的Index，其对应的就是 ES 中的 Shard
	当有新⽂档写⼊时，会⽣成新 Segment，查询时会同时查询所有 Segments，并且对结果汇总。Lucene 中有⼀个⽂件，⽤来记录所有Segments 信息，叫做 Commit Point
	删除的⽂档信息，保存在“.del”⽂件中


什么是 Refresh
	将 Index buffer 写⼊ Segment 的过程叫Refresh。Refresh 不执⾏ fsync 操作
	Refresh 频率：默认 1 秒发⽣⼀次，可通过index.refresh_interval 配置。Refresh 后，数据就可以被搜索到了。这也是为什么Elasticsearch 被称为近实时搜索
	如果系统有⼤量的数据写⼊，那就会产⽣很多的 Segment
	Index Buffer 被占满时，会触发 Refresh，默认值是 JVM 的 10%


什么是 Transaction Log

	Segment 写⼊磁盘的过程相对耗时，借助⽂件系统缓存，Refresh 时，先将Segment 写⼊缓存以开放查询
	为了保证数据不会丢失。所以在 Index ⽂档时，同时写 Transaction Log，⾼版本开始，Transaction Log 默认落盘。每个分⽚有⼀个 Transaction Log
	在 ES Refresh 时，Index Buffer 被清空，Transaction log 不会清空


什么是 Flush
	ES Flush & Lucene Commit
		调⽤ Refresh，Index Buffer 清空并且 Refresh
		调⽤ fsync，将缓存中的 Segments写⼊磁盘
		清空（删除）Transaction Log
		默认 30 分钟调⽤⼀次
		Transaction Log 满 （默认 512 MB）


--ES Flush-> index buffer     --lucene commit->segment
			 Transaction log


Merge
	Segment 很多，需要被定期被合并
		减少 Segments / 删除已经删除的⽂档
	ES 和 Lucene 会⾃动进⾏ Merge 操作
		POST my_index/_forcemerge


--------------


## 剖析分布式查询及相关性算分

分布式搜索的运⾏机制
	Elasticsearch 的搜索，会分两阶段进⾏
		第⼀阶段 - Query
		第⼆阶段 - Fetch
	Query-then-Fetch


Query 阶段
	⽤户发出搜索请求到 ES 节点。节点收到请求后， 会以 Coordinating 节点的身份，在 6 个主副分⽚中随机选择 3 个分⽚，发送查询请求
	被选中的分⽚执⾏查询，进⾏排序。然后，每个分⽚都会返回 From + Size 个排序后的⽂档 Id 和排序值 给 Coordinating 节点

		CN(P0,R1)
	DN(P1,R2)   DN(P2,R0)


Fetch 阶段
	Coordinating Node 会将 Query 阶段，从从每个分⽚获取的排序后的⽂档 Id 列表，重新进⾏排序。选取 From 到 From + Size个⽂档的 Id
	以 multi get 请求的⽅式，到相应的分⽚获取详细的⽂档数据


Query Then Fetch 潜在的问题
	性能问题
		每个分⽚上需要查的⽂档个数 = from + size
		最终协调节点需要处理：number_of_shard * ( from+size )
		深度分⻚
	相关性算分	
		每个分⽚都基于⾃⼰的分⽚上的数据进⾏相关度计算。这会导致打分偏离的情况，特别是数据量很少时。相关性算分在分⽚之间是相互独⽴。
		当⽂档总数很少的情况下，如果主分⽚⼤于 1，主分⽚数越多 ，相关性算分会越不准


解决算分不准的⽅法
	数据量不⼤的时候，可以将主分⽚数设置为 1 
		当数据量⾜够⼤时候，只要保证⽂档均匀分散在各个分⽚上，结果⼀般就不会出现偏差
	使⽤ DFS Query Then Fetch
		搜索的URL 中指定参数 “_search?search_type=dfs_query_then_fetch” 
		到每个分⽚把各分⽚的词频和⽂档频率进⾏搜集，然后完整的进⾏⼀次相关性算分，耗费更加多的 CPU 和内存，执⾏性能低下，⼀般不建议使⽤


DELETE message
PUT message
{
  "settings": {
    "number_of_shards": 20
  }
}

GET message

POST message/_doc?routing=1
{
  "content":"good"
}

POST message/_doc?routing=2
{
  "content":"good morning"
}

POST message/_doc?routing=3
{
  "content":"good morning everyone"
}

POST message/_search
{
  "explain": true,
  "query": {
    "match_all": {}
  }
}


POST message/_search
{
  "explain": true,
  "query": {
    "term": {
      "content": {
        "value": "good"
      }
    }
  }
}


POST message/_search?search_type=dfs_query_then_fetch
{

  "query": {
    "term": {
      "content": {
        "value": "good"
      }
    }
  }
}

使⽤ 1 个主分⽚测试， Good 应该排在第⼀，Good DF(索引统计) 数值应该是 3
当 多个 个主分⽚时，3 个⽂档的算分都⼀样。 可以通过 Explain API 进⾏分析
在 3 个主分⽚上 执⾏ DFS Query Then Fetch，结果和⼀个分⽚上⼀致



------

## 排序及 Doc Values & Field Data

排序
	Elasticsearch 默认采⽤相关性算分对结果进⾏降序排序
	可以通过设定 sorting 参数，⾃⾏设定排序
	如果不指定_score，算分为 Null({"_score":{ "order": "desc"}})


#单字段排序


POST /kibana_sample_data_ecommerce/_search
{
  "size": 5,
  "query": {
    "match_all": {}
  },
  "sort": [{
      "order_date": {
        "order": "desc"
      }
    }
  ]
}



#多字段排序
	组合多个条件
	优先考虑写在前⾯的排序
	⽀持对相关性算分进⾏排序	

POST /kibana_sample_data_ecommerce/_search
{
  "size": 5,
  "query": {
    "match_all": {

    }
  },
  "sort": [
    {"order_date": {"order": "desc"}},
    {"_doc":{"order": "asc"}},
    {"_score":{ "order": "desc"}}
  ]
}



# 排序的过程
	排序是针对字段原始内容进⾏的。 倒排索引⽆法发挥作⽤
	需要⽤到正排索引。通过⽂档 Id 和字段快速得到字段原始内容
	Elasticsearch 有两种实现⽅法
		Fielddata
		Doc Values （列式存储，对 Text 类型⽆效）


		 Doc Values 					Field data
何时创建  索引时，和倒排索引⼀起创建 		搜索时候动态创建
创建位置  磁盘⽂件 						JVM Heap
优点 	 避免⼤量内存占⽤ 				索引速度快，不占⽤额外的磁盘空间
缺点 	 降低索引速度，占⽤额外磁盘空间 	⽂档过多时，动态创建开销⼤，占⽤ 过多JVM Heap，
缺省值 	 ES 2.x 之后 					ES 1.x 及之前


GET kibana_sample_data_ecommerce/_mapping

#对 text 字段进行排序。默认会报错，需打开fielddata
POST /kibana_sample_data_ecommerce/_search
{
  "size": 5,
  "query": {
    "match_all": {

    }
  },
  "sort": [
    {"customer_full_name": {"order": "desc"}}
  ]
}


#打开 text的 fielddata
	默认关闭，可以通过 Mapping 设置打开。修改设置后，即时⽣效，⽆需重建索引
	其他字段类型不⽀持，只⽀持对 Text 进⾏设定
	打开后，可以对 Text 字段进⾏排序。但是是对分词后的 term 排序，所以，结果往往⽆法满⾜预期，不建议使⽤
	部分情况下打开，满⾜⼀些聚合分析的特定需求
	Text 类型打开 Fielddata后，可以查看分词后的数据

PUT kibana_sample_data_ecommerce/_mapping
{
  "properties": {
    "customer_full_name": {
      "type": "text",
      "fielddata": true,
      "fields": {
        "keyword": {
          "type": "keyword",
          "ignore_above": 256
        }
      }
    }
  }
}


#关闭 keyword的 doc values
	默认启⽤，可以通过 Mapping 设置关闭
		增加索引的速度 / 减少磁盘空间
	如果重新打开，需要重建索引
	什么时候需要关闭
		明确不需要做排序及聚合分析

PUT test_keyword/_mapping
{
  "properties": {
    "user_name":{
      "type": "keyword",
      "doc_values":false
    }
  }
}

DELETE test_keyword

PUT test_text
PUT test_text/_mapping
{
  "properties": {
    "intro":{
      "type": "text",
      "doc_values":true
    }
  }
}

DELETE test_text


DELETE temp_users
PUT temp_users
PUT temp_users/_mapping
{
  "properties": {
    "name":{"type": "text","fielddata": true},
    "desc":{"type": "text","fielddata": true}
  }
}

Post temp_users/_doc
{"name":"Jack","desc":"Jack is a good boy!","age":10}

#打开fielddata 后，查看 docvalue_fields数据
POST  temp_users/_search
{
  "docvalue_fields": [
    "name","desc"
    ]
}

#查看整型字段的docvalues
POST  temp_users/_search
{
  "docvalue_fields": [
    "age"
    ]
}



对 Text 字段设置 fielddata，⽀持随时修改
Doc Values 可以在 Mapping 中关闭。但是需要重新索引
Text 不⽀持 Doc Values
使⽤ docvalue_fields 查看存储的信息



-------------



## 分⻚与遍历 – From，Size，Search After & Scroll API


From / Size
	默认情况下，查询按照相关度算分排序，返回前10 条记录
	容易理解的分⻚⽅案
		From：开始位置
		Size：期望获取⽂档的总数
		From + Size 必须⼩与 10000

分布式系统中深度分⻚的问题
	ES 天⽣就是分布式的。查询信息，但是数据分别保存在多个分⽚，多台机器上，ES 天⽣就需要满⾜排序的需要（按照相关性算分）当⼀个查询： From = 990， Size =10
	会在每个分⽚上先都获取 1000 个⽂档。然后，通过 Coordinating Node 聚合所有结果。最后再通过排序选取前 1000 个⽂档
	⻚数越深，占⽤内存越多。为了避免深度分⻚带来的内存开销。ES 有⼀个设定，默认限定到10000 个⽂档
		Index.max_result_window

POST tmdb/_search
{
  "from": 10000,
  "size": 1,
  "query": {
    "match_all": {

    }
  }
}


# Search After 避免深度分⻚的问题
	避免深度分⻚的性能问题，可以实时获取下⼀⻚⽂档信息
		不⽀持指定⻚数（From） 
		只能往下翻
	第⼀步搜索需要指定 sort，并且保证值是唯⼀的（可以通过加⼊ _id 保证唯⼀性）
	然后使⽤上⼀次，最后⼀个⽂档的 sort 值进⾏查询

Search After 是如何解决深度分⻚的问题
	假定 Size 是 10
	当查询 990 – 1000
	通过唯⼀排序值定位，将每次要处理的⽂档数都控制在 10

POST users/_search
{
    "size": 1,
    "query": {
        "match_all": {}
    },
    "search_after":
        [
          10,
          "ZQ0vYGsBrR8X3IP75QqX"],
    "sort": [
        {"age": "desc"} ,
        {"_id": "asc"}    
    ]
}


#Scroll API
	通过快照，遍历数据
	创建⼀个快照，有新的数据写⼊以后，⽆法被查到
	每次查询后，输⼊上⼀次的 Scroll Id
	scroll=1m(保持游标查询窗口一分钟)

DELETE users
POST users/_doc
{"name":"user1","age":10}

POST users/_doc
{"name":"user2","age":20}

POST users/_doc
{"name":"user3","age":30}

POST users/_doc
{"name":"user4","age":40}

POST /users/_search?scroll=5m
{
    "size": 1,
    "query": {
        "match_all" : {
        }
    }
}

POST users/_doc
{"name":"user5","age":50}

POST /_search/scroll
{
    "scroll" : "1m",
    "scroll_id" : "DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAWAWbWdoQXR2d3ZUd2kzSThwVTh4bVE0QQ=="
}




不同的搜索类型和使⽤场景
	Regular
		需要实时获取顶部的部分⽂档。例如查询最新的订单
	Scroll
		需要全部⽂档，例如导出全部数据
	Pagination
		From 和 Size
		如果需要深度分⻚，则选⽤ Search After



------------

## 处理并发读写操作


并发控制的必要性
	两个 Web 程序同时更新某个⽂档，如果缺乏有效的并发，会导致更改的数据丢失
	悲观并发控制
		假定有变更冲突的可能。会对资源加锁，防⽌冲突。例如数据库⾏锁
	乐观并发控制
		假定冲突是不会发⽣的，不会阻塞正在尝试的操作。如果数据在读写中被修改，更新将会失败。应⽤程序决定如何解决冲突，例如重试更新，使⽤新的数据，或者将错误报告给⽤户
	ES 采⽤的是乐观并发控制


ES 的乐观并发控制
	ES 中的⽂档是不可变更的。如果你更新⼀个⽂档，会将就⽂档标记为删除，同时增加⼀个全新的⽂档。同时⽂档的 version 字段加 1 
	内部版本控制
		If_seq_no + If_primary_term
	使⽤外部版本(使⽤其他数据库作为主要数据存储) 
		version + version_type=external



DELETE products
PUT products

PUT products/_doc/1
{
  "title":"iphone",
  "count":100
}



GET products/_doc/1

PUT products/_doc/1?if_seq_no=1&if_primary_term=1
{
  "title":"iphone",
  "count":100
}


PUT products/_doc/1?version=30000&version_type=external
{
  "title":"iphone",
  "count":100
}










