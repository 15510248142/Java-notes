

## 集群身份认证与用户鉴权



数据安全性的基本需求
	身份认证 
		鉴定用户是否合法 
	用户鉴权 
		指定那个用户可以访问哪个索引 
	传输加密 
	日志审计


Authentication - 身份认证
	认证体系的几种类型 
		提供用户名和密码 
		提供秘钥或 Kerberos 票据 
	Realms：X-Pack 中的认证服务 
		内置 Realms （免费） 
			File / Native（用户名密码保存在 Elasticsearch） 
		外部 Realms （收费） 
			LDAP / Active Directory / PKI / SAML / Kerbero


RBAC - 用户鉴权
什么是 RBAC：
	Role Based Access Control， 定义一个角色，并分配一组权限。
	权限包括索引 级，字段级，集群级的不同的操作。然后通过将角色分配给用户，使得用户拥有这些权限

	User：The authenticated User 
	Role：A named set of permissions 
	Permission – A set of one or more privileges against a secured resource 
	Privilege – A named group of 1 or more actions that user may execute against a secured resource

Privilege
	Cluster Privileges
		all / monitor / manager / manage_index / manage_index_template / manage_rollup
	Indices Privileges
		all / create / create_index / delete / delete_index / index / manage / read /write view_index_metadata

创建内置的用户和角色

内置的角色与用户
	elastic
	kibana
	logstash_system
	beats_system
	apm_system
	Remote_monitoring_user 


开启并配置 X-Pack 的认证与鉴权
	修改配置文件，打开认证与授权
		bin/elasticsearch -E node.name=node0 -E cluster.name=geektime -E path.data=node0_data -E http.port=9200 -E xpack.security.enabled=true
	创建默认的用户和分组
		bin/elasticsearch-password interactive
	当集群开启身份认证之后，配置 Kibana	
		# 修改 kibana.yml
		elasticsearch.username: "kibana"
		elasticsearch.password: "changeme"		
	启动。使用用户名，elastic，密码elastic
		./bin/kibana
	创建一个 Role，配置为对某个索引只读权限 / 创建一个用户，把用户加入 Role	



-------


## 集群内部间的安全通信


为什么要加密通讯
	加密数据 – 避免数据抓包，敏感信息泄漏 
	验证身份 – 避免 Impostor Node 
		Data / Cluster State


为节点创建证书
	TLS
		TLS 协议要求 Trusted Certificate Authority（CA）签发的 X.509的证书
	证书认证的不同级别	
		Certificate – 节点加入需要使用相同 CA 签发的证书
		Full Verification – 节点加入集群需要相同 CA 签发的证书，还需要验证 Host name 或 IP 地址
		No Verification – 任何节点都可以加入，开发环境中用于诊断目的


生成节点证书
	为您的Elasticearch集群创建一个证书颁发机构
		bin/elasticsearch-certutil ca
	为群集中的每个节点生成证书和私钥。
		bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12
	将证书拷贝到 config/certs目录下
		elastic-certificates.p12		

配置节点间通讯

	bin/elasticsearch -E node.name=node0 -E cluster.name=geektime -E path.data=node0_data -E http.port=9200 -E xpack.security.enabled=true -E xpack.security.transport.ssl.enabled=true -E xpack.security.transport.ssl.verification_mode=certificate -E xpack.security.transport.ssl.keystore.path=certs/elastic-certificates.p12 -E xpack.security.transport.ssl.truststore.path=certs/elastic-certificates.p12

	bin/elasticsearch -E node.name=node1 -E cluster.name=geektime -E path.data=node1_data -E http.port=9201 -E xpack.security.enabled=true -E xpack.security.transport.ssl.enabled=true -E xpack.security.transport.ssl.verification_mode=certificate -E xpack.security.transport.ssl.keystore.path=certs/elastic-certificates.p12 -E xpack.security.transport.ssl.truststore.path=certs/elastic-certificates.p12


	#不提供证书的节点，无法加入
	bin/elasticsearch -E node.name=node2 -E cluster.name=geektime -E path.data=node2_data -E http.port=9202 -E xpack.security.enabled=true -E xpack.security.transport.ssl.enabled=true -E xpack.security.transport.ssl.verification_mode=certificate



## elasticsearch.yml 配置

#xpack.security.transport.ssl.enabled: true
#xpack.security.transport.ssl.verification_mode: certificate

#xpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12
#xpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12


------


## 集群与外部间的安全通信

为什么需要 HTTPS
	浏览器访问kibana
	kibana访问es
	es访问logstash
	都需要https保护数据

配置 Elasticsearch for HTTPS
	xpack.security.http.ssl.enabled: true
	xpack.security.http.ssl.keystore.path: certs/elastic-certificates.p12
	xpack.security.http.ssl.truststore.path: certs/elastic-certificates.p12

	# ES 启用 https
	bin/elasticsearch -E node.name=node0 -E cluster.name=geektime -E path.data=node0_data -E http.port=9200 -E xpack.security.enabled=true -E xpack.security.transport.ssl.enabled=true -E xpack.security.transport.ssl.verification_mode=certificate -E xpack.security.transport.ssl.keystore.path=certs/elastic-certificates.p12 -E xpack.security.http.ssl.enabled=true -E xpack.security.http.ssl.keystore.path=certs/elastic-certificates.p12 -E xpack.security.http.ssl.truststore.path=certs/elastic-certificates.p12


配置 Kibana 连接 ES HTTPS
	配置使用 HTTPS 访问 Kibana：为kibana生成pem
		openssl pkcs12 -in elastic-certificates.p12 -cacerts -nokeys -out elastic-ca.pem
	将证书拷贝到 config/certs目录
	配置kibana.yml
		elasticsearch.hosts: ["https://localhost:9200"]
		elasticsearch.ssl.certificateAuthorities: [ "/Users/yiruan/geektime/kibana-7.1.0/config/certs/elastic-ca.pem" ]
		elasticsearch.ssl.verificationMode: certificate



为 Kibana 配置 HTTPS
	生成后解压(zip文件)，包含了instance.crt 和 instance.key
	bin/elasticsearch-certutil ca --pem
	将证书拷贝到 config/certs目录
	配置kibana.yml
		server.ssl.enabled: true
		server.ssl.certificate: config/certs/instance.crt
		server.ssl.key: config/certs/instance.key
	启动kibana
		因为通过自签的ca，会报error，不同管


---------



## 常见的集群部署方式

节点类型
	不同角色的节点 Master eligible / Data / Ingest / Coordinating / Machine Learning 
	在开发环境中，一个节点可承担多种角色 
	在生产环境中， 
		根据数据量，写入和查询的吞吐量，选择合适的部署方式 
		建议设置单一角色的节点（dedicated node）


节点参数配置
	一个节点在默认情况会下同时扮演：master eligible，data node 和 ingest node


节点类型 			配置参数 				默认值 
maste eligible 		node.master 			true 
data 				node.data 				true 
ingest 				node.ingest 			true 
coordinating only 	无 						设置上面三个参数全部为false 



单一职责的节点
	一个节点只承担一个角色 
	Master 节点
		node.master: true 
		node.ingest: false 
		node.data: false

	Data 节点
		node.master: false 
		node.ingest: false 
		node.data: true


	Ingest 节点
		node.master: false 
		node.ingest: true 
		node.data: false

	Coordinate 节点
		node.master: false 
		node.ingest: false 
		node.data: false



单一角色：职责分离的好处
	Dedicated master eligible nodes：负责集群状态(cluster state)的管理 
		使用低配置的 CPU，RAM 和磁盘 
	Dedicated data nodes：负责数据存储及处理客户端请求 
		使用高配置的 CPU, RAM 和磁盘 
	Dedicated ingest nodes：负责数据处理 
		使用高配置 CPU；中等配置的RAM； 低配置的磁盘
	Dedicate Coordinating Only Node (Client Node)
		配置：将 Master，Data，Ingest 都配置成 False
		Medium/High CUP；Medium/High RAM；Low Disk
		生产环境中，建议为一些大的集群配置 Coordinating Only Nodes
			扮演 Load Balancers。降低 Master 和 Data Nodes 的负载
			负责搜索结果的 Gather/Reduce
			有时候无法预知客户端会发送怎么样的请求
				大量占用内存的结合操作，一个深度聚合可能会引发 OOM

Dedicate Master Node
	从高可用 & 避免脑裂的角度出发
		一般在生产环境中配置 3 台
		一个集群只有 1 台活跃的主节点
			负责分片管理，索引创建，集群管理等操作
	如果和数据节点或者 Coordinate 节点混合部署
		数据节点相对有比较大的内存占用 
		Coordinate 节点有时候可能会有开销很高的查询，导致 OOM 
		这些都有可能影响 Master 节点，导致集群的不稳定



基本部署：增加节点，水平扩展
	当磁盘容量无法满足需求时，增加数据节点；
	磁盘读写压力大时，增加数据节点


水平扩展：Coordinating Only Node
	当系统中有大量的复杂查询及聚合时候，增加 Coordinating 节点，增加查询的性能

读写分离
	读LB->Coordinating N
	写LV-> Ingest N


在集群中部署 Kibana
	将 Kibana 部署在 Coordinating 节点


异地多活的部署
	集群处在三个数据中心；
	数据三写；
	GTM 分发读请求



------



## Hot & Warm 架构与 Shard Filtering

日志类应用的部署架构
	Hot Data Node
	Warm Data Node
	Master


什么是 Hot & Warm Architecture
	Hot & Warm Architecture
		数据通常不会有Update操作；
		适用于Timebased索引数据（生命周期管理），同时数据量比较大的场景
		引入 Warm 节点，低配置大容量的机器存放老数据，以降低部署成本
	两类数据节点, 不同的硬件配置	
		Hot 节点（通常使用 SSD）：索引有不断有新文档写入。通常使用 SSD 
		Warm 节点（通常使用 HDD）：索引不存在新数据的写入；同时也不存在大量的数据查询


Hot Nodes
	用于数据的写入
		Indexing 对 CPU 和 IO 都有很高的要求。所以需要使用高配置的机器
		存储的性能要好。建议使用 SSD

Warm Nodes
	用于保存只读的索引，比较旧的数据
		通常使用大容量的磁盘（通常是 Spinning Disks）


配置 Hot & Warm Architecture
	使用 Shard Filtering，步骤分为以下几步 
		标记节点 （Tagging）
		配置索引到 Hot Node 
		 配置索引到 Warm 节点


标记节点
	需要通过 “node.attr” 来标记一个节点
		节点的 attribute可以是任何的 key/value
		可以通过 elasticsearch.yml 或者通过 –E 命令指定


# 标记一个 Hot 节点
bin/elasticsearch  -E node.name=hotnode -E cluster.name=geektime -E path.data=hot_data -E node.attr.my_node_type=hot

# 标记一个 warm 节点
bin/elasticsearch  -E node.name=warmnode -E cluster.name=geektime -E path.data=warm_data -E node.attr.my_node_type=warm

# 查看节点
GET /_cat/nodeattrs?v

# 配置到 Hot节点
	创建索引时候，指定将其创建在 hot 节点上
PUT logs-2019-06-27
{
  "settings":{
    "number_of_shards":2,
    "number_of_replicas":0,
    "index.routing.allocation.require.my_node_type":"hot"
  }
}


PUT my_index1/_doc/1
{
  "key":"value"
}



GET _cat/shards?v


# 旧数据移动到 Warm 节点
	Index.routing.allocation 是一个索引级的 dynamic setting，可以通过 API 在后期进行设定
	Curator / Index Life Cycle Management Tool

PUT PUT logs-2019-06-27/_settings
{  
  "index.routing.allocation.require.my_node_type":"warm"
}


Rack Awareness

ES 的节点可能分布在不同的机架 
	当一个机架断电，可能会同时丢失几个节点
	如果一个索引相同的主分片和副本分片，同时在这个机架上，就有可能导致数据的丢失 
	通过 Rack Awareness 的机制， 就可以尽可能避免将同一个索引 的主副分片同时分配在一个机架 的节点上
		ES 的节点可能分布在不同的机架
		一个机架断电，数据可以恢复


标记 Rack 节点 + 配置集群

# 标记一个 rack 1
bin/elasticsearch  -E node.name=node1 -E cluster.name=geektime -E path.data=node1_data -E node.attr.my_rack_id=rack1

# 标记一个 rack 2
bin/elasticsearch  -E node.name=node2 -E cluster.name=geektime -E path.data=node2_data -E node.attr.my_rack_id=rack2

PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes": "my_rack_id"
  }
}

PUT my_index1
{
  "settings":{
    "number_of_shards":2,
    "number_of_replicas":1
  }
}

PUT my_index1/_doc/1
{
  "key":"value"
}


GET _cat/shards?v
DELETE my_index1/_doc/1



# Fore awareness
# 标记一个 rack 1
bin/elasticsearch  -E node.name=node1 -E cluster.name=geektime -E path.data=node1_data -E node.attr.my_rack_id=rack1

# 标记一个 rack 2
bin/elasticsearch  -E node.name=node2 -E cluster.name=geektime -E path.data=node2_data -E node.attr.my_rack_id=rack1


PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes": "my_rack_id",
    "cluster.routing.allocation.awareness.force.my_rack_id.values": "rack1,rack2"
  }
}
GET _cluster/settings

# 集群黄色
GET _cluster/health

# 副本无法分配
GET _cat/shards?v


GET _cluster/allocation/explain?pretty



Shard Filtering

Shard Filtering 
	“node.attr” - 标记节点 
	“index.routing.allocation” – 分配索引到节点


设置 										分配索引到节点，节点的属性规则 
Index.routing.allocation.include.{attr} 	至少包含一个值 
Index.routing.allocation.exclude.{attr} 	不能包含任何一个值 
Index.routing.allocation.require.{attr} 	所有值都需要包含




---------


## 分片设定及管理


单个分片
	7.0 开始，新创建一个索引时，默认只有一个主分片 
		单个分片，查询算分，聚合不准的问题都可以得以避免 
	单个索引，单个分片时候，集群无法实现水平扩展 
		即使增加新的节点，无法实现水平扩展

两个分片
	集群增加一个节点后，Elasticsearch 会自动进行分片的移动，也叫 Shard Rebalancing


如何设计分片数
	当分片数 > 节点数时
		一旦集群中有新的数据节点加入，分片就可以自动进行分配
		分片在重新分配时，系统不会有 downtime
	多分片的好处：一个索引如果分布在不同的节点，多个节点可以并行执行 
		查询可以并行执行 
		数据写入可以分散到多个机器


一些例子
	案例 1 
		每天 1 GB 的数据，一个索引一个主分片，一个副本分片 
		需保留半年的数据，接近 360 GB 的数据量 ， 
	案例 2 
		5 个不同的日志，每天创建一个日志索引。每个日志索引创建 10 个主分片 
		保留半年的数据 
		5 * 10 * 30 * 6 = 9000 个分片


分片过多所带来的副作用
	Shard 是 Elasticsearch 实现集群水平扩展的最小单位
	过多设置分片数会带来一些潜在的问题
		每个分片是一个 Lucene 的 索引，会使用机器的资源。过多的分片会导致额外的性能开销
			Lucene Indices / File descriptors / RAM / CPU
			每次搜索的请求，需要从每个分片上获取数据	
			分片的 Meta 信息由 Master 节点维护。过多，会增加管理的负担。经验值，控制分片总数在 10 W 以内	


如何确定主分片数
	从存储的物理角度看 
		日志类应用，单个分片不要大于 50 GB 
		搜索类应用，单个分片不要超过20 GB 
	为什么要控制分片存储大小 
		提高 Update 的性能 
		Merge 时，减少所需的资源 
		丢失节点后，具备更快的恢复速度 / 便于分片在集群内 Rebalancing


如何确定副本分片数
	副本是主分片的拷贝 
		提高系统可用性：响应查询请求，防止数据丢失 
		需要占用和主分片一样的资源 
	对性能的影响 
		副本会降低数据的索引速度：有几份副本就会有几倍的 CPU 资源消耗在索引上 
		会减缓对主分片的查询压力，但是会消耗同样的内存资源 
		如果机器资源充分，提高副本数，可以提高整体的查询 QPS


调整分片总数设定，避免分配不均衡
	ES 的分片策略会尽量保证节点上的分片数大致相同
		扩容的新节点没有数据，导致新索引集中在新的节点 
		热点数据过于集中，可能会产生新能问题




--------


## 如何对集群进行容量规划 


容量规划
	一个集群总共需要多少个节点？ 一个索引需要设置几个分片？ 
		规划上需要保持一定的余量，当负载出现波动，节点出现丢失时，还能正常运行

	做容量规划时，一些需要考虑的因素 
		机器的软硬件配置 
		单条文档的尺寸 / 文档的总数据量 / 索引的总数据量（Time base 数据保留的时间）/ 副本分片数 
		文档是如何写入的（Bulk的尺寸） 
		文档的复杂度，文档是如何进行读取的（怎么样的查询和聚合）


评估业务的性能需求
	数据吞吐及性能需求 
		数据写入的吞吐量，每秒要求写入多少数据？ 
		查询的吞吐量？ 
		单条查询可接受的最大返回时间？ 
	了解你的数据 
		数据的格式和数据的 Mapping 
		实际的查询和聚合长的是什么样的


常见用例
	搜索：固定大小的数据集 
		搜索的数据集增长相对比较缓慢
	日志：基于时间序列的数据 
		使用 ES 存放日志与性能指标。数据每天不断写入，增长速度较快 
		结合 Warm Node 做数据的老化处理


硬件配置
	选择合理的硬件，数据节点尽可能使用 SSD 
	搜索等性能要求高的场景，建议 SSD 
		按照 1 ：10 的比例配置内存和硬盘 
	日志类和查询并发低的场景，可以考虑使用机械硬盘存储 
		按照 1：50 的比例配置内存和硬盘 
	单节点数据建议控制在 2 TB 以内，最大不建议超过 5 TB 
	JVM 配置机器内存的一半，JVM 内存配置不建议超过 32 G


部署方式
	按需选择合理的部署方式 
	如果需要考虑可靠性高可用，建议部署 3 台 dedicated 的 Master 节点 
	如果有复杂的查询和聚合，建议设置 Coordinating 节点


容量规划案例 1: 固定大小的数据集
	一些案例：唱片信息库 / 产品信息 
	一些特性 
		被搜索的数据集很大，但是增长相对比较慢（不会有大量的写入）。更关心搜索和聚合的读取性能 
		数据的重要性与时间范围无关。关注的是搜索的相关度 
	估算索引的的数据量，然后确定分片的大小 
		单个分片的数据不要超过 20 GB 
		可以通过增加副本分片，提高查询的吞吐量


拆分索引
	如果业务上有大量的查询是基于一个字段进行 Filter，该字段又是一个数量有限的枚举值
		例如订单所在的地区

	如果在单个索引有大量的数据，可以考虑将索引拆分成多个索引
		查询性能可以得到提高
		如果要对多个索引进行查询，还是可以在查询中指定多个索引得以实现

	如果业务上有大量的查询是基于一个字段进行 Filter，该字段数值并不固定
		可以启用 Routing 功能，按照 filter 字段的值分布到集群中不同的 shard， 提高 CPU 利用率


容量规划案例 2: 基于时间序列的数据
	相关的用例
		日志 / 指标 / 安全相关的 Events 
		舆情分析 
	一些特性
		每条数据都有时间戳；文档基本不会被更新（日志和指标数据） 
		用户更多的会查询近期的数据；对旧的数据查询相对较少 
		对数据的写入性能要求比较高


创建基于时间序列的索引
	创建 time-based 索引 
		在索引的名字中增加时间信息 
		按照 每天 / 每周 / 每月 的方式进行划分 
	带来的好处 
		更加合理的组织索引，例如随着时间推移，便于对索引做的老化处理 
			利用 Hot & Warm Architecture 
			备份和删除效率高。（ Delete By Query 执行速度慢，底层不也不会立刻释放空间，而 Merge 时又很消 耗资源）


写入时间序列的数据：基于 Date Math 的方式
		容易使用 
		如果时间发生变化，需要重新部署代码

	2019-08-01T00:00:00
		<logs-{now/d}> logs-2019.08.01 
		<logs-{now{YYYY.MM}}> logs-2019.08 
		<logs-{now/w}> logs-2019..7.29


写入时间序列的数据 – 基于 Index Alias
	Time-based 索引 
		创建索引，每天 / 每周 / 每月 
		在索引的名字中增加时间信息


集群扩容
	增加 Coordinating / Ingest Node
		解决 CPU 和 内存开销的问题
	增加数据节点
		解决存储的容量的问题
		为避免分片分布不均的问题，要提前监控磁盘空间，提前清理数据或增加节点（70%）






PUT logs_2019-06-27
PUT logs_2019-06-26


POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "logs_2019-06-27",
        "alias": "logs_write"
      }
    },
    {
      "remove": {
        "index": "logs_2019-06-26",
        "alias": "logs_write"
      }
    }
  ]
}


# POST /<logs-{now/d}/_search
POST /%3Clogs-%7Bnow%2Fd%7D%3E/_search

# POST /<logs-{now/w}/_search
POST /%3Clogs-%7Bnow%2Fw%7D%3E/_search





---------



## 在私有云上管理 Elasticsearch 的一些方法



管理单个集群
	集群容量不够时，需手工增加节点
	有节点丢失时，手工修复或更换节点
		确保 Rack Awareness
	集群版本升级；数据备份；滚动升级	
		完全手动，管理成本高
		无法统一管理，例如整合变更管理等


ECE，帮助你管理多个 Elasticsearch 集群
	通过单个控制台，管理多个集群
		支持不同方式的集群部署（支持各类部署） / 跨数据中心 / 部署 Anti Affinity
		统一监控所有集群的状态
		图形化操作
			增加删除节点 
			升级集群 / 滚动更新 / 自动数据备份


基于 Kubernetes 的方案
	基于容器技术，使用 Operator 模式进行编排管理 
	配置，管理监控多个集群 
	支持 Hot & Warm 
	数据快照和恢复


Kubernetes Operator 模式





-----------



## 生产环境常用配置和上线清单


Development vs. Production Mode
	从 ES 5 开始，支持 Development 和 Production 两种运行模式

	Dev Cluster 
		http.port: 9200 
		http.host: localhost 
		Transfort.tcp.port: 9300 
		Transport.bind_host: localhost
	Production Cluster	
		http.port: 9200 
		http.host: 192.168.1.32 
		Transfort.tcp.port: 9300 
		Transport.bind_host: 192.168.1.32


Bootstrap Checks
	一个集群在 Production Mode 时，启动时必须通过所有 Bootstrap 检测，否则会启动失败
	Bootstrap Checks 可以分为两类：JVM & Linux Checks。Linux Checks 只针对 Linux 系统


JVM 设定
	从 ES 6 开始， 只支持 64 位 的JVM
		配置 config / jvm.options
	避免修改默认配置
		将 内存 Xms 和 Xmx 设置成一样，避免 heap resize 时引发停顿
		Xmx 设置不要超过物理内存的 50%；单个节点上，最大内存建议不要超过 32 G 内存
		生产环境，JVM 必须使用 Server 模式
		关闭 JVM Swapping


集群的 API 设定

	优先级 	设置 
	1 		Transient Settings 
	2 		Persistent Settings 
	3 		Command-line settings 
	4 		Config file settings


	静态设置和动态设定
		静态配置文件尽量简洁：按照文档设置所有相关系统参数。 
		elasticsearch.yml 配置文件中尽量只写必备参数

	其他的设置项可以通过 API 动态进行设定。 
	动态设定分 transient 和 persistent 两种， 都会覆盖 elasticsearch.yaml 中的设
		Transient 在集群重启后会丢失
		Persistent 在集群中重启后不会丢失


最佳实践：网络
	单个集群不要跨数据中心进行部署（不要使用 WAN）
	节点之间的 hops 越少越好
	如果有多块网卡，最好将 transport 和 http 绑定到不同的网卡，并设置不同的防火墙 Rules
	按需为 Coordinating Node 或 Ingest Node 配置负载均衡


最佳实践: 内存设定计算实例
	内存大小要根据 Node 需要存储的数据来进行估算
		搜索类的比例建议： 1:16
		日志类： 1:48 - 1:96 之间
	总数据量 1 T， 设置一个副本 = 2T 总数据量	
		如果搜索类的项目，每个节点 31 *16 = 496 G，加上预留空间。所以每个节点最多 400 G 数据，至少需 要 5 个数据节点
		如果是日志类项目，每个节点 31*50 = 1550 GB，2 个数据节点 即可


最佳实践：存储
	推荐使用 SSD，使用本地存储（Local Disk）。避免使用 SAN NFS / AWS / Azure filesystem	
	可以在本地指定多个 “path.data”，以支持使用多块磁盘
	ES 本身提供了很好的 HA 机制；无需使用 RAID 1/5/10
	可以在 Warm 节点上使用 Spinning Disk，但是需要关闭 Concurrent Merges
		Index.merge.scheduler.max_thread_count: 1
	Trim 你的 SSD	
		https://www.elastic.co/blog/is-your-elasticsearch-trimmed


最佳实践：服务器硬件
	建议使用中等配置的机器，不建议使用过于强劲的硬件配置
		Medium machine over large machine	
	不建议在一台服务器上运行多个节点	


集群设置：Throttles 限流
	为 Relocation 和 Recovery 设置限流，避免过多任务对集群产生性能影响	
	Recovery 
		Cluster.routing.allocation.node_concurrent_recoveries: 2 
	Relocation 
		Cluster.routing.allocation.cluster_concurrent_rebalance: 2


集群设置：关闭 Dynamic Indexes
	可以考虑关闭动态索引创建的功能
		PUT _cluster/Settings
		{
			"Persistent":{
				"acton.auto_create_index":false
			}
		}

或者通过模版设置白名单
		PUT _cluster/Settings
		{
			"Persistent":{
				"acton.auto_create_index":"logstash-*,kibana*"
			}
		}


集群安全设定
	为 Elasticsearch 和 Kibana 配置安全功能	
		打开 Authentication & Authorization
		实现索引和和字段级的安全控制
	节点间通信加密	
	Enable HTTPS
	Audit logs(审计日志)




--------


## 集群写性能优化

提高写入性能的方法
	写性能优化的目标：增大写吞吐量（Events Per Second），越高越好
	客户端：多线程，批量写
		可以通过性能测试，确定最佳文档数量
		多线程：需要观察是否有 HTTP 429 返回，实现 Retry 以及线程数量的自动调节
	服务器端：单个性能问题，往往是多个因素造成的。需要先分解问题，在单个节点上进行调整并 且结合测试，尽可能压榨硬件资源，以达到最高吞吐量	
		使用更好的硬件。观察 CPU / IO Block
		线程切换 / 堆栈状况


服务器端优化写入性能的一些手段
	降低 IO 操作
		使用 ES 自动生成的文档 Id / 一些相关的 ES 配置，如 Refresh Interval
	降低 CPU 和存储开销	
		减少不必要分词 / 避免不需要的 doc_values /文档的字段尽量保证相同的顺序，可以提高文档的压缩率
	尽可能做到写入和分片的均衡负载，实现水平扩展	
		Shard Filtering / Write Load Balancer
	调整 Bulk 线程池和队列	


优化写入性能
	ES 的默认设置，已经综合考虑了数据可靠性，搜索的实时性质，写入速度，一般不要盲目修改
	一切优化，都要基于高质量的数据建模


关闭无关的功能
	只需要聚合不需要搜索， Index 设置成 false
	不需要算分， Norms 设置成 false
	不要对字符串使用默认的 dynamic mapping。字段 数量过多，会对性能产生比较大的影响
	Index_options 控制在创建倒排索引时，哪些内容 会被添加到倒排索引中。优化这些设置，一定程度 可以节约 CPU
	关闭 _source，减少 IO 操作；（适合指标型数据）


针对性能的取舍
	如果需要追求极致的写入速度，可以牺牲数据可靠性及搜索实时性以换取性能
		牺牲可靠性：将副本分片设置为 0，写入完毕再调整回去
		牺牲可靠性：修改 Translog 的配置
		牺牲搜索实时性：增加 Refresh Interval 的时间


数据写入的过程
	Refresh
		将文档先保存在 Index buffer 中， 以 refresh_interval 为间隔时间，定期清空 buffer，生成 segment，借助文件系统缓存的特性，先将 segment 放在文件系统缓存中，并开放查询，以提升搜索的实 时性
	Translog
		Segment 没有写入磁盘，即便发生了当机，重启后，数据也能恢复，默认配置是每次请求都会落盘
	Flush	
		删除旧的 translog 文件
		生成 Segment 并写入磁盘 / 更新 commit point 并写入磁盘。 ES 自动完成，可优化点不多


Refresh Interval
	降低 Refresh 的频率
		增加 refresh_interval 的数值。默认为 1s ，如果设置成 -1 ，会禁止自动 refresh
			避免过于频繁的 refresh，而生成过多的 segment 文件
			但是会降低搜索的实时性	
		增大静态配置参数 indices.memory.index_buffer_size		
			默认是 10%， 会导致自动触发 refresh


Translog
	降低写磁盘的频率，但是会降低容灾能力
		Index.translog.durability：默认是 request，每个请求都落盘。设置成 async，异步写入
		Index.translog.sync_interval 设置为 60s，每分钟执行一次
		Index.translog.flush_threshod_size: 默认 512 mb，可以适当调大。 当 translog 超过该值，会触发 flush	


分片设定
	副本在写入时设为 0，完成后再增加
	合理设置主分片数，确保均匀分配在所有数据节点上
		Index.routing.allocation.total_share_per_node: 限定每个索引在每个节点上可分配的主分片数
	5 个节点的集群。 索引有 5 个主分片，1 个副本，应该如何设置？	
		（5+5） / 5 = 2
		生产环境中要适当调大这个数字，避免有节点下线时，分片无法正常迁移


Bulk，线程池和队列大小
	客户端
		单个 bulk 请求体的数据量不要太大，官方建议大约5-15mb
		写入端的 bulk 请求超时需要足够长，建议60s 以上
		写入端尽量将数据轮询打到不同节点
	服务器端		
		索引创建属于计算密集型任务，应该使用固定大小的线程池来配置。
		来不及处理的放入队列，线程数应该 配置成 CPU 核心数 +1 ，避免过多的上下文切换
		队列大小可以适当增加，不要过大，否则占用的内存会成为 GC 的负担


一个索引设定的例子
PUT my_index
{
  "settings": {
    "index":{
      "refresh_interval":"30s",
      "number_of_shards":2
    },
    "routing": {
      "allocation": {
        "total_shards_per_node": "3"
      }
    },
    "translog":{
      "sync_interval":"30s",
      "durability":"async"
    },
    "number_of_replicas": 0
  },
  "mappings": {
    "dynamic":false, // 避免不必要的字段索引。必要 时可以通过 update by query 索引必要的字段
    "properties": {
    }
  }
}



----------


## 集群读性能优化
	
尽量 Denormalize 数据
	Elasticsearch ！= 关系型数据库
	尽可能 Denormalize 数据，从而获取最佳的性能
		使用 Nested 类型的数据。查询速度会慢几倍
		使用 Parent / Child 关系。查询速度会慢几百倍

数据建模
	尽量将数据先行计算，然后保存到 Elasticsearch 中。尽量避免查询时的 Script 计算
	尽量使用 Filter Context，利用缓存机制，减少不必要的算分
	结合 profile，explain API 分析慢查询的问题，持续优化数据模型
		严禁使用 * 开头通配符 Terms 查询


避免查询时脚本
	可以在 Index 文档时，使用 Ingest Pipeline，计算并写入某个字段
		PUT blogs/_doc/1
		{
		  "title":"elk"
		}
		GET blogs/_search
		{
		  "query": {
		    "bool": {
		      "must": [
		        {
		          "match": {
		            "title": "elk"
		          }
		        }
		      ],
		      "filter": {
		        "script": {
		          "script": {
		            "source": "doc['title.keyword'].value.length()>5"
		          }
		        }
		      }
		    }
		  }
		}



常见的查询性能问题 - 使用 Query Context
	GET blogs/_search
	{
	  "query": {
	    "bool": {
	      "must": [
	        {
	          "match": {
	            "title": "elk"
	          }
	        },
	        {
	          "range": {
	            "publish_data": {
	              "gte": 2017,
	              "lte": 2019
	            }
	          }
	        }
	      ]
	    }
	  }
	}

	GET blogs/_search
	{
	  "query": {
	    "bool": {
	      "must": {
	        "match": {
	          "title": "elk"
	        }
	      },
	      "filter": {
	        "range": {
	          "publish_data": {
	            "gte": 2017,
	            "lte": 2019
	          }
	        }
	      }
	    }
	  }
	}



聚合文档消耗内存
	聚合查询会消耗内存，特别是针对很大的数据集进行聚合运 算	
		如果可以控制聚合的数量，就能减少内存的开销
	当需要使用不同的 Query Scope，可以使用 Filter Bucket	
	GET logstash*/_search
	{
	  "size": 0,
	  "query": {
	    "bool": {
	      "filter": {
	        "range": {
	          "runtime": {
	            "gte": 10,
	            "lte": 20
	          }
	        }
	      }
	    }
	  },
	  "aggs": {
	    "my_aggs": {
	      "range": {
	        "field": "",
	        "ranges": [
	          {
	            "from": 50,
	            "to": 100
	          }
	        ]
	      }
	    }
	  }
	}



通配符开始的正则表达
	通配符开头的正则，性能非常糟糕，需避免使用


优化分片
	避免 Over Sharing
		一个查询需要访问每一个分片，分片过多，会导致不必要的查询开销
	结合应用场景，控制单个分片的尺寸	
		Search： 20GB 
		Logging：40GB
	Force-merge Read-only 索引	
		使用基于时间序列的索引，将只读的索引进行 force merge，减少 segment 数量


读性能优化
	影响查询性能的一些因素 
		数据模型和索引配置是否优化 
		数据规模是否过大，通过 Filter 减少不必要的数据计算 
		查询语句是否优化



----------


## 诊断集群的潜在问题



防患于未然，避免集群奔溃
	Master 节点 / 数据节点当机 – 负载过高，导致节点失联
	副本丢失，导致数据可靠性受损
	集群压力过大，数据写入失败
提升集群性能	
	数据节点负载不均衡（避免单节点瓶颈） / 优化分片，segment
	规范操作方式（利用别名 / 避免 Dynamic Mapping 引发过多字段，对索引的合理性进行管控）


阿里云 – EYOU 智能运维工具



---------



## 解决集群 Yellow 与 Red 的问题


集群健康度
	分片健康
		红：至少有一个主分片没有分配
		黄：至少有一个副本没有分配
		绿：主副本分片全部正常分配
	索引健康: 最差的分片的状态
	集群健康: 最差的索引的状态


Health 相关的 API
	GET _cluster/health 集群的状态（检查 节点数量）
	GET _cluster/health?level=indices 所有索引的健康状态 （查看有问题的 索引）
	GET _cluster/health?level=shards 分片级的索引  
	GET _cluster/health/my_index 单个索引的健康状 态（查看具体的索 引） 
	GET _cluster/allocation/explain 返回第一个未分配 Shard 的原因



案例 1
	症状：集群变红
	分析：通过 Allocation Explain API 发现 创建索引失败，因为无法找到标记了相应 box type 的节点
	解决：删除索引，集群变绿。重新创建索引，并且指定正确的 routing box type，索引创建成 功。集群保持绿色状态


案例 2
	症状：集群变黄
	分析：通过 Allocation Explain API 发现无法在相同的节点上创建副本
	解决：将索引的副本数设置为 0，或者通过增加节点解决


分片没有被分配的一些原因
	INDEX_CREATE: 创建索引导致。在索引的全部分片分配完成之前，会有短暂的 Red，不一定代表 有问题
	CLUSTER_RECOVER：集群重启阶段，会有这个问题	
	INDEX_REOPEN：Open 一个之前 Close 的索引
	DANGLING_INDEX_IMPORTED：一个节点离开集群期间，有索引被删除。这个节点重新返回时，会 导致 Dangling 的问题



常见问题与解决方法
	集群变红，需要检查是否有节点离线。如果有，通常通过重启离线的节点可以解决问题
	由于配置导致的问题，需要修复相关的配置（例如错误的 box_type，错误的副本数）
		如果是测试的索引，可以直接删除
	因为磁盘空间限制，分片规则（Shard Filtering）引发的，需要调整规则或者增加节点	
	对于节点返回集群，导致的 dangling 变红，可直接删除 dangling 索引	


集群 Red & Yellow 问题的总结
	除了集群故障，一些创建，增加副本等操作， 都会导致集群短暂的 Red 和 Yellow，所以 监控和报警时需要设置一定的延时	
	通过检查节点数，使用 ES 提供的相关 API， 找到真正的原因
	可以指定 Move 或者 Reallocate 分片
		PUT _cluster/reroute
		{
		  "commands":[
		    {
		      "move":{
		        "index":"index_name",
		        "shard":0,
		        "from_node":"node_name_1",
		        "to_node":"node_name_2"
		      }
		    }
		  ]
		}

		POST _cluster/reroute?explain
		{
		  "commands": [
		    {
		      "allocate": {
		        "index": "index_name",
		        "shard": 0,
		        "node": "nodename"
		      }
		    }
		  ]
		}



--------------



## 集群压力测试


ES Rally
	Elastic 官方开源，基于 Python 3 的压力测试工具
	功能介绍
		自动创建，配置，运行测试，并且销毁 ES 集群	
		支持不同的测试数据的比较，也支持将数据导入 ES 集群，进行二次分析
		支持测试时指标数据的搜集，方便对测试结果进行深度的分析
	运行 esrally –distribution-version=7.1.0	
	运行 1000 条测试数据： esrally –distribution-version=7.1.0 --test-mode



实例：比较不同的版本的性能
	测试
		esrally race --distribution-version=6.0.0 --track=nyc_taxis --challenge=append-no- conflicts --user-tag="version:6.0.0”
		esrally race --distribution-version=7.1.0 --track=nyc_taxis --challenge=append-no- conflicts --user-tag="version:7.1.0"
	比较结果	
		esrally list races
		esrally compare --baseline=[6.0.0 race] --contender=[7.1.0 race]


实例：比较不同 Mapping 的性能
	测试
		esrally race --distribution-version=7.1.0 --track=nyc_taxis --challenge=append-no- conflicts --user-tag="enableSource:true" --include-tasks="type:index”
		修改：benchmarks/tracks/default/nyc_taxis/mappings.json，修改 _source.enabled 为 false
		esrally race --distribution-version=7.1.0 --track=nyc_taxis --challenge=append-no- conflicts --user-tag="enableSource:false" --include-tasks="type:index
	比较	
		esrally compare --baseline=[enableAll race] --contender=[disableAll race]


实例：测试现有集群的性能
	esrally race --pipeline=benchmark-only --target-hosts=127.0.0.1:9200 --track=geonames -- challenge=append-no-conflicts




---------


## 段合并优化及注意事项



Lucene Index 原理回顾
	An ES Shard = A Lucene Index
	在 Lucene 中，单个倒排索引文件被称为 Segment。Segment 是自包含的，不可变更的。 多个 Segments 汇总在一起，称为 Lucene 的 Index，其对应的就是 ES 中的 Shard
	当有新文档写入时，并且执行 Refresh，就会 会生成一个新 Segment。 Lucene 中有一个文 件，用来记录所有 Segments 信息，叫做 Commit Point。查询时会同时查询所有 Segments，并且对结果汇总。
	删除的文档信息，保存在“.del”文件中，查 询后会进行过滤。
	Segment 会定期 Merge，合并成一个，同时删 除已删除文档


Merge 优化
	ES 和 Lucene 会自动进行 Merge 操作
	Merge 操作相对比较重，需要优化，降低对系统的影响
	优化点一：降低分段产生的数量/频率
		可以将 Refresh Interval 调整到分钟级别 / indices.memory.index_buffer_size (默认是 10%)
		尽量避免文档的更新操作
	优化点二：降低最大分段大小，避免较大的分段继续参与 Merge，节省系统资源。（最 终会有多个分段）	
		Index.merge.policy.segments_per_tier，默认为 10， 越小需要越多的合并操作
		Index.merge.policy.max_merged_segment, 默认 5 GB， 超过此大小以后，就不再参与后续的 合并操作


Force Merge
	当 Index 不再有写入操作的时候，建议对其进行 force merge
		提升查询速度 / 减少内存开销
	最终分成几个 segments 比较合适？	
		越少越好，最好可以 force merge 成 1 个，但是，Force Merge 会占用大量的网络，IO 和 CPU		
		如果不能在业务高峰期之前做完，就需要考虑增大最终的分段数
			Shard 的大小 / Index.merge.policy.max_merged_segment 的大小



------------


## 缓存及使用 Circuit Breaker 限制内存使用


Inside the JVM Heap
	Elasticsearch 的缓存主要分成三大类
		Node Query Cache （Filter Context）
		Shard Query Cache （Cache Query的结果）
		Fielddata Cache


Node Query Cache
	每一个节点有一个 Node Query 缓存
		由该节点的所有 Shard 共享，只缓存 Filter Context 相关内容
		Cache 采用 LRU 算法
	静态配置，需要设置在每个 Data Node 上	
		Node Level - indices.queries.cache.size: ”10%”
		Index Level: index.queries.cache.enabled: true


Shard Request Cache
	缓存每个分片上的查询结果
		只会缓存设置了 size=0 的查询对应的结果。不会缓存 hits。但是会缓存 Aggregations 和 Suggestions
	Cache Key	
		LRU 算法，将整个 JSON 查询串作为 Key，与 JSON 对 象的顺序相关	
	静态配置	
		数据节点：indices.requests.cache.size: “1%”


Fielddata Cache
	除了 Text 类型，默认都采用 doc_values。节约了内存
		Aggregation 的 Global ordinals 也保存在 Fielddata cache 中
	Text 类型的字段需要打开 Fileddata 才能对其进行聚合和排序	
		Text 经过分词，排序和聚合效果不佳，建议不要轻易使用
	配置	
		可以控制 Indices.fielddata.cache.size, 避免产生 GC （默认无限制）


缓存失效
	Node Query Cache
		保存的是 Segment 级缓存命中的结果。Segment 被合并后，缓存会失效
	Shard Request Cache	
		分片 Refresh 时候，Shard Request Cache 会失效。如果 Shard 对应的数据频繁发生变化，该缓存的效 率会很差
	Fielddata Cache	
		Segment 被合并后，会失效	


管理内存的重要性
	Elasticsearch 高效运维依赖于内存的合理分配
		可用内存一半分配给 JVM，一半留给操作系统，缓存索引文件
	内存问题，引发的问题	
		长时间 GC，影响节点，导致集群响应缓慢
		OOM， 导致丢节点


查看各个节点的内存状况
	GET _cat/nodes?v  
	GET _nodes/stats/indices?pretty 
	GET _cat/nodes?v&h=name,queryCacheMemory,queryCacheEvictions,requestCacheMemory,reques tCacheHitCount,request_cache.miss_count 
	GET _cat/nodes?h=name,port,segments.memory,segments.index_writer_memory,fielddata.memo ry_size,query_cache.memory_size,request_cache.memory_size&v


一些常见的内存问题
	Segments 个数过多，导致 full GC
		现象：集群整体响应缓慢，也没有特别多的数据读写。但是发现节点在持续进行 Full GC
		分析：查看 Elasticsearch 的内存使用，发现 segments.memory 占用很大空间
		解决：通过 force merge，把 segments 合并成一个。	
		建议：
			对于不在写入和更新的索引，可以将其设置成只读。
			同时，进行 force merge 操作。如 果问题依然存在，则需要考虑扩容。
			此外，对索引进行 force merge ，还可以减少对 global_ordinals 数据结构的构建，减少对 fielddata cache 的开销

	Field data cache 过大，导致 full GC		
		现象：集群整体响应缓慢，也没有特别多的数据读写。但是发现节点在持续进行 Full GC
		分析：查看 Elasticsearch 的内存使用，发现 fielddata.memory.size 占用很大空间。同时， 数据不存在写入和更新，也执行过 segments merge。
		解决：将 indices.fielddata.cache.size 设小，重启节点，堆内存恢复正常
		建议：
			Field data cache 的构建比较重，Elasticsearch 不会主动释放，所以这个值应该设置 的保守一些。
			如果业务上确实有所需要，可以通过增加节点，扩容解决

	复杂的嵌套聚合，导致集群 full GC		
		现象：节点响应缓慢，持续进行 Full GC	
		分析：导出 Dump 分析。发现内存中有大量 bucket 对象，查看 日志，发现复杂的嵌套聚合
		解决：优化聚合
		建议：
			在大量数据集上进行嵌套聚合查询，需要很大的堆内存来完成。
			如果业务场景确实需要。 则需要增加硬件进行扩展。
			同时，为了避免这类查询影响整个集群，需要设置 Circuit Breaker 和 search.max_buckets 的数值


Circuit Breaker
	包含多种断路器，避免不合理操作引发的 OOM，每个断路器可以指定内存使用的限制
		Parent circuit breaker：设置所有的熔断器可以使用的内存的总量
			indices.breaker.total.limit: 70%
			indices.breaker.total.use_real_memory: true
			父级断路器是否应考虑实际内存使用情况（true）或仅考虑子级断路器保留的数量（false）。默认为true。
		Fielddata circuit breaker：加载 fielddata 所需要的内存 Elasticsearch系统会估计有多少数据被加载到内存中。 它可以避免因为列字段加载（缓存）增长过多带来的异常。
			index.breaker.fielddata.limit默认为JVM堆的40％
			indices.breaker.fielddata.overhead 所有列数据乘以一个常量得到最终的值。 默认为1.03
		Request circuit breaker：防止每个请求级数据结构超过一定的内存（例如聚合计算的内存）
			indices.breaker.request.limit 默认为JVM堆的60%
			indices.breaker.request.overhead 默认为1
		In flight circuit breaker：Request中的断路器 内存使用是基于请求本身的内容长度。
			network.breaker.inflight_requests.limit(inflight_requests) 请求中熔断器，默认为100％的JVM堆。 这意味着受限于父母断路器配置的极限。
			network.breaker.inflight_requests.overhead 默认为2
		Accounting request circuit breaker：请求结束后不能释放的对象所占用的内存
			PUT _cluster
			{
				"settings":{
					"persistent":{
						"indices.breaker.request.limit":"45%"
					}
				}
			}	

Circuit Breaker 统计信息
	GET /_nodes/stats/breaker?
		Tripped 大于 0， 说明有过熔断
		Limit size 与 estimated size 约接近，越可能引发熔断
	千万不要触发了熔断，就盲目调大参数，有可能会导致集群出问 题，也不因该盲目调小，需要进行评估	
	建议将集群升级到 7.x，更好的 Circuit Breaker 实现机制
		增加了 indices.breaker.total.use_real_memory 配置项，可以更 加精准的分析内存状况，避免 OOM




----------





## 监控 Elasticsearch 集群


Elasticsearch Stats 相关的 API
	# Node Stats：
		GET _nodes/stats
	#Cluster Stats:
		GET _cluster/stats
	#Index Stats:
		GET kibana_sample_data_ecommerce/_stats


查看 Task 相关的 API
	#Pending Cluster Tasks API:
		GET _cluster/pending_tasks
	# 查看所有的 tasks，也支持 cancel task
		GET _tasks

监控 Thread Pools
	GET _nodes/thread_pool
	GET _nodes/stats/thread_pool
	GET _cat/thread_pool?v
	GET _nodes/hot_threads
	GET _nodes/stats/thread_pool


# 设置 Index Slowlogs
	支持将分片上， Search 和 Fetch 阶段的慢查询写入文件
	支持为 Query 和 Fetch 分别定义阈值	
	索引级的动态设置，可以按需设置，或者通 过 Index Template 统一设定
	Slog log 文件通过 log4j2.properties 配 置

DELETE my_index
//"0" logs all queries
PUT my_index/
{
  "settings": {
    "index.search.slowlog.threshold": {
      "query.warn": "10s",
      "query.info": "3s",
      "query.debug": "2s",
      "query.trace": "0s",
      "fetch.warn": "1s",
      "fetch.info": "600ms",
      "fetch.debug": "400ms",
      "fetch.trace": "0s"
    }
  }
}


# the first 1000 characters of the doc's source will be logged
PUT my_index/_settings
{
  "index.indexing.slowlog":{
    "threshold.index":{
      "warn":"10s",
      "info": "4s",
      "debug":"2s",
      "trace":"0s"
    },
    "level":"trace",
    "source":1000  
  }
}



--------------


## 索引全生命周期管理及工具介绍


索引生命周期常见的阶段
	Hot Warm Cold Delete
		Hot: 索引还存在着大量的读写操作
		Warm：索引不存在写操作，还有被查询的需要
		Cold：数据不存在写操作，读操作也不多
		Delete：索引不再需要，可以被安全删除


Elasticsearch Curator
	配置 Actions
		内置 10 多种 Index 相关的操作
		每个动作可以顺序执行
	Filters	
		支持各种条件，过滤出需要操作的索引


Index Lifecycle Management
	基于 X-Pack Basic License，可免费使用
	ILM 概念
		Policy
		Phase
		Action





# 运行三个节点，分片 将box_type设置成 hot，warm和cold
# 具体参考 github下，docker-hot-warm-cold 下的docker-compose 文件


# 设置 1秒刷新1次，生产环境10分种刷新一次
PUT _cluster/settings
{
  "persistent": {
    "indices.lifecycle.poll_interval":"1s"
  }
}

# 设置 Policy
PUT /_ilm/policy/log_ilm_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_docs": 5
          }
        }
      },
      "warm": {
        "min_age": "10s",
        "actions": {
          "allocate": {
            "include": {
              "box_type": "warm"
            }
          }
        }
      },
      "cold": {
        "min_age": "15s",
        "actions": {
          "allocate": {
            "include": {
              "box_type": "cold"
            }
          }
        }
      },
      "delete": {
        "min_age": "20s",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}



# 设置索引模版
PUT /_template/log_ilm_template
{
  "index_patterns" : [
      "ilm_index-*"
    ],
    "settings" : {
      "index" : {
        "lifecycle" : {
          "name" : "log_ilm_policy",
          "rollover_alias" : "ilm_alias"
        },
        "routing" : {
          "allocation" : {
            "include" : {
              "box_type" : "hot"
            }
          }
        },
        "number_of_shards" : "1",
        "number_of_replicas" : "0"
      }
    },
    "mappings" : { },
    "aliases" : { }
}



#创建索引
PUT ilm_index-000001
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "index.lifecycle.name": "log_ilm_policy",
    "index.lifecycle.rollover_alias": "ilm_alias",
    "index.routing.allocation.include.box_type":"hot"
  },
  "aliases": {
    "ilm_alias": {
      "is_write_index": true
    }
  }
}

# 对 Alias写入文档
POST  ilm_alias/_doc
{
  "dfd":"dfdsf"
}


























































