## 杂

### 集群、分布式

- 集群
  - 集群，是指同一种组件的多个实例，形成的逻辑上的整体。
  - 单机处理到达瓶颈的时候，你就把单机复制几份，这样就构成了一个“集群”。集群中每台服务器就叫做这个集群的一个“节点”，所有节点构成了一个集群。每个节点都提供相同的服务，那么这样系统的处理能力就相当于提升了好几倍
  - 但问题是用户的请求究竟由哪个节点来处理呢
    - 负载均衡服务器
  - 当你的业务发展到一定程度的时候，你会发现一个问题——无论怎么增加节点，貌似整个集群性能的提升效果并不明显了。这时候，你就需要使用微服务结构了。
  - 分布式结构就是将一个完整的系统，按照业务功能，拆分成一个个独立的子系统，在分布式结构中，每个子系统就被称为“服务”。这些子系统能够独立运行在web容器中，它们之间通过RPC方式通信。
- 分布式
  - 分布式不一定就是不同的组件，同一个组件也可以，关键在于是否通过交换信息的方式进行协作。比如说Zookeeper的节点都是对等的，但它自己就构成一个分布式系统。
  - 也就是说，分布式是指通过网络连接的多个组件，通过交换信息协作而形成的系统。
  - 好处
    - 系统之间的耦合度大大降低，可以独立开发、独立部署、独立测试
    - 系统之间的耦合度降低，从而系统更易于扩展
    - 服务的复用性更高。比如，当我们将用户系统作为单独的服务后，该公司所有的产品都可以使用该系统作为用户系统，无需重复开发。
- 可以看出这两个概念并不完全冲突，分布式系统也可以是一个集群，例子就是前面说的zookeeper等，它的特征是服务之间会互相通信协作。
- 情况
  - 是分布式系统不是集群的情况，就是多个不同组件构成的系统
  - 是集群不是分布式系统的情况，比如多个经过负载均衡的HTTP服务器，它们之间不会互相通信，如果不带上负载均衡的部分的话，一般不叫做分布式系统。

### 数据库高并发解决方法

- 概述
  - 关键是如何解决慢和等，核心一个是短，一个是少，一个是分流,最后一个是集群/横向扩张/读写分离/建立主从。
    - 短是指路径（请求）要短
      - 页面静态化
      - 缓存
      - 储存过程
      - 批量读取
      - 延迟修改
      - 索引
    - 少是指查询的数据要少
      - 分表
      - 分离活跃数据
      - 分块
    - 分流
      - 集群
        - 并发请求分配到不同的服务器上
      - 分布式
        - 把单次请求的多项业务逻辑分配到多个服务器上
      - CDN
        - 例如将华南地区的用户请求分配到华南的服务器，华中地区的用户请求分配到华中的服务器
- 解决数据库高并发访问瓶颈问题
  - 缓存式的Web应用程序架构
    - 在Web层和db层之间加一层cache层
  - 业务拆分
    - 每一个模块都使用单独的数据库来进行存储，不同的业务访问不同的数据库
  - MySQL主从复制，读写分离
    - 主从复制技术（master-slave模式）来达到读写分离，以提高读写性能和读库的可扩展性
    - 主从复制
      - 数据复制的实际就是Slave从Master获取Binary log文件，然后在本地镜像的执行日志中记录的操作
    - 读写分离
      - 只在主服务器上写，只在从服务器上读
      - 让主数据库处理事务性查询，而从数据库处理select查询
      - 数据库复制被用于把事务性查询（增删改）导致的改变更新同步到集群中的从数据库
    - 实现主从分离可以使用MySQL中间件如：Atlas
  - 分表分库
    - 采用Master-Slave复制模式的MySQL架构，只能对数据库的读进行扩展，而对数据的写操作还是集中在Master上
    - 分表
      - 对于访问极为频繁且数据量巨大的单表来说，首先要做的是减少单表的记录条数
    - 分表能够解决单表数据量过大带来的查询效率下降的问题，但是却无法给数据库的并发处理能力带来质的提升
      - 分表的实质还是在一个数据库上进行的操作，很容易受数据库IO性能的限制
    - 分库
      - 当数据库master服务器无法承载写操作压力时，不管如何扩展Slave服务器都是没有意义的
    - 数据库分表可以解决单表海量数据的查询性能问题，分库可以解决单台数据库的并发访问压力问题
    - 经过业务拆分及分库分表，虽然查询性能和并发处理能力提高了。但是原本跨表的事务上升为分布式事务
    - 分库分表后需要进一步对系统进行扩容（路由策略变更）将变得非常不方便，需要重新进行数据迁移
    - 分库分表的策略
      - １、中间变量　＝ user_id%（库数量*每个库的表数量）; 　　

　　２、库序号　＝　取整（中间变量／每个库的表数量）; 　　

　　３、表序号　＝　中间变量％每个库的表数量;

### 分布式架构系统生成全局唯一序列号

- 概述
  - 分布式架构下，唯一序列号生成是我们在设计一个系统，尤其是数据库使用分库分表的时候常常会遇见的问题
  - 当分成若干个sharding表后，如何能够快速拿到一个唯一序列号，是经常遇到的问题
- 需求
  - 全局唯一
    支持高并发
    能够体现一定属性
    高可靠，容错单点故障
    高性能
    - 通常分布式系统采用主从模式，一个主机连接多个处理节点，主节点负责分发任务，而子节点负责处理业务，当主节点发生故障时，会导致整个系统发故障，我们把这种故障叫做单点故障
- 业内方案
  - Snowflake 算法
    - 全局唯一ID生成服务
    - 41位的时间序列
    - 10位的机器标识
    - 12位的计数顺序号
    - snowflake的结构如下(每部分用-分开)： 
      0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000
    - 一共加起来刚好64位，为一个Long型
    - 生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞
    - 缺点：需要独立的开发和部署
  - Redis生成ID
    - Redis是单线程的，所以也可以用生成全局唯一的ID。可以用Redis的原子操作INCR和INCRBY来实现
    - 可以使用Redis集群来获取更高的吞吐量
    - 可以初始化每台Redis的值分别是1,2,3,4,5，然后步长都是5
    - 比较适合使用Redis来生成每天从0开始的流水号。比如订单号=日期+当日自增长号
    - 需要编码和配置的工作量比较大，多环境运维很麻烦
  - Flicker的解决方案
    - MySQL本身支持auto_increment操作
    - Flicker在解决全局ID生成方案里就采用了MySQL自增长ID的机制（auto_increment + replace into + MyISAM）
  - 其他一些方案
    - 订单号尽可能要多些冗余的业务信息
      - 滴滴：时间+起点编号+车牌号
      - 淘宝订单：时间戳+用户ID
    - 携程方案
      - 以flicker方案为基础进行优化改进。具体实现是，单表递增，内存缓存号段的方式
        - replace to来更新记录来获得唯一id
        - replace into 首先尝试插入数据到表中， 如果发现表中已经有此行数据（根据主键或者唯一索引判断）则先删除此行数据，然后插入新的数据
        - 插入数据的表必须有主键或者是唯一索引！否则的话，replace into 会直接插入数据，这将导致表中出现重复的数据
        - 再用 SELECT id FROM sequenceid WHERE ip = “192.168.1.1”  把它拿回来
      - 但是追根溯源，在原理上，方案还是依靠数据库的特性，每次生成id都要请求db，开销很大
      - 把这个id作为一个号段，而并不是要发出去的序列号
      - 现在的问题就是要解决同一台服务器在高并发场景，让大家顺序拿号，别拿重复，也别漏拿
        - 保持这个号段对象隔离性的问题
          - 当第一次拿回号段id后，扩大1000倍，然后赋值给这个变量atomic，这就是这个号段的第一个号码。

atomic.set(n * 1000);

并且内存里保存一下最大id，也就是这个号段的最后一个号码

currentMaxId = (n + 1) * 1000;

一个号段就形成了。
					- 此时每次有请求来取号时候，判断一下有没有到最后一个号码，没有到，就拿个号，走人

```
					- Long uid = atomic.incrementAndGet();

				- 如果到达了最后一个号码，那么阻塞住其他请求线程，最早的那个线程去db取个号段，再更新一下号段的两个值，就可以了。

	- 美团

		- 用户通过Round-robin的方式调用Leaf Server的各个服务，所以某一个Client获取到的ID序列可能是：1，1001，2001，2，1002，2002……也可能是：1，2，1001，2001，2002，2003，3，4……当某个Leaf Server号段用完之后，下一次请求就会从DB中加载新的号段，这样保证了每次加载的号段是递增的。
		- DB压力
		- 采用了异步更新的策略，同时通过双Buffer的方式，保证无论何时DB出现问题，都能有一个Buffer的号段可以正常对外提供服务
		- 动态调整Step

			- 服务QPS为Q，号段长度为L，号段更新周期为T，那么Q * T = L
			- Leaf本质的需求是希望T是固定的。那么如果L可以和Q正相关的话，T就可以趋近一个定值了
			- Leaf每次更新号段的时候，根据上一次更新号段的周期T和号段长度step，来决定下一次的号段长度nextStep

				- - T < 15min，nextStep = step * 2
```

- 15min < T < 30min，nextStep = step
- T > 30min，nextStep = step / 2

### 高并发系统的设计

- 设置http连接池
  - 如果不采用连接池，每次连接发起Http请求的时候 都会重新建立TCP连接(经历3次握手)，用完就会关闭连接(4次挥手)，如果采用连接池则减少了这部分时间损耗
  - 采用连接池，连接的复用，可以提高并发访问量
  - 降低延迟，支持更大的并发
- 把一些静态资源先加载到浏览器缓存里面
- 可以对服务器端的数据进行压缩
- 反向代理服务器可以保护服务器的安全（服务器的负载均衡）
- NIO模型
  - 解决线程资源受限的方案，实际开发过程中，我们会开多个线程，每个线程都管理着一批连接
  - 处理器访问任何寄存器和 Cache 等封装以外的数据资源都可以当成 I/O 操作，包括内存，磁盘，显卡等外部设备。
- 线程池
  - 设置一个最大线程数量和最小线程数量
  - 阻塞队列的大小要有界
    - 阻塞队列
      - 线程把请求放到阻塞队列里面
  - 线程池的失败策略
- 数据库连接池
- 数据存储部分
  - 数据库的优化，包括合理的事务隔离级别、SQL语句优化、索引的优化
- 缓存
  - 使用缓存，尽量减少数据库 IO
  - 分布式数据库、分布式缓存
- 数据库考虑集群、分库分表。
- 一致性哈希算法实现分布式缓存数据库
- 若有重复数据，布隆过滤器去重

### 消息队列

- 为什么使用消息队列
  - 其实就是问问你消息队列都有哪些使用场景
  - 优点
    - 解耦
      - 如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费
    - 异步
      - 使用 MQ， A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，本地写库要 3ms,A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 = 8ms
      - BCD 三个系统分别写库要 300ms、450ms、200ms
    - 削峰
      - 如果使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。
      - 哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。
      - 只要高峰期一过，A 系统就会快速将积压的消息给解决掉
  - 缺点
    - 系统可用性降低
      - MQ 一挂，整套系统崩溃
    - 系统复杂度提高
      - 保证消息没有重复消费
      - 处理消息丢失的情况
      - 等等
    - 一致性问题
      - A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了
  - 特性
    - 单机吞吐量
      - 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景
    - topic 数量对吞吐量的影响
      - topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源
    - 时效性
      - 延迟在 ms 级以内
    - 可用性
      - 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用
    - 消息可靠性
      - 经过参数优化配置，可以做到 0 丢失
    - 功能支持
      - 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用
- 设计消息队列
  - 技术的基本原理、核心组成部分、基本架构构成
  - 支持可伸缩性
    - kafka
      - broker -> topic -> partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，给 topic 增加 partition，然后做数据迁移，增加机器
  - 落地磁盘
    - 顺序写
  - 可用性
    - kafka 的高可用保障机制
      - 多副本 -> leader & follower -> broker 挂了重新选举 leader 即可对外服务。
  - 数据 0 丢失
    - kafka 数据零丢失方案
- 如何保证消息不被重复消费？或者说，如何保证消息消费的幂等性？
  - 先大概说一说可能会有哪些重复消费的问题
    - 这问题通常不是 MQ 自己保证的，是由我们开发来保证的
    - consumer 有些消息处理了，但是没来得及提交 offset
    - Kafka 实际上有个offset的概念，就是每个消息写进去，都有一个 offset，代表消息的序号，然后 consumer 消费了数据之后，每隔一段时间（定时定期），会把自己消费过的消息的 offset 提交一下，表示“我已经消费过了，下次我要是重启啥的，你就让我继续从上次消费到的 offset 来继续消费吧”。
  - 怎么保证幂等性
    - 一条数据重复出现两次，数据库里就只有一条数据，这就保证了系统的幂等性
    - 几个思路
      - 根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。
      - 写 Redis，那没问题了，反正每次都是 set，天然幂等性
      - 生产者发送每条数据的时候，里面加一个全局唯一的 id，消费到了之后，先根据这个 id 去比如 Redis 里查一下
      - 基于数据库的唯一键来保证重复数据不会重复插入多条
- 如何保证消息的可靠性传输？或者说，如何处理消息丢失的问题？
  - 数据不能多一条，也不能少一条
    - 不能多
      - 重复消费和幂等性问题
    - 不能少
      - 这数据别搞丢了
  - 消费端弄丢了数据
    - 你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢了
    - Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢
      - 但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了
    - 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了
  - Kafka 弄丢了数据
    - Kafka 某个 broker 宕机，此时其他的 follower 刚好还有些数据没有同步
    - 此时一般是要求起码设置如下 4 个参数
      - replication.factor
        - 这个值必须大于 1，要求每个 partition 必须有至少 2 个副本
      - min.insync.replicas
        - 这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系
      - producer 端设置 acks=all
        - 要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了
      - producer 端设置 retries=MAX
        - 要求一旦写入失败，就无限重试
    - 这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失
  - 生产者会不会弄丢数据
    - 如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了
    - 如果没满足这个条件，生产者会自动不断的重试，重试无限次
- 如何保证消息的顺序性
  - 你在 mysql 里增删改一条数据，对应出来了增删改 3 条 binlog日志，接着这三条 binlog发送到 MQ 里面，再消费出来依次执行
  - 顺序会错乱的场景
    - 生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的
    - 多个线程来并发处理消息
      - 多个线程并发跑的话，顺序可能就乱掉了
  - 解决方案
    - 写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可
- 消息队列的高可用
  - 架构
    - Kafka由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据
    - 天然的分布式消息队列,一个 topic 的数据，是分散放在多个机器上的，每个机器就放一部分数据
  - HA 机制
    - replica（复制品） 副本机制
      - 每个 partition 的数据都会同步到其它机器上，形成自己的多个 replica 副本
      - 所有 replica 会选举一个 leader 出来，那么生产和消费都跟这个 leader 打交道，然后其他 replica 就是 follower
      - 写的时候，leader 会负责把数据同步到所有 follower 上去，读的时候就直接读 leader 上的数据即可
        - 写数据的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据
    - Kafka 会均匀地将一个 partition 的所有 replica 分布在不同的机器上，这样才可以提高容错性
- 如何解决消息队列的延时以及过期失效问题？消息队列满了以后该怎么处理？有几百万消息持续积压几小时，说说怎么解决？
  - 大量消息在 mq 里积压了几个小时了还没解决
    - 一般这个时候，只能临时紧急扩容了
      - 先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉
      - 新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量
      - 然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue
      - 接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据
      - 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。
  - mq 中的消息过期失效了
    - 批量重导
      - 将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来
  - mq 都快写满了
    - 临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案，到了晚上再补数据吧





# 系统的请求量突然增大数倍怎么办

一般的业务服务系统大体上就是通过**网络远程对DB进行读写**。如果流量突然飙大，总有一个资源会遇到瓶颈。按照经验大概出问题地方是DB磁盘io、CPU、带宽、连接数、内存其中的一个或几个。不同的业务，不同的系统设计，出问题的地方会有所不同。如果流量增大数倍，势必某个资源会在瞬间被榨干，然后所有的服务都会“开小差”，引起用户的抱怨。而解决问题的关键，是在问题发生时，尽量减少出问题的资源被访问。

对于这个问题，我这里给出两个回答，一个是应付面试的，一个面向实际的。大家各取所需。

## 面试中怎么回答

面试官其实就想听到几个术语的解释而已——**缓存、服务降级、限流**。

**缓存**，就是用内存来顶替一部分DB的查询+数据的处理。这应该是所有业务开发人员的必修课。业务上大致可以把缓存分为三类：浏览器缓存（HTTP Cache-Control Header)，[CDN](https://cloud.tencent.com/product/cdn?from=10680)和服务器业务缓存。而业务缓存根据实现的结构可以分多个层级，可以用in-memory cache (如Guava Cache），或者是分布式共享Cache（如[Redis](https://cloud.tencent.com/product/crs?from=10680)）。在设计缓存一致性更新模式时，无非就是*Cache Aside*、*Read/Write Through*和*Write Behind*这三大种模式。有些超级NB的缓存系统自带Cluster加持（比如Ehcache即可单机用，也可以组集群）。限于本文主题，具体的缓存设计不赘述。

留意下这里说的缓存仅仅是利用了内存访问比磁盘访问快得多的特性（大概可以理解为2～3个数量级），并不会让用户感知到数据一致性哪里不对劲（与下面的降级不同）。

**服务降级**，是指通过降低服务质量的方法，达到节省资源的目的。简单来说就是**弃车保帅**。比如你的服务有ABC，平时消耗差不多的资源。突发事件时，A的请求量极大的增高了，B和C没有变化。那么可以比如减少或者暂停B和C的服务，省出资源给A用。

再比如，一个热点新闻的业务，有新闻内容，有评论，有点赞数等等。一旦新闻热点了，就可以把所有这些内容“静态化”，不必再查DB。这时虽然评论，点赞的数暂时就不准了，但是主要的服务——内容，还是让用户可以看到。这就足够了。

可以看到，降级的具体的方案，要结合业务和系统实现来综合设计，并没有定法。

降级很多时候也会用到缓存。只不过这时候使用缓存的方法就可能会以牺牲数据一致性为代价——内存里的数据和DB不一样，就不一样吧，业务上可接受，并且这段热点时间段过去了，能够恢复为一致就可以。

**限流**，即限制用户的请求流量。具体的做法有*计数器*、*滑动窗口*、*滴漏*、*服务token*、*请求队列化*等办法。这些方法的详细解释，在[这里](https://www.jianshu.com/p/d9504fc0af4d)都说得比较清楚，所以我就不重复了。只是值得注意的是，现在很多生产级别的服务都是多节点分布式架构。很多单机上容易做的算法和控制逻辑到了分布式下就会带来一些实现上的麻烦。这又涉及到了分布式一致性、CAP的权衡等等问题。

怎么样，这些足够你在10分钟内和面试官白话一番了吧。下面我们说说真的。

## 真实世界的流量问题

**凡事总有第一次**

如果你的系统是第一次遇到流量突然增大的问题，而之前又没有任何准备的话，那么此问题**无解**。这一般出现在初创公司。你只能眼睁睁的和你的同事大眼瞪小眼，彼此唉声叹气，并祈祷这段时间赶紧过去，同时老板最好不要得到系统挂掉的消息（虽然几乎不可能）。



有人说，流量来了，我马上去买几台[云服务器](https://cloud.tencent.com/product/cvm?from=10680)，配合使用来自于NB公司的服务框架的配置调整，然后nginx reload一把，不就手到擒来了？问题在于，尽管对于无状态的服务可以比较简单的扩容，作为有状态的DB是无法随意扩容的——而DB在流量突然增高的情况下往往就是系统瓶颈。退一步说，就算一开始是无状态的应用服务器的CPU是瓶颈，然后马上扩容应用服务器，那么这个瓶颈也会最终压到DB上。DB的扩容方案需要另花时间长篇大论一番，这里就不再深入讲了。

另外，即便是在新机器上部署业务服务器，在没有准备的情况下十之八九也会出问题。why？对于一个干净服务器：

- 你得装环境镜像吧。万一着急OS版本号没看清，和其他业务服务器不一样，从而使得程序跑起来不一样怎么办？内核bug并不少见。万一现有的主机被调整了一些系统参数（比如tcp一类的），而新的环境忘了怎么办？
- 云服务器的实际环境你是看不见的。你新上了一批，怎么就敢拍胸脯说它们和之前的机器——那个文词怎么说来着——同构？
- 你得装最新的应用包吧。之前得装业务服务本身（比如一个war包），你能确保在混乱的情况下这里给的war就是能用的war？现在有了docker相对好一些，但是docker是可能有bug的，尤其是早期版本（我遇到过……），比如在一个地方起得来，换一个地方就起不来，或者连不上网。你敢打包票没问题？
- 有了应用得配置吧，不管是环境变量还是properties文件，外部依赖、DB链接、各种秘钥证书、各种只有开发才知道什么意思的配置可能多达几十个，**全都得弄对**。你敢说你能一次搞对？

以上这些事情，有任何一步出错，造成的问题都可能比流量本身更严重。我相信，如果这个流量问题是第一次，那么几乎可以肯定，项目组应该没准备过任何的服务治理的方案，更不要说演练。

那些N年前云计算吹嘘的弹性如何如何，能快速部署如何如何的slogan，在没有业务和系统设计的支持下，只是噱头罢了，不要太当真。

以上问题，如果你的公司可以轻易的做到，就说明**运维已经做的极度可靠和强大**。运维可以达到这种程度，是经过不懈的努力和大量的资源付出才能得到的。

所以，关键的问题是，**要对高流量的到来有所准备**。要提前设计方案，无论是降级限流还是别的什么歪招。要**设计、实现、测试、生产实机演习**，只有这样才能保证到了关键的时候才能用得起来。而这些事情将会花费技术团队大量的资源。

**总是预先准备**

当设计一个业务时，产品设计和研发团队应该找个时间，除了讨论产品本身怎么实现之外，还应该关心一下如下几点的实施：

**流量估算**。到底大概有多少人可能会用呢？对于大公司，都有长时间运营的经验，可以参照之前的产品/活动给出一个量化的估算结果。但是小公司往往就只能拍脑袋。但即便是拍脑袋也比没有强，可以作为设计容量的依据。这下你明白为啥有些公司面试时会出“你觉得本城市有多少个辆汽车”这样的题目了吧。

作为一个经验，可以把设计流量*3作为系统压力的下限——即，实现完了要压测，压测得到的结果要达到设计流量 * 3。当然如果是你的话，要 * 4， * 5都可以，关键是要给系统留些缓冲。一旦发生了什么，不至于挂的太惨。此时，一般会得到一个带缓存的业务服务系统。考虑到缓存高于后台服务2～3个数量级的性能优势，多撑几倍流量一般不成问题。

**降级方案**。降级总得是用户可以买账的方式才行，不能瞎降。能降级成什么样，显示成什么样子，都得预先设计好。UI上有的要配图，有的要出警告语提示。而作为后台服务器，需要有对应的实时开关，一旦设置，立刻进入降级方案。

但是，如果核心服务就是热点本身，就没得降级，那就尴尬了…… 比如下单就是下单，不能下一半，不能砍掉支付，不能随机性有的能买有的不能买。这时就得靠限流。

**限流方案**。上面提到了种种限流算法——*计数器*、*滑动窗口*、*滴漏*、*服务token*、*请求队列化*，等办法技术在更加传统的模版式页面的网站更容易做——整个界面是由一个GET请求在后台通过模版产生的。所以只要在这个请求处理过程中做限流控制即可。

但是在SPA或者移动端App时，一个界面可能是由数个或者数十个Ajax接口分批获得。每个请求都做限流可能会得到随机的半残的界面——有几个接口没限制，有几个被限制了。此时做限流还得考虑前后端架构设计。一般来讲，每个主要界面都应该有个主控接口来实现限流（比如产品详情接口）——即，一旦该接口说限流了，后续的前端代码就得配合按照预先的设计显示限流后的界面。同时会影响关键资源的接口在后端要再做一道限流——毕竟你不知道有没有人绕开前端直接压接口使坏不是。嗯，抢票就是这么来的。

**提前安排开发和演练排期**。如果一切安排妥当，就可以做作演习了。你可以找个没人用你服务的时间点（大半夜？）使用流量replay压一下你的真实生产环境，看看真的发生了流量增高的问题，系统是否足够健壮能够应对，之前设计的种种方案是不是可以达到设计的需要。根据**墨菲定律**，可能会发生的事情一定会发生，不经演练的系统上线到了出问题的时候100%会让你大开眼界。

然而，高能预警，做演练时：**千万千万不要产生像你的模拟流量代替用户下了一单并且扣款的事情……**。

**为啥技术方案难落地**

讲真，相关的工具开发和规范的制定其实并不难。大道理就那么几条。难的是**让高层意识到这个事情有多么的重要，如果不做的话会带来多么巨大的代价**。这是因为一个公司的高层可能是产品出身、营销出身、供应出身，可以是个彻底的技术外行。让他们理解技术的重要性，难度差不多就相当于让娘亲学会怎么把手机里的视频投屏到电视。此外，技术团队的主要目标往往是**实现公司业务需求**。在劈天盖地的业务需求的夹缝中找到资源来优化技术基础设施相当的苦难。毕竟公司**活下去**才是第一位的。

如果你成功的把你的头头劝服，并且为此申请了一大笔资金购买机器、招NB的人来做相关的基础设施，而又不耽误业务的话，你就是三楼楼长——呀不对，是CTO的好苗子。



**也许是个业务问题**

也许解决了技术问题还远远不够。比如电商这个业务，可能业务上会预计有多少流量，但是到底备多少货呢？多了费库存，少了用户骂。2017年双十一就很明显。淘宝上在双十一的前一周网站的各大热门产品都要“预约”。用户预约了能拿到更多一点折扣，而对于商家，也大致了解了要备货的数量。双赢。

对于我做的业务——财富管理业务，买入信号是个很关键的触发点。我们一旦侦测到了价格合适，就可以**分期分批**给用户发通知，提示他们机会难得，该买啦。这样用户得到了方便（提示就买，不用自己刷刷刷），而我们可以大致控制进入系统的用户数量不至于过多。双赢。



**相关问题**

这里稍微说一下高流量问题带来的一些相关的问题。这里仅仅是简单列举，具体内容之后找时间细细说。

-  **雪崩效应**——如果用户看到“服务开小差”，他的第一反应一定是再刷一次；如果是微服务架构，服务与服务之间可能会有自动重试机制。而这些，会让已经半死的系统死的更透彻。对此类问题，一般使用**断路器**的方案，简单来说就是，如果一个服务已经证明快挂了，就别再调用了，直接fallback。等一会再试。nginx里的upstream控制有`max_fails`和`fail_timeout`处理这个问题。Hystrix也实现了该机制。但断路了不等于让用户看到404页面骂娘，一定要结合业务+产品设计来实现断路方案。
-  **无效的服务响应**——在高压下，可以简单将等待处理的服务看作是在排队，队首的请求被处理。但被最终“见”到处理逻辑的请求从队尾排到队首时可能已经过了比较长的时间，而客户端那边可能早就超时了。所以业务服务处理了也是白处理。这时如果队列系统做得好，比如要处理前先猜一次是不是处理完了会超时，如果是就忽略扔掉。可以减少这种问题的发生几率。这也算是一种服务降级。
-  **大量的TIME_WAIT**——如果业务服务器的压力造成服务端大量主动关闭连接，就会产生大量的TIME_WAIT状态的TCP链接。这些链接会在数分钟内像僵尸一样堆在那里，榨干所有的连接数。这种问题尤其以自研业务服务框架容易出现。
-  **一致性**——为了服务降级，可能会把用户请求放内存里缓一缓，再批量进DB。那么一旦系统出现故障，就意味着比如下单数据不一致，支付状态不一致等问题。有时，这些问题在业务上极大的影响用户的使用体验。当系统降级时，尽量保证，要不就告诉用户现在不能给你服务，要服务了结果就明确。对于交易这种业务，事前打脸还是比事后扯皮要好一些。两害取其轻。
-  **系统可能会临时stop the world**——对于java这样的系统，会因为GC而暂时卡那么一下；对于mongoDB，可能因为要底层flush数据到磁盘，也会卡那么一下；平时写的什么正则表达式处理一类的逻辑，在高峰期也可能会卡那么一下…… 平时一般没事，但是赶上高峰时，这些问题一旦出现就有可能成为压垮骆驼的最后一根稻草。因此平时还是多多压测和演习，心里踏实。



# 双十一抢购性能瓶颈调优

首先，由于没有限流，超过预期的请求量导致了系统卡顿；其次，基于 Redis 实现的分布式锁分发抢购名额的功能抛出了大量异常；再次，就是我们误判了横向扩容服务可以起到的作用，其实第一波抢购的性能瓶颈是在数据库，横向扩容服务反而又增加了数据库的压力，起到了反作用；最后，就是在服务挂掉的情况下，丢失了异步处理的业务请求。

## 抢购业务流程

在进行具体的性能问题讨论之前，我们不妨先来了解下一个常规的抢购业务流程，这样方便我们更好地理解一个抢购系统的性能瓶颈以及调优过程。

- 用户登录后会进入到商品详情页面，此时商品购买处于倒计时状态，购买按钮处于置灰状态。
- 当购买倒计时间结束后，用户点击购买商品，此时用户需要排队等待获取购买资格，如果没有获取到购买资格，抢购活动结束，反之，则进入提交页面。
- 用户完善订单信息，点击提交订单，此时校验库存，并创建订单，进入锁定库存状态，之后，用户支付订单款。
- 当用户支付成功后，第三方支付平台将产生支付回调，系统通过回调更新订单状态，并扣除数据库的实际库存，通知用户购买成功。

## 抢购系统中的性能瓶颈

熟悉了一个常规的抢购业务流程之后，我们再来看看抢购中都有哪些业务会出现性能瓶颈。

1. **商品详情页面**

如果你有过抢购商品的经验，相信你遇到过这样一种情况，在抢购马上到来的时候，商品详情页面几乎是无法打开的。

这是因为大部分用户在抢购开始之前，会一直疯狂刷新抢购商品页面，尤其是倒计时一分钟内，查看商品详情页面的请求量会猛增。此时如果商品详情页面没有做好，就很容易成为整个抢购系统中的第一个性能瓶颈。

类似这种问题，我们通常的做法是提前将整个抢购商品页面生成为一个静态页面，并 push 到 CDN 节点，并且在浏览器端缓存该页面的静态资源文件，通过 CDN 和浏览器本地缓存这两种缓存静态页面的方式来实现商品详情页面的优化。

2. **抢购倒计时**

在商品详情页面中，存在一个抢购倒计时，这个倒计时是服务端时间的，初始化时间需要从服务端获取，并且在用户点击购买时，还需要服务端判断抢购时间是否已经到了。

如果商品详情每次刷新都去后端请求最新的时间，这无疑将会把整个后端服务拖垮。我们可以改成初始化时间从客户端获取，每隔一段时间主动去服务端刷新同步一次倒计时，这个时间段是随机时间，避免集中请求服务端。这种方式可以避免用户主动刷新服务端的同步时间接口。

**3. 获取购买资格**

可能你会好奇，在抢购中我们已经通过库存数量限制用户了，那为什么会出现一个获取购买资格的环节呢？

我们知道，进入订单详情页面后，需要填写相关的订单信息，例如收货地址、联系方式等，在这样一个过程中，很多用户可能还会犹豫，甚至放弃购买。如果把这个环节设定为一定能购买成功，那我们就只能让同等库存的用户进来，一旦用户放弃购买，这些商品可能无法再次被其他用户抢购，会大大降低商品的抢购销量。

增加购买资格的环节，选择让超过库存的用户量进来提交订单页面，这样就可以保证有足够提交订单的用户量，确保抢购活动中商品的销量最大化。

获取购买资格这步的并发量会非常大，还是基于分布式的，通常我们可以通过 Redis 分布式锁来控制购买资格的发放。

**4. 提交订单**

由于抢购入口的请求量会非常大，可能会占用大量带宽，为了不影响提交订单的请求，我建议将提交订单的子域名与抢购子域名区分开，分别绑定不同网络的服务器。

用户点击提交订单，需要先校验库存，库存足够时，用户先扣除缓存中的库存，再生成订单。如果校验库存和扣除库存都是基于数据库实现的，那么每次都去操作数据库，瞬时的并发量就会非常大，对数据库来说会存在一定的压力，从而会产生性能瓶颈。与获取购买资格一样，我们同样可以通过分布式锁来优化扣除消耗库存的设计。

由于我们已经缓存了库存，所以在提交订单时，库存的查询和冻结并不会给数据库带来性能瓶颈。但在这之后，还有一个订单的幂等校验，为了提高系统性能，我们同样可以使用分布式锁来优化。

而保存订单信息一般都是基于数据库表来实现的，在单表单库的情况下，碰到大量请求，特别是在瞬时高并发的情况下，磁盘 I/O、数据库请求连接数以及带宽等资源都可能会出现性能瓶颈。此时我们可以考虑对订单表进行分库分表，通常我们可以基于 userid 字段来进行 hash 取模，实现分库分表，从而提高系统的并发能力。

**5. 支付回调业务操作**

在用户支付订单完成之后，一般会有第三方支付平台回调我们的接口，更新订单状态。

除此之外，还可能存在扣减数据库库存的需求。如果我们的库存是基于缓存来实现查询和扣减，那提交订单时的扣除库存就只是扣除缓存中的库存，为了减少数据库的并发量，我们会在用户付款之后，在支付回调的时候去选择扣除数据库中的库存。

此外，还有订单购买成功的短信通知服务，一些商城还提供了累计积分的服务。

在支付回调之后，我们可以通过异步提交的方式，实现订单更新之外的其它业务处理，例如库存扣减、积分累计以及短信通知等。通常我们可以基于 MQ 实现业务的异步提交。

## 性能瓶颈调优

了解了各个业务流程中可能存在的性能瓶颈，我们再来讨论下商城基于常规优化设计之后，还可能出现的一些性能问题，我们又该如何做进一步调优。

**1. 限流实现优化**

限流是我们常用的兜底策略，无论是倒计时请求接口，还是抢购入口，系统都应该对它们设置最大并发访问数量，防止超出预期的请求集中进入系统，导致系统异常。

通常我们是在网关层实现高并发请求接口的限流，如果我们使用了 Nginx 做反向代理的话，就可以在 Nginx 配置限流算法。Nginx 是基于漏桶算法实现的限流，这样做的好处是能够保证请求的实时处理速度。

Nginx 中包含了两个限流模块：[ngx_http_limit_conn_module](http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html) 和 [ngx_http_limit_req_module](http://nginx.org/en/docs/http/ngx_http_limit_req_module.html)，前者是用于限制单个 IP 单位时间内的请求数量，后者是用来限制单位时间内所有 IP 的请求数量。以下分别是两个限流的配置：

```
limit_conn_zone $binary_remote_addr zone=addr:10m;
 
server {
    location / {
        limit_conn addr 1;
    }
http {
    limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
    server {
        location / {
            limit_req zone=one burst=5 nodelay;
        }
} 
```

在网关层，我们还可以通过 lua 编写 OpenResty 来实现一套限流功能，也可以通过现成的 Kong 安装插件来实现。除了网关层的限流之外，我们还可以基于服务层实现接口的限流，通过 Zuul RateLimit 或 Guava RateLimiter 实现。

**2. 流量削峰**

瞬间有大量请求进入到系统后台服务之后，首先是要通过 Redis 分布式锁获取购买资格，这个时候我们看到了大量的“JedisConnectionException Could not get connection from pool”异常。

这个异常是一个 Redis 连接异常，由于我们当时的 Redis 集群是基于哨兵模式部署的，哨兵模式部署的 Redis 也是一种主从模式，我们在写 Redis 的时候都是基于主库来实现的，在高并发操作一个 Redis 实例就很容易出现性能瓶颈。

你可能会想到使用集群分片的方式来实现，但对于分布式锁来说，集群分片的实现只会增加性能消耗，这是因为我们需要基于 Redission 的红锁算法实现，需要对集群的每个实例进行加锁。

后来我们使用 Redission 插件替换 Jedis 插件，由于 Jedis 的读写 I/O 操作还是阻塞式的，方法调用都是基于同步实现，而 Redission 底层是基于 Netty 框架实现的，读写 I/O 是非阻塞 I/O 操作，且方法调用是基于异步实现。

但在瞬时并发非常大的情况下，依然会出现类似问题，此时，我们可以考虑在分布式锁前面新增一个等待队列，减缓抢购出现的集中式请求，相当于一个流量削峰。当请求的 key 值放入到队列中，请求线程进入阻塞状态，当线程从队列中获取到请求线程的 key 值时，就会唤醒请求线程获取购买资格。

**3. 数据丢失问题**

无论是服务宕机，还是异步发送给 MQ，都存在请求数据丢失的可能。例如，当第三方支付回调系统时，写入订单成功了，此时通过异步来扣减库存和累计积分，如果应用服务刚好挂掉了，MQ 还没有存储到该消息，那即使我们重启服务，这条请求数据也将无法还原。

重试机制是还原丢失消息的一种解决方案。在以上的回调案例中，我们可以在写入订单时，同时在数据库写入一条异步消息状态，之后再返回第三方支付操作成功结果。在异步业务处理请求成功之后，更新该数据库表中的异步消息状态。

假设我们重启服务，那么系统就会在重启时去数据库中查询是否有未更新的异步消息，如果有，则重新生成 MQ 业务处理消息，供各个业务方消费处理丢失的请求数据。

**总结**

减少抢购中操作数据库的次数，缩短抢购流程，是抢购系统设计和优化的核心点。

抢购系统的性能瓶颈主要是在数据库，即使我们对服务进行了横向扩容，当流量瞬间进来，数据库依然无法同时响应处理这么多的请求操作。我们可以对抢购业务表进行分库分表，通过提高数据库的处理能力，来提升系统的并发处理能力。

除此之外，我们还可以分散瞬时的高并发请求，流量削峰是最常用的方式，用一个队列，让请求排队等待，然后有序且有限地进入到后端服务，最终进行数据库操作。当我们的队列满了之后，可以将溢出的请求放弃，这就是限流了。通过限流和削峰，可以有效地保证系统不宕机，确保系统的稳定性。



# 高并发对服务器的需求有哪些？

1）从客户端看
尽量减少请求数量，比如：依靠客户端自身的缓存或处理能力。
尽量减少对服务端资源的不必要耗费，比如：重复使用某些资源，如连接池客户端处理的基本原则就是：能不访问服务端就不要访问。
2）从服务端看
增加资源供给，比如：更大的网络带宽，使用更高配置的服务器，使用高性能的Web服务器，使用高性能的数据库。
请求分流，比如：使用集群,分布式的系统架构。
应用优化，比如：使用更高效的编程语言,优化处理业务逻辑的算法,优化访问数据库的SQL。
基本原则：分而治之，并提高单个请求的处理速度。



一个系统的并发能力是多少呢？怎么衡量？

衡量指标常用的有响应时间，吞吐量，每秒查询率QPS，并发用户数

响应时间：系统对请求做出响应的时间。你简单理解为一个http请求返回所用的时间

吞吐量：单位时间内处理的请求数量。

QPS：每秒可以处理的请求数

并发用户数：同时承载正常使用系统功能的用户数量。也就是多少个人同时使用这个系统，这个系统还能正常运行。这个用户数量就是并发用户数啦。

# 高并发服务器的设计--架构与瓶颈的设计



不同人的处理方法不同，据我经验，可以将瓶颈子分成两类：

1.阻塞串行处理

2.异步并行处理

mysql,中间件的处理属于第一类，异步网关查询属于第二类。

对于第一类，一种通用的解决方法是增加处理进程，其实是横向扩容的思想，打个比方，一个进程的并发是600，10个进程就可以达到6000了，如何才能将请求均匀地分配到这10个进程是关键。

多个进程同时监听一个端口，[负载均衡](https://cloud.tencent.com/product/clb?from=10680)的方法很多，这里介绍nginx的做法

nginx用一个全局变量ngx_accept_disabled 来控制单个进程的负载，当负载达到一定值的时候，不再接受新的负载。

对于第二类情况，解决的方法就像名字一样，异步并行解决。

拿跨网查询为例：

创建一个查询的请求，将请求放进事件模型中，等待服务端的返回，异步处理。

熟悉nginx的就知道nginx的upstream反向代理，这个解决方案跟反向代理很像，只不过在与上游服务器交互的前后分别还有其他的业务处理，而且可能还会有多次交互。

相应的流水图是这样的：

![img](https://img-blog.csdn.net/20130527204452023)

当客户端请求量大时，事件模型的容量会成为瓶颈，这样仍然需要横向扩容的方式来解决，增加处理进程。

这两种情况的处理方法大致如此，有时候特殊问题特殊对待，比哪数据库的瓶颈可以借助缓存解决，有些高配服务器的内存128G，甚至几台高配服务器只为一个业务，这样的情况下，不吃点内存难免对不起老板的money.





# 1秒1000并发 高并发需要什么样的服务器

- 我这边一分钟产生40万条数据，大概是400MB，期间要有其它程序处理这些数据。最初采用了Redis和MySQL，因为有读有写，发现写库根本来不及。最后采用的方式是：先缓存数据在内存，将每10万条数据进行序列化，写文件（7200转的硬盘，每秒写100MB），另外一程序解析文件，处理数据（处理完数据没那么多了），之后存库。

  - 所有数据先存队列里（比如beanstalkd），然后异步写入数据库
  - 网络环境是局域网

- 不需要有特殊服务器，一般云上主流主机都可以，主要还是软件架构要符合业务场景。

  - 阿里云10台2核4g

- 宽带肯定是要万兆的，硬件这块其实还好，现在可以用很廉价的pc来做分布式的架构，至于内存和硬盘的大小主要是根据数据量的大小和存储多少来决定的。

- 先普及一下基础知识：

  一、硬件条件。确认服务器硬件是否足够支持当前的流量，一台普通的P4服务器一般最多能支持每天10万独立IP，如果访问量比这个还要大， 那么必须首先配置一台更高性能的专用服务器才能解决问题 ，另外就是增加服务器数量，否则怎么优化都不可能彻底解决性能问题。

  

  二、数据库。优化数据库访问前台实现完全的静态化当然最好，可以完全不用访问数据库，不过对于频繁更新的网站， 静态化往往不能满足某些功能。缓存技术就是另一个解决方案，就是将动态数据存储到缓存文件中，动态网页直接调用 这些文件，而不必再访问数据库，WordPress和Z-Blog都大量使用这种缓存技术。如果确实无法避免对数据库的访问，那么可以尝试优化数据库的查询SQL，避免使用 Select * from这样的语句，每次查询只返回自己需要的结果，避免短时间内的大,尽量做到"所查即所得" ,遵循以小表为主,附表为辅,查询条件先索引,先小后大的原则,提高查询效率.量SQL查询。

  三、禁止盗链。外部网站的图片或者文件盗链往往会带来大量的负载压力，因此应该严格限制外部对于自身的图片或者文件盗链，好在目前可以简单地通过refer来控制盗链，Apache自 己就可以通过配置来禁止盗链，IIS也有一些第三方的ISAPI可以实现同样的功能。当然，伪造refer也可以通过代码来实现盗链，不过目前蓄意伪造refer盗链的还不多， 可以先不去考虑，或者使用非技术手段来解决，比如在图片上增加水印。

  

  四、控制大文件的下载。大文件的下载会占用很大的流量，并且对于非SCSI硬盘来说，大量文件下载会消耗 CPU，使得网站响应能力下降。因此，尽量不要提供超过2M的大文件下载，如果需要提供，建议将大文件放在另外一台服务器上。

  

  五、镜像分流。将文件放在不同的主机上，提供不同的镜像供用户下载。比如如果觉得RSS文件占用流量大，那么使用FeedBurner或者FeedSky等服务将RSS输出放在其他主机上，这样别人访问的流量压力就大多集中在FeedBurner的主机上，RSS就不占用太多资源了。

  

  六、做好流量监控。在网站上安装一个流量分析统计软件，可以即时知道哪些地方耗费了大量流量，哪些页面需要再进行优化，因此，解决流量问题还需要进行精确的统计分析才可以。推荐使用的流量分析统计软件是Google Analytics（Google分析）。

  

  接下来我们来说说服务器架构，前段时间我们请到了国内服务器顶级攻城狮，他把服务器那点事讲得非常通透简单。对于一个刚起步的创业公司，不需要考虑太多复杂的服务器架构，能把业务跑起来就行了。但是在早期业务逻辑设计时，懂一些稍微复杂的服务器架构的逻辑，后面可以少走很多弯路。

  

  下面这个图估计大家都明白，这就是最基础的服务器架构。傻瓜式的方法是把应用服务器、文件服务器、数据库服务器全部混合在一起，呵呵呵！但这并不是最科学的。

  ![img](https://pic3.zhimg.com/80/v2-8e496f310fc643f9c12934395fa7c445_1440w.png)

  

  

  当业务量持续增加到一定量以后，执行应用程序、读写文件、访问数据库应该有所区分，保证各自的需求都能得到满足，这时候你需要考虑把应用服务器、文件服务器、数据库服务器分离，这个时候的服务器架构应该是下面这样的，它是由三个独立的服务器组成，各司其职。

  ![img](https://pic1.zhimg.com/80/v2-3c7d638f12854bfff9defe7448a2648a_1440w.png)

  

  

  随着业务量持续增加，应用程序访问缓存数据会成为瓶颈，这个时候需要增加本地缓存，有的也需要分布式缓存。分布式缓存是指缓存部署在多个服务器组成的服务器集群中，以集群的方式提供缓存服务，其架构方式主要有两种，一种是以JBoss Cache为代表的需要同步更新的分布式缓存，一种是以Memchached为代表的互不通信的分布式缓存。如下图：

  ![img](https://pic1.zhimg.com/80/v2-2fe94a063184df2ba0ef279145e728c6_1440w.png)

  

  

  接下来，需要更多台应用服务器以应对复杂的业务逻辑，同时需要负载均衡调度服务器来调度和分配应用服务器的工作任务。

  ![img](https://pic1.zhimg.com/80/v2-073ccab95e5175268865591de2353804_1440w.png)

  

  

  再往后，需要考虑数据库服务器的承压能力，通常可以采用主从式数据库服务器架构，把读、写两部分分开，既可以提高数据访问的安全性，也能提高数据读写的效率。

  ![img](https://pic4.zhimg.com/80/v2-44bee1d539b9b6e8ce96a15a231c7c5c_1440w.png)

  

  

  随着业务量暴增，单一区域的服务器带宽将不能承载全国的业务需求，这时候需要增加反向代理和CDN服务器。CDN系统能够实时地根据网络流量和各节点的连接、负载状况以及到用户的距离和响应时间等综合信息将用户的请求重新导向离用户最近的服务节点上。其目的是使用户可就近取得所需内容，解决 Internet网络拥挤的状况，提高用户访问网站的响应速度。

  ![img](https://picb.zhimg.com/80/v2-cfbd61431a0b08b140184deb0c77d063_1440w.png)

  

  

  同样，服务器架构师应该分析文件服务器和数据库服务器的网络读写速度，进一步部署分布式文件和分布式数据库的架构。

  ![img](https://pic3.zhimg.com/80/v2-019478359e8a916b405b724000922e4f_1440w.png)

  

  

  对于有搜索和大量查询的网络业务，还需要增加独立的搜索引擎和NoSQL服务器。

  ![img](https://picb.zhimg.com/80/v2-967cc701131edeffd524857d84660d0a_1440w.png)

  

  

  对于更复杂的系统，还需要进一步拆分应用服务器，增加消息队列服务器。增加消息队列服务器有以下几点好处：

  1，由于消息队列服务器的速度远远高于数据库服务器，所以能够快递处理并返回数据；

  2，消息队列服务器具有更好的扩展性；

  3，在高并发的情况下，延迟写入数据库，可以有效降低数据库的压力。

  ![img](https://picb.zhimg.com/80/v2-1ebdedfc6cb79c4cbeab8203f5926a15_1440w.png)

  

  

  对于一些超大型综合互联网业务，应用服务器也需要分布式的架构，这个时候在不同业务的应用服务器之间做好消息协同会有较大的挑战。

  

  

  ![img](https://pic2.zhimg.com/80/v2-ea826176a298de845a07acb7318f35e9_1440w.png)

  

  读完后，是不是感觉很简单，**基本上就是围绕着应用服务器、文件服务器、数据库服务器，以及不断提升其性能需要增加的服务器**。好在如今腾讯云、阿里云、金山云都提供了完整的解决方案

# 设计一个高并发、高可用秒杀系统



## **秒杀系统的难点**

- 友好的用户体验

  - 用户不能接受破窗的体验，例如：系统超时、系统错误的提示，或者直接 404 页面

- 瞬时高并发流量的挑战

  - 木桶短板理论，整个系统的瓶颈往往都在 DB，如何设计出高并发、高可用系统？

## **如何设计**

![img](https://pic1.zhimg.com/v2-e3a128f561bb4739c156ed6e392cb426_b.jpg)

上图是一个典型的互联网业务，用户完成一个写操作，一般会通过接入层和逻辑层，这里的服务都是无状态，可以通过平行拓展去解决高并发的问题；到了 db 层，必须要落到介质中，可以是磁盘/ssd/内存，如果出现 key 的冲突，会有一些并发控制技术，例如 cas/加锁/串行排队等。

**直筒型**

直筒型业务，指的是用户请求 1:1 的洞穿到 db 层，如下图所示。在比较简单的业务中，才会采用这个模型。随着业务规模复杂度上来，一定会有 db 和逻辑层分离、逻辑层和接入层分离。

![img](https://pic2.zhimg.com/v2-15f4981679385991797c2c435160d2dd_b.jpg)

**漏斗型**

漏斗型业务，指的是，用户的请求，从客户端到 db 层，层层递减，递减的程度视业务而定。例如当 10w 人去抢 1 个物品时，db 层的请求在个位数量级，这就是比较理想的模型。如下图所示

![img](https://picb.zhimg.com/v2-243232ef478e347dd4d594af21a03d8a_b.jpg)

这个模型，是高并发的基础，翻译一下就是下面这些：

- 及早发现，及早拒绝
- Fast Fail
- 前端保护后端
  

**如何实现漏斗型系统**

漏斗型系统需要从产品策略/客户端/接入层/逻辑层/DB 层全方位立体的设计。

![img](https://pic4.zhimg.com/v2-e0a777b9cacabc2e32df83f4e7d6d829_b.jpg)

**产品策略**

- 轻重逻辑分离，以秒杀为例，将抢到和到账分开；

- - 抢到，是比较轻的操作，库存扣成功后，就可以成功了
  - 到账，是比较重的操作，需要涉及到到事务操作
    

- 用户分流，以整点秒杀活动为例，在 1 分钟内，陆续对用户放开入口，将所有用户请求打散在 60s 内，请求就可以降一个数量级
  
- 页面简化，在秒杀开始的时候，需要简化页面展示，该时刻只保留和秒杀相关的功能。例如，秒杀开始的时候，页面可以不展示推荐的商品。
  

**客户端**

- 重试策略非常关键，如果用户秒杀失败了，频繁重试，会加剧后端的雪崩。如何重试呢？根据后端返回码的约定，有两种方法：
  
- - 不允许重试错误，此时 ui 和文案都需要有一个提示。同时不允许重试
  - 可重试错误，需要策略重试，例如二进制退避法。同时文案和 ui 需要提示。
    

- ui 和文案，秒杀开始前后，用户的所有异常都需要有精心设计的 ui 和文案提示。例如：【当前活动太火爆，请稍后再重试】【你的货物堵在路上，请稍后查看】等
  
- 前端随机丢弃请求可以作为降级方案，当用户流量远远大于系统容量时，人工下发随机丢弃标记，用户本地客户端开始随机丢弃请求。
  

**接入层**

- 所有请求需要鉴权，校验合法身份
  
- - 如果是长链接的服务，鉴权粒度可以在 session 级别；如果是短链接业务，需要应对这种高并发流量，例如 cache 等

- 根据后端系统容量，需要一个全局的限流功能，通常有两种做法：
  
- - 设置好 N 后，动态获取机器部署情况 M，然后下发单机限流值 N/M。要求请求均匀访问，部署机器统一。
  - 维护全局 key，以时间戳建 key。有热 key 问题，可以通过增加更细粒度的 key 或者定时更新 key 的方法。
    

- 对于单用户/单 ip 需要频控，主要是防黑产和恶意用户。如果秒杀是有条件的，例如需要完成 xxx 任务，解锁资格，对于获得资格的步骤，可以进行安全扫描，识别出黑产和恶意用户。
  

**逻辑层**

- 逻辑层首先应该进入校验逻辑，例如参数的合法性，是否有资格，如果失败的用户，快速返回，避免请求洞穿到 db。
  
- 异步补单，对于已经扣除秒杀资格的用户，如果发货失败后，通常的两种做法是：
  
- - 事务回滚，回滚本次行为，提示用户重试。这个代价特别大，而且用户重试和前面的重试策略结合的话，用户体验也不大流畅。
  - 异步重做，记录本次用户的 log，提示用户【稍后查看，正在发货中】，后台在峰值过后，启动异步补单。需要服务支持幂等
    

- 对于发货的库存，需要处理热 key。通常的做法是，维护多个 key，每个用户固定去某个查询库存。对于大量人抢红包的场景，可以提前分配。
  

**存储层**

对于业务模型而言，对于 db 的要求需要保证几个原则：

- 可靠性
  
- - 主备：主备能互相切换，一般要求在同城跨机房
    
  - 异地容灾：当一地异常，数据能恢复，异地能选主
    
  - 数据需要持久化到磁盘，或者更冷的设备
    

- 一致性
  
- - 对于秒杀而言，需要严格的一致性，一般要求主备严格的一致。

## **实践——微视集卡瓜分系统**

微视集卡瓜分项目属于微视春节项目之一。用户的体验流程如下：

![img](https://pic2.zhimg.com/v2-9bde162bee150dc33d61df71ffb71724_b.jpg)

**架构图**

![img](https://pic2.zhimg.com/v2-60b7f118b41855263cd1e4d434285b4d_b.jpg)

- 客户端主要是微视主 app 和 h5 页面，主 app 是入口，h5 页面是集卡活动页面和瓜分页面。
  
- 逻辑部分为分：发卡来源、集卡模块、奖品模块，发卡来源主要是任务模块；集卡模块主要由活动模块和集卡模块组成。瓜分部分主要在活动控制层。
  
- 奖品模块主要是发钱和其他奖品。
  

**瓜分降级预案**

为了做好瓜分时刻的高并发，对整个系统需要保证两个重要的事情：

- 全链路梳理，包括调用链的合理性和时延设置
  
- 降级服务预案分析，提升系统的鲁棒性
  

如下图所示，是针对瓜分全链路调用分析如下图，需要特别说明的几点：

![img](https://picb.zhimg.com/v2-2357aac83e60101d1b143f634725b5b2_b.jpg)

- 时延很重要，需要全链路分析。不但可以提高吞吐量，而且可以快速暴露系统的瓶颈。
  
- 峰值时刻，补单逻辑需要关闭，避免加剧雪崩。
  

我们的降级预案大概如下：

- 一级预案，瓜分时刻前后 5 分钟自动进入：
  
- - 入口处 1 分钟内陆续放开入口倒计时，未登录用户不弹入口
    
  - 主会场排队，以进入主会场 100wqps 为例，超过了进入排队，由接入层频控控制
    
  - 拉取资格接口排队，拉取资格接口 100wqps，超过了进入排队，由接入层频控控制
    
  - 抢红包排队，抢红包 100wqps，超过了进入排队，由接入层频控控制
    
  - 红包到账排队，如果资格扣除成功，现金发放失败，进入排队，24 小时内到账。异步补单
    
  - 入口处调用后端非关键 rpc:ParticipateStatus，手动关闭
    
  - 异步补单逻辑关闭。
    

- 二级预案，后端随机丢请求，接入层频控失效或者下游服务过载，手动开启进入
  
- 三级预案，前端随机丢请求，后端服务过载或者宕机进入。手动开启
  

综上，整个瓜分时刻体验如下所示：

![img](https://pic4.zhimg.com/v2-4551c7d39e776fe0d494c4cc6005fa17_b.jpg)

回顾下漏斗模型，总结下整个实践：

![img](https://picb.zhimg.com/v2-db80d36c4a81173ec75db7fe88b25a75_b.jpg)





降级开关设计，过载感知，分流策略设计



## 游戏服务器

游戏服务器特征：

长期运行，要求有一定的稳定性和性能。如果需要动态扩容来提高承载能力还要考虑到维护部署的方便性。如果是手游服务器还要考虑到弱联网，保证通信交互的顺畅。

对于服务端需求主要有以下几点：

1：玩家交互数据的广播，同步

2：玩家数据存储

3：做好验证，防止外挂

4：交互的流畅性

为了满足以上需求，我们得考虑服务器的内存，cpu，带宽等因素，来制定最优的服务器开发和部署方案。



**第三代网游服务器**

![img](https://pic1.zhimg.com/80/v2-5c80ce49a944b07ec87c22c9dfbde44a_1440w.jpg)



网关部分分离成单端的gate服务器，DB部分分离为DB服务器，把网络功能单独提取出来，让用户统一去连接一个网关服务器，再有网关服务器转发数据到后端游戏服务器。而游戏服务器之间数据交换也统一连接到网管进行交换。所有有DB交互的，都连接到DB服务器来代理处理。



上面版本的进化版

![img](https://picb.zhimg.com/80/v2-cf4bcbf82399e1d4abaea41d79c1d02d_1440w.jpg)

每个相同的模块分布到一台服务器处理，多组服务器集群共同组成一个游戏服务端。一般地，我们可以将一个组内的服务器简单地分成两类：场景相关的(如：行走、战斗等)以及场景不相关的(如：公会聊天、不受区域限制的贸易等)。经常可以见到的一种方案是：gate服务器、场景服务器、非场景服务器、聊天管理器、AI服务器以及数据库代理服务器。



#### 成熟形态的服务器框架

逻辑服务器的负载均摊方法一：按照功能划分多个服务器进程



![img](https://pic2.zhimg.com/80/v2-83d176b88c6ffcf64211a8d34ba9d842_1440w.png)





逻辑服务器的负载均摊方法二：按照场景划分多个服务器进程



![img](https://pic3.zhimg.com/80/v2-3c87d414d04a2ab57c24f6bb78af5d45_1440w.png)



对游戏服务器历史有了基本了解后，成熟形态的游戏服务器很容易理解。简单来说，就是把逻辑服务器单个进程的压力分摊到多个服务器。

难点在逻辑的设计上，要像做手术一样把本来是一体的功能切开，并抽象出若干个API来保持联系（服务器之间是TCP连接）。

在分解时，**要找联系相对最薄弱的环节入手**，比如场景和场景之间分开、单独抽出聊天服务、组队服务、好友服务。

无论如何分解，最终结果只能是有限个服务。而且分解的越细，开发难度就越大。因为跨服务器逻辑是把简单的同步逻辑变成了异步Callback逻辑，而且容易出现时序问题等不易测试的问题。

单个场景服务几乎是无法分解的。分解单个场景难度巨大以至于出现了BigWorld引擎来专门的解决场景分割问题，后面会谈到。



这种成熟形态的游戏服务器已经能满足现实中99%的频繁交互类网游需求，是大型MMO端游、页游的主流形式。

当然有实力的公司在这个基础上会做很多改动，实现动态开辟副本、相位技术等等，但是万变不离其宗，其本质和上图没有什么区别。



#### 附：开房间式的网络游戏

开房间式的网络游戏也是游戏的一个重要分支，英雄联盟、DOTA、很多手游例如皇室战争、王者荣耀等等。

这种游戏房间之间几乎没有交互，只有大厅内有交互，可以理解为原始形态的游戏服务器的平行扩展。

房间式游戏扩展难度较小，只是需要根据玩家数量动态扩展游戏房间的数量、服务器数量。很像网站的架构。



![img](https://pic2.zhimg.com/80/v2-e465c12055355c1170130c7c9602a9d2_1440w.png)





这种游戏架构最最适合放在云平台上，设计合理的话，它可能遇到的问题和大型网站几乎一模一样。不需要特别的讨论它们。

只是，**毕竟游戏不都是开房间的玩法**。



#### 小结：游戏服务器框架特点

1、真正的数据都在内存中，数据库性能不那么重要

· 注：很多大型游戏采用了共享内存，避免宕机时损失过大。

2、单CPU性能比CPU数量重要的多。

3、目前有很多游戏，特别是手游，使用Redis读写代替内存读写，甚至也有用Mongo的。

4、开新服、旧区合服的情况，非常适合云平台。