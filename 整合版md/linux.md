## 平均负载

- 每次发现系统变慢时，我们通常做的第一件事，就是执行 top 或者 uptime 命令

  - 过去 1 分钟、5 分钟、15 分钟的平均负载（LoadAverage）
  - 简单来说，平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。
    - 所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用ps 命令看到的，处于 R 状态（Running 或Runnable）的进程。
    - 不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。
      - 不可中断状态实际上是系统对进程和硬件设备的一种保护机制

- 平均负载为多少时合理

  - 当平均负载比 CPU 个数还大的时候，系统已经出现了过载。
  - 假设我们在一个单 CPU 系统上看到平均负载为 1.73，0.60，7.98，那么说明在过去 1 分钟内，系统有 73% 的超载，而在 15 分钟内，有 698% 的超载，从整体趋势来看，系统的负载在降低。
  - 在实际生产环境中，当平均负载高于 CPU 数量 70% 的时候需要我们重点关注

- 平均负载与 CPU 使用率

  - CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应
  - 平均负载它不仅包括了正在使用 CPU 的进程，还包括等待 CPU 和等待I/O 的进程。
    - CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的；
    - I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高；

- 工具

  - stress 是一个 Linux 系统压力测试工具，这里我们用作异常进程模拟平均负载升高的场景。

    - > 模拟一个 CPU 使用率 100% 的场景    stress --cpu 1 --timeout 600
      >
      > 模拟 I/O 压力，即不停地执行 sync     stress -i 1 --timeout 600
      >
      > 模拟8 个进程                                    stress -c 8 --timeout 600

  - mpstat 是一个常用的多核 CPU 性能分析工具，用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标

  - pidstat 是一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。

    - > -w 参数表示输出进程切换指标，
      >
      > -u 参数则表示输出 CPU 使用指标
      >
      > -d 所有进程的 I/O 使用情况
      >
      > -t 参数表示输出线程的指标。
      >
      > -p 指定进程号



## 用户态和内核态

- 用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。

- 内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。

- 为什么要有用户态和内核态？

  - 由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级 -- 用户态和内核态。

- 用户态与内核态的切换

  所有用户程序都是运行在用户态的, 但是有时候程序确实需要做一些内核态的事情, 例如从硬盘读取数据, 或者从键盘获取输入等. 而唯一可以做这些事情的就是操作系统, 所以此时程序就需要先操作系统请求以程序的名义来执行这些操作.

  这时需要一个这样的机制: 用户态程序切换到内核态, 但是不能控制在内核态中执行的指令

  这种机制叫系统调用, 在CPU中的实现称之为**陷阱指令**(Trap Instruction)

  他们的**工作流程**如下:

  1. 用户态程序将一些数据值放在寄存器中, 或者使用参数创建一个堆栈(stack frame), 以此表明需要操作系统提供的服务.
  2. 用户态程序执行陷阱指令
  3. CPU切换到内核态, 并跳到位于内存指定位置的指令, 这些指令是操作系统的一部分, 他们具有内存保护, 不可被用户态程序访问
  4. 这些指令称之为陷阱(trap)或者系统调用处理器(system call handler). 他们会读取程序放入内存的数据参数, 并执行程序请求的服务
  5. 系统调用完成后, 操作系统会重置CPU为用户态并返回系统调用的结果

  当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态（或简称为内核态）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。即此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。这与处于内核态的进程的状态有些类似。 

  内核态与用户态是操作系统的两种运行级别,跟intel cpu没有必然的联系, intel cpu提供**Ring0-Ring3三种级别的运行模式**，Ring0级别最高，Ring3最低。Linux使用了Ring3级别运行用户态，Ring0作为 内核态，没有使用Ring1和Ring2。Ring3状态不能访问Ring0的地址空间，包括代码和数据。Linux进程的4GB地址空间，3G-4G部 分大家是共享的，是内核态的地址空间，这里存放在整个内核的代码和所有的内核模块，以及内核所维护的数据。用户运行一个程序，该程序所创建的进程开始是运 行在用户态的，如果要执行文件操作，网络数据发送等操作，必须通过write，send等系统调用，这些系统调用会调用内核中的代码来完成操作，这时，必 须切换到Ring0，然后进入3GB-4GB中的内核地址空间去执行这些代码完成操作，完成后，切换回Ring3，回到用户态。这样，用户态的程序就不能 随意操作内核地址空间，具有一定的安全保护作用。
  至于说保护模式，是说通过内存页表操作等机制，保证进程间的地址空间不会互相冲突，一个进程的操作不会修改另一个进程的地址空间中的数据。

   

   

  1. **用户态和内核态的概念区别**

  究竟什么是用户态，什么是内核态，这两个基本概念以前一直理解得不是很清楚，根本原因个人觉得是在于因为大部分时候我们在写程序时关注的重点和着眼的角度放在了实现的功能和**代码**的逻辑性上，先看一个例子：

  ```c++
  void testfork(){
  	if(0 = = fork()){
  		printf(“create new process success!\n”);
  	}
  	printf(“testfork ok\n”);
  }
  
  ```

  这段代码很简单，从功能的角度来看，就是实际执行了一个fork()，生成一个新的进程，从逻辑的角度看，就是判断了如果fork()返回的是0则打印相关语句，然后函数最后再打印一句表示执行完整个testfork()函数。代码的执行逻辑和功能上看就是如此简单，一共四行代码，从上到下一句一句执行而已，完全看不出来哪里有体现出用户态和进程态的概念。

  如果说前面两种是静态观察的角度看的话，我们还可以从动态的角度来看这段代码，即它被转换成CPU执行的指令后加载执行的过程，这时这段程序就是一个动态执行的指令序列。而究竟加载了哪些代码，如何加载就是和操作系统密切相关了。

   

  2）**特权级**

  熟悉Unix/Linux系统的人都知道，**fork的工作实际上是以系统调用的方式完成相应功能的**，具体的工作是由sys_fork负责实施。其实无论是不是Unix或者Linux，对于任何操作系统来说，创建一个新的进程都是属于核心功能，因为它要做很多底层细致地工作，消耗系统的物理资源，比如分配物理内存，从父进程拷贝相关信息，拷贝设置页目录页表等等，这些显然不能随便让哪个程序就能去做，于是就自然引出特权级别的概念，显然，最关键性的权力必须由高特权级的程序来执行，这样才可以做到集中管理，减少有限资源的访问和使用冲突。

  **特权级显然是非常有效的管理和控制程序执行的手段**，因此在硬件上对特权级做了很多支持，就Intel x86架构的CPU来说一共有0~3四个特权级，0级最高，3级最低，硬件上在执行每条指令时都会对指令所具有的特权级做相应的检查，相关的概念有CPL、DPL和RPL，这里不再过多阐述。硬件已经提供了一套特权级使用的相关机制，软件自然就是好好利用的问题，这属于操作系统要做的事情，对于Unix/Linux来说，只使用了0级特权级和3级特权级。也就是说在Unix/Linux系统中，一条工作在0级特权级的指令具有了CPU能提供的最高权力，而一条工作在3级特权级的指令具有CPU提供的最低或者说最基本权力。

   

  3）用户态和内核态

  **现在我们从特权级的调度来理解用户态和内核态就比较好理解了**，当程序运行在3级特权级上时，就可以称之为运行在用户态，因为这是最低特权级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态；反之，当程序运行在0级特权级上时，就可以称之为运行在内核态。

  **虽然用户态下和内核态下工作的程序有很多差别，但最重要的差别就在于特权级的不同，即权力的不同。**运行在用户态下的程序不能直接访问操作系统内核数据结构和程序，比如上面例子中的testfork()就不能直接调用sys_fork()，因为前者是工作在用户态，属于用户态程序，而sys_fork()是工作在内核态，属于内核态程序。

  当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态，比如testfork()最初运行在用户态进程下，当它调用fork()最终触发sys_fork()的执行时，就切换到了内核态。

   

  2. 用户态和内核态的转换

  1）用户态切换到内核态的3种方式

  a. 系统调用

  这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。

  b. 异常

  当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。

  c. 外围设备的中断

  当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。

   

  这3种方式是系统在运行时由用户态转到内核态的最主要方式，其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。

   

  2）具体的切换操作

  **从触发方式上看，可以认为存在前述3种不同的类型，但是从最终实际完成由用户态到内核态的切换操作上来说，涉及的关键步骤是完全一致的，没有任何区别，都相当于执行了一个中断响应的过程**，因为系统调用实际上最终是中断机制实现的，而异常和中断的处理机制基本上也是一致的，关于它们的具体区别这里不再赘述。关于中断处理机制的细节和步骤这里也不做过多分析，涉及到由用户态切换到内核态的步骤主要包括：

  [1] 从当前进程的描述符中提取其内核栈的ss0及esp0信息。

  [2] 使用ss0和esp0指向的内核栈将当前进程的cs,eip,eflags,ss,esp信息保存起来，这个

  过程也完成了由用户栈到内核栈的切换过程，同时保存了被暂停执行的程序的下一

  条指令。

  [3] 将先前由中断向量检索得到的中断处理程序的cs,eip信息装入相应的寄存器，开始

  执行中断处理程序，这时就转到了内核态的程序执行了。





## CPU 上下文切换

- 概述

  - Linux 是一个多任务操作系统，它支持远大于 CPU 数量的任务同时运行。当然，这些任务实际上并不是真的在同时运行，而是因为系统在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉。
  - 根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是进程上下文切换、线程上下文切换以及中断上下文切换。

- CPU 上下文切换

  - 而在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说，需要系统事先帮它设置好 CPU 寄存器和程序计数器（Program Counter，PC）
    - CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存
    - 程序计数器，则是用来存储CPU 正在执行的指令位置、或者即将执行的下一条指令位置。
    - 它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做 CPU 上下文。
  - CPU 上下文切换，就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务

- 进程上下文切换

  - 从用户态到内核态的转变，需要通过系统调用来完成
  - 系统调用的过程有没有发生 CPU 上下文的切换呢
    - CPU 寄存器里原来用户态的指令位置，需要先保存起来。
    - 接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。
    - 而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。
  - 进程上下文切换跟系统调用又有什么区别
    - 进程上下文切换，是指从一个进程切换到另一个进程运行
    - 而系统调用过程中一直是同一个进程在运行。
    - 进程是由内核来管理和调度的，进程的切换只能发生在内核态
    - 进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。
  - 进程在什么时候才会被调度到 CPU 上运行呢
    - 其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。
    - 其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。
    - 其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。
    - 其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。
    - 最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。

- 线程上下文切换

  - 线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位
    - 说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源
    - 线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的
  - 线程的上下文切换其实就可以分为两种情况
    - 第一种， 前后两个线程属于不同进程。
    - 第二种，前后两个线程属于同一个进程。

- 中断上下文切换

  - 为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行。
  - 跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。
    - 即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。
    - 中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等

- 怎么查看系统的上下文切换情况

  - vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。

    - r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。
    - b（Blocked）则是处于不可中断睡眠状态的进程数。
    - in（interrupt）则是每秒中断的次数。
    - cs（context switch）是每秒上下文切换的次数。

  - vmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。

    - cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数
      - 自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。
    - nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。
      - 非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。

  - sysbench 是一个多线程的基准测试工具，一般用来评估不同系统参数下的数据库负载情况。当然，在这次案例中，我们只把它当成一个异常进程来看，作用是模拟上下文切换过多的问题

  - > 以 10 个线程运行 5 分钟的基准测试，模拟多线程切换的问题
    >
    > $ sysbench --threads=10 --max-time=300 threads run

  - 中断只发生在内核态，而 pidstat 只是一个进程的性能分析工具，并不提供任何关于中断的详细信息，怎样才能知道中断发生的类型呢

    - /proc 实际上是 Linux 的一个虚拟文件系统，用于内核空间与用户空间之间的通信。

    - > -d 参数表示高亮显示变化的区域
      > $ watch -d cat /proc/interrupts

    - 变化速度最快的是重调度中断（RES）

      - 这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。
      - 这是多处理器系统（SMP）中，调度器用来分散任务到不同 CPU 的机制，通常也被称为处理器间中断



## 某个应用的CPU使用率达到100%

- CPU 使用率

  - 单位时间内 CPU 使用情况的统计，以百分比的方式展示

  - Linux 通过 /proc 虚拟文件系统，向用户空间提供了系统内部状态的信息，而 /proc/stat提供的就是系统的 CPU 和任务统计信息

  - 跟系统的指标类似，Linux也给每个进程提供了运行情况的统计信息，也就是 /proc/[pid]/stat。

  - 事实上，为了计算 CPU 使用率，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率

  - $$
    平均CPU使用率=1-\frac{空闲时间_{new}-空闲时间_{old}}{总CPU时间_{new}-总CPU时间_{old}}
    $$

  - 对比一下 top 和 ps 这两个工具报告的 CPU 使用率，默认的结果很可能不一样，因为 top 默认使用 3 秒时间间隔，而 ps 使用的却是进程的整个生命周期。

  - top命令，每个进程都有一个 %CPU 列，表示进程的CPU 使用率。它是用户态和内核态 CPU 使用率的总和

- pidstat

  - 用户态 CPU 使用率 （%usr）；
  - 内核态 CPU 使用率（%system）；
  - 运行虚拟机 CPU 使用率（%guest）；
  - 等待 CPU 使用率（%wait）；
  - 以及总的 CPU 使用率（%CPU）。

- CPU 使用率过高怎么办

  - 哪种工具适合在第一时间分析进程的 CPU 问题呢？我的推荐是 perf
    - 它以性能事件采样为基础，不仅可以分析系统的各种事件和内核性能，还可以用来分析指定应用程序的性能问题。
  - 第一种常见用法是 perf top，类似于 top，它能够实时显示占用 CPU 时钟最多的函数或者指令，因此可以用来查找热点函数
    - 表格式样的数据
    - 第一列 Overhead ，是该符号的性能事件在所有采样中的比例，用百分比来表示
    - 第二列 Shared ，是该函数或指令所在的动态共享对象（Dynamic Shared Object），如内核、进程名、动态链接库名、内核模块名等。
    - 第三列 Object ，是动态共享对象的类型。比如 [.] 表示用户空间的可执行程序、或者动态链接库，而 [k] 则表示内核空间
    - 最后一列 Symbol 是符号名，也就是函数名。当函数名未知时，用十六进制的地址来表示。
  - 第二种常见用法，也就是 perf record 和 perf report
    - 在实际使用中，我们还经常为 perf top 和 perf record 加上 -g 参数，开启调用关系的采样，方便我们根据调用链来分析性能问题。

  



## 系统的 CPU 使用率很高

- 测试

  - > 并发 100 个请求测试 Nginx 性能，总共测试 1000 个请求
    >
    > 请求时长为 10 分钟（-t 600）                       
    >
    > 2 $ ab -c 100 -n 1000  -t 600 http://192.168.0.10:10000/

- 明明用户 CPU 使用率已经高达 80%，但我却怎么都找不到是哪个进程的问题。

  - 这次从头开始看 top 的每行输出，Tasks 这一行看起来有点奇怪，就绪队列中居然有6 个 Running 状态的进程（6 running），是不是有点多呢？
  - 再仔细看进程列表，这次主要看 Running（R） 状态的进程。你有没有发现， Nginx 和所有的 php-fpm 都处于 Sleep（S）状态，而真正处于 Running（R）状态的，却是几个stress 进程。
  - ps aux | grep 24344  还是没有输出。现在终于发现问题，原来这个进程已经不存在了
  - pstree 就可以用树状形式显示所有进程之间的关系
    - pstree | grep stress
    - 从这里可以看到，stress 是被应用调用的子进程，并且进程数量不止一个（这里是 3个）。找到父进程后，我们能进入 app 的内部分析了。
    - 找到了，果然是 源码中直接调用了 stress 命令。
    - stress 会通过 write() 和 unlink() 对 I/O 进程进行压测，看来，这应该就是系统 CPU 使用率升高的根源了。
  - 是不是真的有大量的 stress 进程。该用什么工具或指标呢？
    - perf ，它可以用来分析 CPU 性能事件

- execsnoop

  - 在这个案例中，我们使用了 top、pidstat、pstree 等工具分析了系统 CPU 使用率高的问题，并发现 CPU 升高是短时进程 stress 导致的，但是整个分析过程还是比较复杂的。
  - execsnoop 就是一个专为短时进程设计的工具。它通过 ftrace 实时监控进程的 exec() 行为，并输出短时进程的基本信息，包括进程 PID、父进程 PID、命令行参数以及执行的结果
  - 用 execsnoop 监控上述案例，就可以直接得到 stress 进程的父进程 PID 以及它的命令行参数，并可以发现大量的 stress 进程在不停启动

- **碰到常规问题无法解释的 CPU 使用率情况时，首先要想到有可能是短时应用导致的问题，比如有可能是下面这两种情况**

  - 第一，应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过 top 等工具也不容易发现。
  - 第二，应用本身在不停地崩溃重启，而启动过程的资源初始化，很可能会占用相当多的CPU
  - 对于这类进程，我们可以用 pstree 或者execsnoop 找到它们的父进程，再从父进程所在的应用入手，排查问题的根源。







## 系统中出现大量不可中断进程和僵尸进程

- 当 iowait 升高时，进程很可能因为得不到硬件的响应，而长时间处于不可中断状态。

- R、D、Z、S、I 等几个状态

  - R 是 Running 或 Runnable 的缩写，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行
  - D 是 Disk Sleep 的缩写，也就是不可中断状态睡眠（Uninterruptible Sleep），一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断
  - Z 是 Zombie 的缩写。它表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。
    - 这是多进程应用很容易碰到的问题。正常情况下，当一个进程创建了子进程后，它应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源；而子进程在结束时，会向它的父进程发送 SIGCHLD 信号，所以，父进程还可以注册SIGCHLD 信号的处理函数，异步回收资源。
    - 如果父进程没这么做，或是子进程执行太快，父进程还没来得及处理子进程状态，子进程就已经提前退出，那这时的子进程就会变成僵尸进程。
  - S 是 Interruptible Sleep 的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入 R 状态。
  - I 是 Idle 的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。
    - 硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。

- iowait 分析

  - dstat ，可以同时查看 CPU 和 I/O 这两种资源的使用情况，便于对比分析。
  - 进程想要访问磁盘，就必须使用系统调用，所以接下来，重点就是找出 app 进程的系统调用了。
    - strace 正是最常用的跟踪进程系统调用的工具
      - 这儿出现了一个奇怪的错误，strace 命令居然失败了，并且命令报出的错误是没有权限
      - 因为进程已经变成了 Z 状态，也就是僵尸进程。
    - 你可以用 perf top 看看有没有新发现
      - 罪魁祸首是进程内部进行了磁盘的直接 I/O

- 僵尸进程

  - 找出父进程，然后在父进程里解决

    - > 1 # -a 表示输出命令行选项
      > 2 # p 表示 PID
      > 3 # s 表示指定进程的父进程
      > 4 $ pstree -aps 3084

- iowait 高不一定代表 I/O 有性能瓶颈。当系统中只有 I/O 类型的进程在运行时，iowait 也会很高，但实际上，磁盘的读写远没有达到性能瓶颈的程度。

  - 因此，碰到 iowait 升高时，需要先用 dstat、pidstat 等工具，确认是不是磁盘 I/O 的问题，然后再找是哪些进程导致了 I/O。
  - 等待 I/O 的进程一般是不可中断状态，所以用 ps 命令找到的 D 状态（即不可中断状态）的进程，多为可疑进程。
  - 这种情况下，我们用了 perf 工具，来分析系统的 CPU 时钟事件，最终发现是直接 I/O 导致的问题。这时，再检查源码中对应位置的问题，就很轻松了。





## Linux软中断

- 进程的不可中断状态是系统的一种保护机制，可以保证硬件的交互过程不被意外打断。所以，短时间的不可中断状态是很正常的。

- 软中断

  - 为了解决中断处理程序执行过长和中断丢失的问题，Linux 将中断处理过程分成了两个阶段，也就是上半部和下半部
  - 网卡接收数据包的例子
    - 网卡接收到数据包后，会通过硬件中断的方式，通知内核有新的数据到了。这时，内核就应该调用中断处理程序来响应它。
    - 上半部直接处理硬件请求，也就是我们常说的硬中断，特点是快速执行；
    - 而下半部则是由内核触发，也就是我们常说的软中断，特点是延迟执行。
  - 实际上，上半部会打断 CPU 正在执行的任务，然后立即执行中断处理程序。而下半部以内核线程的方式执行，并且每个 CPU 都对应一个软中断内核线程，名字为 “ksoftirqd/CPU编号”，比如说， 0 号 CPU 对应的软中断内核线程的名字就是 ksoftirqd/0。

- 查看软中断和内核线程

  - Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看/proc/softirqs 来观察软中断的运行情况。

  - > /proc/softirqs 提供了软中断的运行情况；
    > /proc/interrupts 提供了硬中断的运行情况。

  - 每个 CPU 都对应一个软中断内核线程，这个软中断内核线程就叫做 ksoftirqd/CPU 编号

    - > $ ps aux | grep softirq
      > 这些线程的名字外面都有中括号，这说明 ps 无法获取它们的命令行参数。一般来说，ps 的输出中，名字括在中括号里的，一般都是内核线程。





## 系统的软中断CPU使用率升高

- sar、 hping3 和 tcpdump

  - sar 是一个系统活动报告工具，既可以实时查看系统的当前活动，又可以配置保存和报告历史统计数据

    - sar 可以用来查看系统的网络收发情况，还有一个好处是，不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数），还可以观察网络收发的 PPS，即每秒收发的网络帧数。

    - > 1 # -n DEV 表示显示网络收发的报告，间隔 1 秒输出一组数据
      > 2 $ sar -n DEV 1

  - hping3 是一个可以构造 TCP/IP 协议数据包的工具，可以对系统进行安全审计、防火墙测试等。

    - 运行 hping3 命令，来模拟 Nginx 的客户端请求

    - > 1 # -S 参数表示设置 TCP 协议的 SYN（同步序列号），-p 表示目的端口为 80
      > 2 # -i u100 表示每隔 100 微秒发送一个网络帧
      > 3 # 注：如果你在实践过程中现象不明显，可以尝试把 100 调小，比如调成 10 甚至 1
      > 4 $ hping3 -S -p 80 -i u100 192.168.0.30

  - tcpdump 是一个常用的网络抓包工具，常用来分析各种网络问题。

    - > 1 # -i eth0 只抓取 eth0 网卡，-n 不解析协议名和主机名
      > 2 # tcp port 80 表示只抓取 tcp 协议并且端口号为 80 的网络帧
      > 3 $ tcpdump -i eth0 -n tcp port 80

- $ watch -d cat /proc/softirqs

  - TIMER（定时中断）、NET_RX（网络接收）、NET_TX（网络发送）、SCHED（内核调度）、RCU（RCU 锁）等这几个软中断都在不停变化
  - NET_RX，也就是网络数据包接收软中断的变化速率最快。
  - 而其他几种类型的软中断，是保证 Linux 调度、时钟和临界区保护这些正常工作所必需的，所以它们有一定的变化倒是正常的。

- 到这里，我们已经做了全套的性能诊断和分析。从系统的软中断使用率高这个现象出发，通过观察 /proc/softirqs 文件的变化情况，判断出软中断类型是网络接收中断；再通过 sar和 tcpdump ，确认这是一个 SYN FLOOD 问题。

  - SYN FLOOD 问题最简单的解决方法，就是从交换机或者硬件防火墙中封掉来源 IP，这样SYN FLOOD 网络帧就不会发送到服务器中。





## 如何迅速分析出系统CPU的瓶颈在哪里

- CPU 性能指标
  - 最容易想到的应该是 CPU 使用率
    - CPU 使用率描述了非空闲时间占总 CPU 时间的百分比，根据 CPU 上运行任务的不同，又被分为用户 CPU、系统 CPU、等待 I/O CPU、软中断和硬中断等。
  - 平均负载（Load Average）
  - 进程上下文切换
    - 无法获取资源而导致的自愿上下文切换；
    - 被系统强制调度导致的非自愿上下文切换。
  - 还有一个指标，CPU 缓存的命中率
- 性能工具
  - 平均负载的案例。我们先用 uptime， 查看了系统的平均负载；而在平均负载升高后，又用 mpstat 和 pidstat ，分别观察了每个 CPU 和每个进程 CPU 的使用情况，进而找出了导致平均负载升高的进程，也就是我们的压测工具stress。
  - 第二个，上下文切换的案例。我们先用 vmstat ，查看了系统的上下文切换次数和中断次数；然后通过 pidstat ，观察了进程的自愿上下文切换和非自愿上下文切换情况；最后通过pidstat ，观察了线程的上下文切换情况，找出了上下文切换次数增多的根源，也就是我们的基准测试工具 sysbench。
  - 第三个，进程 CPU 使用率升高的案例。我们先用 top ，查看了系统和进程的 CPU 使用情况，发现 CPU 使用率升高的进程是 php-fpm；再用 perf top ，观察 php-fpm 的调用链，最终找出 CPU 升高的根源，也就是库函数 sqrt() 。
  - 第四个，系统的 CPU 使用率升高的案例。我们先用 top 观察到了系统 CPU 升高，但通过top 和 pidstat ，却找不出高 CPU 使用率的进程。于是，我们重新审视 top 的输出，又从CPU 使用率不高但处于 Running 状态的进程入手，找出了可疑之处，最终通过 perfrecord 和 perf report ，发现原来是短时进程在捣鬼。
  - 第五个，不可中断进程和僵尸进程的案例。我们先用 top 观察到了 iowait 升高的问题，并发现了大量的不可中断进程和僵尸进程；接着我们用 dstat 发现是这是由磁盘读导致的，于是又通过 pidstat 找出了相关的进程。但我们用 strace 查看进程系统调用却失败了，最终还是用 perf 分析进程调用链，才发现根源在于磁盘直接 I/O 。
  - 最后一个，软中断的案例。我们通过 top 观察到，系统的软中断 CPU 使用率升高；接着查看 /proc/softirqs， 找到了几种变化速率较快的软中断；然后通过 sar 命令，发现是网络小包的问题，最后再用 tcpdump ，找出网络帧的类型和来源，确定是一个 SYN FLOOD攻击导致的。
- **性能指标和性能工具**
  - 第一个维度，从 CPU 的性能指标出发。也就是说，当你要查看某个性能指标时，要清楚知道哪些工具可以做到。
    - ![img](https://upload-images.jianshu.io/upload_images/11462765-6d4f67c35d53bf18.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp)
  - 第二个维度，从工具出发。也就是当你已经安装了某个工具后，要知道这个工具能提供哪些指标。
    - ![img](https://upload-images.jianshu.io/upload_images/11462765-7d6c6ae539f628cc.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp)
  - 为了缩小排查范围，我通常会先运行几个支持指标较多的工具，如 top、vmstat 和pidstat
    - ![img](https://upload-images.jianshu.io/upload_images/11462765-faed248242889ff0.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp)
  - **第一个例子，pidstat 输出的进程用户 CPU 使用率升高**，会导致 top 输出的用户 CPU 使用率升高。所以，当发现 top 输出的用户 CPU 使用率有问题时，可以跟 pidstat 的输出做对比，观察是否是某个进程导致的问题。
    - 而找出导致性能问题的进程后，就要用进程分析工具来分析进程的行为，比如使用 strace 分析系统调用情况，以及使用 perf 分析调用链中各级函数的执行情况。
  - **第二个例子，top 输出的平均负载升高**，可以跟 vmstat 输出的运行状态和不可中断状态的进程数做对比，观察是哪种进程导致的负载升高。
  - **最后一个例子，当发现 top 输出的软中断 CPU 使用率升高时**，可以查看 /proc/softirqs 文件中各种类型软中断的变化情况，确定到底是哪种软中断出的问题。比如，发现是网络接收中断导致的问题，那就可以继续用网络分析工具 sar 和 tcpdump 来分析。



## **CPU** **性能优化的几个思路**

- **怎么评估性能优化的效果**

  - 应用程序的维度，我们可以用**吞吐量和请求延迟**来评估应用程序的性能。 
  - 系统资源的维度，我们可以用 **CPU 使用率**来评估系统的 CPU 使用情况。 
  - 还是以刚刚的 Web 应用为例，对应上面提到的几个指标，我们可以选择 ab 等工具，测试Web 应用的并发请求数和响应延迟。而测试的同时，还可以用 vmstat、pidstat 等性能工具，观察系统和进程的 CPU 使用率。这样，我们就同时获得了应用程序和系统资源这两个 维度的指标数值。

- **CPU** **优化**

  - **应用程序优化**

    - 比如减少循环的层次、减少递归、减少动态内存分配等等。
    - **编译器优化**
    - **算法优化**
    - **异步处理**
    - **多线程代替多进程**
    - **善用缓存**

  - **系统优化**

    - 从系统的角度来说，优化 CPU 的运行，一方面要充分利用 CPU 缓存的本地性，加速缓存访问；另一方面，就是要控制进程的 CPU 使用情况，减少进程间的相互影响。 
    - **CPU 绑定**：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。 
    - **CPU 独占**：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配 进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。 
    - **优先级调整**：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。优先级 的数值含义前面我们提到过，忘了的话及时复习一下。在这里，适当降低非核心应用的优 先级，增高核心应用的优先级，可以确保核心应用得到优先处理。 
    - **为进程设置资源限制**：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于 某个应用自身的问题，而耗尽系统资源。 
    - **NUMA（Non-Uniform Memory Access）优化**：支持 NUMA 的处理器会被划分为 多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。 
    - **中断负载均衡**：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均 衡到多个 CPU 上。

    



## Linux 文件系统是怎么工作的

- 概述

  - 磁盘为系统提供了最基本的持久化存储
  - 文件系统则在磁盘的基础上，提供了一个用来管理文件的树状结构

- 索引节点和目录项

  - 为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。
  - 换句话说，索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。
    - 索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。
    - 目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。
  - 实际上，磁盘读写的最小单位是扇区，然而扇区只有 512B 大小，如果每次都读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成了逻辑块，然后每次都以逻辑块为最小单元，来管理数据。常见的逻辑块大小为 4KB，也就是由连续的 8 个扇区组成。

- 磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区

  - 超级块，存储整个文件系统的状态。
  - 索引节点区，用来存储索引节点。
  - 数据块区，则用来存储文件数据。

- 虚拟文件系统

  - 为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS（Virtual File System）。
    - 用户进程和内核中的其他子系统，只需要跟 VFS 提供的统一接口进行交互就可以了，而不需要再关心底层各种文件系统的实现细节。
  - 这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。
    - 基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录（/），在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。

- 文件系统 I/O

  - 文件读写方式的各种差异，导致 I/O 的分类多种多样。
    - 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。
    - 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存。
    - 无论缓冲 I/O 还是非缓冲 I/O，它们最终还是要经过系统调用来访问文件。我们知道，系统调用后，还会通过页缓存，来减少磁盘的 I/O 操作。
  - 第一种，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O。
  - 第二，根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O。
    - 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。
    - 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘
  - 第三，根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O：
  - 第四，根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O

- 你也应该可以理解，“Linux 一切皆文件”的深刻含义。无论是普通文件和块设备、还是网络套接字和管道等，它们都通过统一的 VFS 接口来访问。

- 性能观测

  - > 对文件系统来说，最常见的一个问题就是空间不足。用 df 命令，就能查看文件系统的磁盘空间使用情况
    > 	 $ df  -h /dev/sda1

  - 不过有时候，明明你碰到了空间不足的问题，可是用 df 查看磁盘空间后，却发现剩余空间还有很多。这是怎么回事呢？

    - 除了文件数据，索引节点也占用磁盘空间。你可以给 df 命令加上 -i 参数，查看索引节点的使用情况
    - 当你发现索引节点空间不足，但磁盘空间充足时，很可能就是过多小文件导致的。
    - 所以，一般来说，删除这些小文件，或者把它们移动到索引节点充足的其他磁盘中，就可以解决这个问题







## 磁盘I/O

- 通用块层

  - 为了减小不同块设备的差异带来的影响，Linux 通过一个统一的通用块层，来管理各种不同的块设备。
  - 通用块层，其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。它主要有两个功能
    - 向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序
    - 对 I/O 请求排序的过程，也就是我们熟悉的 I/O 调度

- I/O 栈

  - 我们可以把 Linux 存储系统的 I/O 栈，由上到下分为三个层次，分别是文件系统层、通用块层和设备层

    - 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。它为上层的应用程序，提供标准的文件访问接口；对下会通过通用块层，来存储和管理磁盘数据。

    - 通用块层，包括块设备 I/O 队列和 I/O 调度器。它会对文件系统的 I/O 请求进行排队，再通

      过重新排序和请求合并，然后才要发送给下一级的设备层。

    - 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O 操作。

  - 存储系统的 I/O ，通常是整个系统中最慢的一环。所以， Linux 通过多种缓存机制来优化I/O 效率

    - 比方说，为了优化文件访问的性能，会使用页缓存、索引节点缓存、目录项缓存等多种缓存机制，以减少对下层块设备的直接调用。
    - 同样，为了优化块设备的访问效率，会使用缓冲区，来缓存块设备的数据

- 磁盘性能指标

  - 必须要提到五个常见指标，也就是我们经常用到的，使用率、饱和度、IOPS、吞吐量以及响应时间等。这五个指标，是衡量磁盘性能的基本指标。

    - 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈

      - 这里要注意的是，使用率只考虑有没有 I/O，而不考虑 I/O 的大小。换句话说，当使用率是100% 的时候，磁盘依然有可能接受新的 I/O 请求。

    - 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性

      能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。

    - IOPS（Input/Output Per Second），是指每秒的 I/O 请求数。

    - 吞吐量，是指每秒的 I/O 请求大小。

    - 响应时间，是指 I/O 请求从发出到收到响应的间隔时间

- 磁盘 I/O 观测

  - iostat 是最常用的磁盘 I/O 性能观测工具，它提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats。

  - > 1 # -d -x 表示显示所有磁盘 I/O 的指标
    > 2 $ iostat -d -x 1
    >
    > 
    >
    > %util ，就是我们前面提到的磁盘 I/O 使用率；
    > r/s+ w/s ，就是 IOPS；
    > rkB/s+wkB/s ，就是吞吐量；
    > r_await+w_await ，就是响应时间。

- 进程 I/O 观测

  - 要观察进程的 I/O 情况，你还可以使用 pidstat 和 iotop 这两个工具。

  - > $ pidstat -d 1

  - iotop。它是一个类似于 top 的工具，你可以按照 I/O大小对进程排序，然后找到 I/O 较大的那些进程。





## I/O瓶颈

- 我们可以先用 top ，来观察 CPU 和内存的使用情况；然后再用 iostat ，来观察磁盘的 I/O 情况。

- 观察 top 的输出，CPU0 的使用率非常高，它的系统 CPU 使用率（sys%）为6%，而 iowait 超过了 90%。这说明 CPU0 上，可能正在运行 I/O 密集型的进程

- 看内存的使用情况，总内存 8G，剩余内存只有 730 MB，而 Buffer/Cache 占用内存高达 6GB 之多，这说明内存主要被缓存占用。

- 到这一步，你基本可以判断出，CPU 使用率中的 iowait 是一个潜在瓶颈，而内存部分的缓存占比较大，那磁盘 I/O 又是怎么样的情况呢？

- 使用 pidstat 加上 -d 参数，就可以显示每个进程的 I/O 情况

- lsof

  - 它专门用来查看进程打开文件列表，不过，这里的“文件”不只有普通文件，还包括了目录、块设备、动态库、网络套接字等

  - -p 参数需要指定进程号

    - > -t 表示显示线程，-a 表示显示命令行参数
      >
      > $ pstree -t -a -p [pid]
      >
      > 找到了原因，lsof 的问题就容易解决了。把线程号换成进程号，继续执行 lsof 命令

  - > FD 表示文件描述符号，TYPE 表示文件类型，NAME 表示文件路径
    >
    > python 18940 root 3w REG 8,1 117944320 303 /tmp/logtest.txt
    >
    > 再看最后一行，这说明，这个进程打开了文件 /tmp/logtest.txt，并且它的文件描述符是 3号，而 3 后面的 w ，表示以写的方式打开







## 磁盘I/O延迟很高

- 随便执行一个命令，比如执行 df 命令，查看一下文件系统的使用情况。奇怪的是，这么简单的命令，居然也要等好久才有输出

  - 写文件是由子线程执行的，所以直接strace跟踪进程没有看到write系统调用，可以通过pstree查看进程的线程信息，再用strace跟踪。或者，通过strace -fp pid 跟踪所有线程。
  - 从 strace 中，你可以看到大量的 stat 系统调用，并且大都为 python 的文件，但是，请注意，这里并没有任何 write 系统调用。
  - 我们只好综合 strace、pidstat 和 iostat 这三个结果来分析了。很明显，你应该发现了这里的矛盾：iostat 已经证明磁盘 I/O 有性能瓶颈，而 pidstat 也证明了，这个瓶颈是由12280 号进程导致的，但 strace 跟踪这个进程，却没有找到任何 write 系统调用
  - 文件写，明明应该有相应的 write 系统调用，但用现有工具却找不到痕迹，这时就该想想换工具的问题了。怎样才能知道哪里在写文件呢？

- filetop

  - 它是 bcc 软件包的一部分，主要跟踪内核中文件的读写情况，并输出线程 ID（TID）、读写大小、读写类型以及文件名称。

  - > 1 # 切换到工具目录
    > 2 $ cd /usr/share/bcc/tools 
    > 34 # -C 选项表示输出新内容时不清空屏幕
    > 5 $ ./filetop -C
    >
    > 
    >
    > 多观察一会儿，你就会发现，每隔一段时间，线程号为 514 的 python 应用就会先写入大量的 txt 文件，再大量地读。

- opensnoop 

  - 它同属于 bcc 软件包，可以动态跟踪内核中的open 系统调用。这样，我们就可以找出这些 txt 文件的路径。
  - 这次，通过 opensnoop 的输出，你可以看到，这些 txt 路径位于 /tmp 目录下。你还能看到，它打开的文件数量，按照数字编号，从 0.txt 依次增大到 999.txt，这可远多于前面用filetop 看到的数量
  - 综合 filetop 和 opensnoop ，我们就可以进一步分析了。我们可以大胆猜测，案例应用在写入 1000 个 txt 文件后，又把这些内容读到内存中进行处理。
  - 结合前面的所有分析，我们基本可以判断，案例应用会动态生成一批文件，用来临时存储数据，用完就会删除它们。但不幸的是，正是这些文件读写，引发了 I/O 的性能瓶颈，导致整个处理过程非常慢

  





## SQL慢查询

- top、iostat、pidstat、strace
- lsof
  - 从输出中可以看到， mysqld 进程确实打开了大量文件，而根据文件描述符（FD）的编号，我们知道，描述符为 38 的是一个路径为/var/lib/mysql/test/products.MYD 的文件。这里注意， 38 后面的 u 表示， mysqld 以读写的方式访问文件。
    - MYD 文件，是 MyISAM 引擎用来存储表数据的文件；
    - 文件名就是数据表的名字；
    - 而这个文件的父目录，也就是数据库的名字。
    - 换句话说，这个文件告诉我们，mysqld 在读取数据库 test 中的 products 表。
- 既然已经找出了数据库和表，接下来要做的，就是弄清楚数据库中正在执行什么样的 SQL了







## Redis响应严重延迟

- top、iostat、pidstat、strace
- lsof
  - 结合磁盘写的现象，我们知道，只有 7 号普通文件才会产生磁盘写，而它操作的文件路径是 /data/appendonly.aof，相应的系统调用包括 write 和 fdatasync
  - 这对应着正是 Redis 持久化配置中的 appendonly 和 appendfsync 选项
- 查询 appendonly 和 appendfsync 的配置
  - 从这个结果你可以发现，appendfsync 配置的是 always，而 appendonly 配置的是yes。
  - appendfsync 配置的是 always，意味着每次写数据时，都会调用一次 fsync，从而造成比较大的磁盘 I/O 压力。
- iowait不代表磁盘I/O存在瓶颈，只是代表CPU上I/O操作的时间占用的百分比。假如这时候没有其他进程在运行，那么很小的I/O就会导致iowait升高
- 进程iowait高，磁盘iowait不高，说明是单个进程使用了一些blocking的磁盘打开方式，比如每次都fsync







## 如何分析出系统I/O的瓶颈

- 性能指标
  - ![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8yMTEwNTgwNi05ZGIzYmZmNTgyNmRmNDBmLnBuZw?x-oss-process=image/format,png)
  - **文件系统 I/O 性能指标**
    - 首先，最容易想到的是存储空间的使用情况，包括容量、使用量以及剩余空间等
      - 不过要注意，这些只是文件系统向外展示的空间使用，而非在磁盘空间的真实用量，因为文件系统的元数据也会占用磁盘空间。
      - 而且，如果你配置了 RAID，从文件系统看到的使用量跟实际磁盘的占用空间，也会因为RAID 级别的不同而不一样。比方说，配置RAID10 后，你从文件系统最多也只能看到所有磁盘容量的一半。
      - 除了数据本身的存储空间，还有一个容易忽略的是索引节点的使用情况，它也包括容量、使用量以及剩余量等三个指标。如果文件系统中存储过多的小文件，就可能碰到索引节点容量已满的问题
    - 其次，你应该想到的是前面多次提到过的缓存使用情况，包括页缓存、目录项缓存、索引节点缓存以及各个具体文件系统（如 ext4、XFS 等）的缓存
    - 除了以上这两点，文件 I/O 也是很重要的性能指标，包括 IOPS（包括 r/s 和 w/s）、响应时间（延迟）以及吞吐量（B/s）等
  - **磁盘 I/O 性能指标**
    - 四个核心的磁盘 I/O 指标。
      - **使用率**，是指磁盘忙处理 I/O 请求的百分比。过高的使用率（比如超过 60%）通常意味 着磁盘 I/O 存在性能瓶颈。 
      - **IOPS**（Input/Output Per Second），是指每秒的 I/O 请求数。 
      - **吞吐量**，是指每秒的 I/O 请求大小。
      - **响应时间**，是指从发出 I/O 请求到收到响应的间隔时间。 
    - 考察这些指标时，一定要注意综合 I/O 的具体场景来分析，比如读写类型（顺序还是随机）、读写比例、读写大小、存储类型（有无RAID 以及 RAID 级别、本地存储还是网络存储）等
    - 缓冲区（Buffer）也是要重点掌握的指标，它经常出现在内存和磁盘问题的分析中
- **性能工具**
  - 第一，在文件系统的原理中，我介绍了查看文件系统容量的工具 df。它既可以查看文件系统数据的空间容量，也可以查看索引节点的容量。至于文件系统缓存，我们通过 /proc/meminfo、/proc/slabinfo 以及 slabtop 等各种来源，观察页缓存、目录项缓存、索引节点缓存以及具体文件系统的缓存情况。
  - 第二，在磁盘 I/O 的原理中，我们分别用 iostat 和 pidstat 观察了磁盘和进程的 I/O 情况。它们都是最常用的 I/O 性能分析工具。通过 iostat ，我们可以得到磁盘的 I/O 使用 率、吞吐量、响应时间以及 IOPS 等性能指标；而通过 pidstat ，则可以观察到进程的 I/O吞吐量以及块设备 I/O 的延迟等。 
  - 第三，在狂打日志的案例中，我们先用 top 查看系统的 CPU 使用情况，发现 iowait 比较 高；然后，又用 iostat 发现了磁盘的 I/O 使用率瓶颈，并用 pidstat 找出了大量 I/O 的进程；最后，通过 strace 和 lsof，我们找出了问题进程正在读写的文件，并最终锁定性能问题的来源——原来是进程在狂打日志。 
  - 第四，在磁盘 I/O 延迟的单词热度案例中，我们同样先用 top、iostat ，发现磁盘有 I/O 瓶颈，并用 pidstat 找出了大量 I/O 的进程。可接下来，想要照搬上次操作的我们失败 了。在随后的 strace 命令中，我们居然没看到 write 系统调用。于是，我们换了一个思路，用新工具 filetop 和 opensnoop ，从内核中跟踪系统调用，最终找出瓶颈的来源。 
  - 最后，在 MySQL 和 Redis 的案例中，同样的思路，我们先用 top、iostat 以及 pidstat ， 确定并找出 I/O 性能问题的瓶颈来源，它们正是 mysqld 和 redis-server。随后，我们又用 strace+lsof 找出了它们正在读写的文件。 
  - 关于 MySQL 案例，根据 mysqld 正在读写的文件路径，再结合 MySQL 数据库引擎的原理，我们不仅找出了数据库和数据表的名称，还进一步发现了慢查询的问题，最终通过优化索引解决了性能瓶颈。
  - 至于 Redis 案例，根据 redis-server 读写的文件，以及正在进行网络通信的 TCPSocket，再结合 Redis 的工作原理，我们发现 Redis 持久化选项配置有问题；从 TCP Socket 通信的数据中，我们还发现了客户端的不合理行为。于是，我们修改 Redis 配置选 项，并优化了客户端使用 Redis 的方式，从而减少网络通信次数，解决性能问题

## **磁盘** **I/O** **性能优化的几个思路**

- **I/O** **基准测试**

  - 为了更客观合理地评估优化效果，我们首先应该对磁盘和文件系统进行基准测试，得到文件系统或者磁盘 I/O 的极限性能。 

  - fio（Flexible I/O Tester）正是最常用的文件系统和磁盘 I/O 性能基准测试工具。它提供 了大量的可定制化选项，可以用来测试，裸盘或者文件系统在各种场景下的 I/O 性能，包括了不同块大小、不同 I/O 引擎以及是否使用缓存等场景。

  - fio 的选项非常多， 我会通过几个常见场景的测试方法，介绍一些最常用的选项。这些常见场景包括随机读、随机写、顺序读以及顺序写等，你可以执行下面这些命令来测试：

    ```text
    #随机读
    fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G
    #随机写
    fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G
    #顺序读
    fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=
    #顺序写
    fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjob
    ```

    在这其中，有几个参数需要你重点关注一下。

    - direct，表示是否跳过系统缓存。上面示例中，我设置的 1 ，就表示跳过系统缓存。

    - iodepth，表示使用异步 I/O（asynchronous I/O，简称 AIO）时，同时发出的 I/O 请求上限。在上面的示例中，我设置的是 64。

    - rw，表示 I/O 模式。我的示例中， read/write 分别表示顺序读 / 写，而randread/randwrite 则分别表示随机读 / 写。

    - ioengine，表示 I/O 引擎，它支持同步（sync）、异步（libaio）、内存映射（mmap）、网络（net）等各种 I/O 引擎。上面示例中，我设置的 libaio 表示使用异步I/O。

    - bs，表示 I/O 的大小。示例中，我设置成了 4K（这也是默认值）。

    - filename，表示文件路径，当然，它可以是磁盘路径（测试磁盘性能），也可以是文件路径（测试文件系统性能）。示例中，我把它设置成了磁盘 /dev/sdb。不过注意，用磁盘路径测试写，会破坏这个磁盘中的文件系统，所以在使用前，你一定要事先做好数据备份。

    - 这个报告中，需要我们重点关注的是， slat、clat、lat ，以及 bw 和 iops 这几行。

    - 先来看刚刚提到的前三个参数。事实上，slat、clat、lat 都是指 I/O 延迟（latency）。不同之处在于：

      - slat ，是指从 I/O 提交到实际执行 I/O 的时长（Submission latency）；
      - clat ，是指从 I/O 提交到 I/O 完成的时长（Completion latency）；
      - 而 lat ，指的是从 fio 创建 I/O 到 I/O 完成的总时长。

      这里需要注意的是，对同步 I/O 来说，由于 I/O 提交和 I/O 完成是一个动作，所以 slat 实际上就是 I/O 完成的时间，而 clat 是 0。而从示例可以看到，使用异步 I/O（libaio）时，lat 近似等于 slat + clat 之和。

      再来看 bw ，它代表吞吐量。在我上面的示例中，你可以看到，平均吞吐量大约是 16MB（17005 KiB/1024）。

      最后的 iops ，其实就是每秒 I/O 的次数，上面示例中的平均 IOPS 为 4250。

    - 幸运的是，fio 支持 I/O 的重放。借助前面提到过的 blktrace，再配合上 fio，就可以实现对应用程序 I/O 模式的基准测试。你需要先用 blktrace ，记录磁盘设备的 I/O 访问情况；然后使用 fio ，重放 blktrace 的记录。

      比如你可以运行下面的命令来操作：

      ```text
      #使用blktrace跟踪磁盘I/O，注意指定应用程序正在操作的磁盘
      $ blktrace /dev/sdb
      #查看blktrace记录的结果
      # ls
      sdb.blktrace.0  sdb.blktrace.1
      #将结果转化为二进制文件
      $ blkparse sdb -d sdb.bin
      #使用fio重放日志
      $ fio --name=replay --filename=/dev/sdb --direct=1 --read_iolog=sdb.bin
      ```

      这样，我们就通过 blktrace+fio 的组合使用，得到了应用程序 I/O 模式的基准测试报告。

- **I/O性能优化**

  - ![img](https://pic2.zhimg.com/80/v2-659237ee83559f10ff60837d591549fd_720w.jpg)

  - **应用程序优化**

    - 应用程序处于整个 I/O 栈的最上端，它可以通过系统调用，来调整 I/O 模式（如顺序还是随机、同步还是异步）， 同时，它也是 I/O 数据的最终来源。在我看来，可以有这么几种方式来优化应用程序的 I/O 性能。
    - 第一，可以用追加写代替随机写，减少寻址开销，加快 I/O 写的速度。
    - 第二，可以借助缓存 I/O ，充分利用系统缓存，降低实际 I/O 的次数。
    - 第三，可以在应用程序内部构建自己的缓存，或者用 Redis 这类外部缓存系统。这样，一方面，能在应用程序内部，控制缓存的数据和生命周期；另一方面，也能降低其他应用程序使用缓存对自身的影响。
      - 比如，在前面的 MySQL 案例中，我们已经见识过，只是因为一个干扰应用清理了系统缓存，就会导致 MySQL 查询有数百倍的性能差距（0.1s vs 15s）。
      - 再如， C 标准库提供的 fopen、fread 等库函数，都会利用标准库的缓存，减少磁盘的操作。而你直接使用 open、read 等系统调用时，就只能利用操作系统提供的页缓存和缓冲区等，而没有库函数的缓存可用。
    - 第四，在需要频繁读写同一块磁盘空间时，可以用 mmap 代替 read/write，减少内存的拷贝次数。
    - 第五，在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写入磁盘，即可以用 fsync() 取代 O_SYNC。
    - 第六，在多个应用程序共享相同磁盘时，为了保证 I/O 不被某个应用完全占用，推荐你使用 cgroups 的 I/O 子系统，来限制进程 / 进程组的 IOPS 以及吞吐量。
    - 最后，在使用 CFQ 调度器时，可以用 ionice 来调整进程的 I/O 调度优先级，特别是提高核心应用的 I/O 优先级。ionice 支持三个优先级类：Idle、Best-effort 和 Realtime。其中， Best-effort 和 Realtime 还分别支持 0-7 的级别，数值越小，则表示优先级别越高。

  - **文件系统优化**

    - 应用程序访问普通文件时，实际是由文件系统间接负责，文件在磁盘中的读写。所以，跟文件系统中相关的也有很多优化 I/O 性能的方式。
    - 第一，你可以根据实际负载场景的不同，选择最适合的文件系统。比如 Ubuntu 默认使用ext4 文件系统，而 CentOS 7 默认使用 xfs 文件系统。
      - 相比于 ext4 ，xfs 支持更大的磁盘分区和更大的文件数量，如 xfs 支持大于 16TB 的磁盘。但是 xfs 文件系统的缺点在于无法收缩，而 ext4 则可以。
    - 第二，在选好文件系统后，还可以进一步优化文件系统的配置选项，包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback）、挂载选项（如 noatime）等等。
      - 比如，使用 tune2fs 这个工具，可以调整文件系统的特性（tune2fs 也常用来查看文件系统超级块的内容）。 而通过 /etc/fstab ，或者 mount 命令行参数，我们可以调整文件系统的日志模式和挂载选项等。
    - 第三，可以优化文件系统的缓存。
      - 比如，你可以优化 pdflush 脏页的刷新频率（比如设置 dirty_expire_centisecs 和dirty_writeback_centisecs）以及脏页的限额（比如调整 dirty_background_ratio 和dirty_ratio 等）
      - 再如，你还可以优化内核回收目录项缓存和索引节点缓存的倾向，即调整vfs_cache_pressure（/proc/sys/vm/vfs_cache_pressure，默认值 100），数值越大，就表示越容易回收。
    - 最后，在不需要持久化时，你还可以用内存文件系统tmpfs，以获得更好的 I/O 性能 。tmpfs 把数据直接保存在内存中，而不是磁盘中。比如 /dev/shm/ ，就是大多数 Linux 默认配置的一个内存文件系统，它的大小默认为总内存的一半。

  - **磁盘优化**

    - 第一，最简单有效的优化方法，就是换用**性能更好的磁盘**，比如用 SSD 替代 HDD。

    - 第二，我们可以使用 **RAID** ，把多块磁盘组合成一个逻辑磁盘，构成冗余独立磁盘阵列。这样做既可以提高数据的可靠性，又可以提升数据的访问性能。

    - 第三，针对磁盘和应用程序 I/O 模式的特征，我们可以选择最适合的 **I/O 调度算法**。比方说，SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法。而数据库应用，我更推荐使用deadline 算法。

    - 第四，我们可以对应用程序的数据，进行**磁盘级别的隔离**。比如，我们可以为日志、数据库等 I/O 压力比较重的应用，配置单独的磁盘。

    - 第五，**在顺序读比较多的场景中，我们可以增大磁盘的预读数据**，比如，你可以通过下面两种方法，调整 /dev/sdb 的预读大小。

      > 调整内核选项 /sys/block/sdb/queue/read_ahead_kb，默认大小是 128 KB，单位为KB。 使用 blockdev 工具设置，比如 blockdev --setra 8192 /dev/sdb，注意这里的单位是512B（0.5KB），所以它的数值总是 read_ahead_kb 的两倍。

    - 第六，我们可以**优化内核块设备 I/O 的选项**。比如，可以调整磁盘队列的长度/sys/block/sdb/queue/nr_requests，适当增大队列长度，可以提升磁盘的吞吐量（当然也会导致 I/O 延迟增大）。

    - 最后，要注意，**磁盘本身出现硬件错误**，也会导致 I/O 性能急剧下降，所以发现磁盘性能急剧下降时，你还需要确认，磁盘本身是不是出现了硬件错误。

      - 比如，你可以查看 dmesg 中是否有硬件 I/O 故障的日志。还可以使用 badblocks、smartctl 等工具，检测磁盘的硬件问题，或用 e2fsck 等来检测文件系统的错误。如果发现问题，你可以使用 fsck 等工具来修复。

    



## Linux内存是怎么工作的

- **内存映射**

  - 我们通常所说的内存容量，就像我刚刚提到的 8GB，其实指的是物理内存。物理内存也称为主存，大多数计算机用的主存都是动态随机访问内存（DRAM）。只有内核才可以直接访问物理内存。那么，进程要访问内存时，该怎么办呢？
  - Linux 内核给每个进程都提供了一个独立的虚拟地址空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存
    - 虚拟地址空间的内部又被分为内核空间和用户空间两部分
    - 进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。
    - 虽然每个进程的地址空间都包含了内核空间，但这些内核空间，其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。 既然每个进程都有一个这么大的地址空间，那么所有进程的虚拟内存加起来，自然要比实际的物理内存大得多。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过内存映射来管理的。
  - **内存映射，其实就是将虚拟内存地址映射到物理内存地址**
    - 为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系
    - 页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。
    - 当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行
    - TLB（转译后备缓冲器）会影响 CPU 的内存访问性能。TLB 其实就是 MMU 中页表的高速缓存。
    - MMU 并不以字节为单位来管理内存，而是规定了一个内存映射的最小单位，也就是页，通常是 4 KB 大小。这样，每一次内存映射，都需要关联 4 KB 或者 4KB 整数倍的内存空间
      - 页的大小只有 4 KB ，导致的另一个问题就是，整个页表会变得非常大
      - Linux 提供了两种机制，也就是多级页表和大页（HugePage）。

- **虚拟内存空间分布**

  - 进一步了解虚拟内存空间的分布情况。最上方的内核空间不用多讲，下方的用户空间内存，其实又被分成了多个不同的段。以 32 位系统为例，从下往上为

    1. 只读段，包括代码和常量等。
    2. 数据段，包括全局变量等。
    3. 堆，包括动态分配的内存，从低地址开始向上增长。
    4. 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。
    5. 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。

    - 在这五个内存段中，堆和文件映射段的内存是动态分配的。

- 如何查看内存使用情况

  - free 输出的是一个表格，其中的数值都默认以字节为单位
  - free 显示的是整个系统的内存使用情况。比如总内存、已用内存、缓存、可用内存等。其中缓存是 Buffer 和 Cache 两部分的总和 。
  - 如果你想查看进程的内存使用情况，可以用 top 或者 ps 等工具（top 按下 M 切换到内存排序）。

- free 数据的来源

  - Buffer 和 Cache 还是我们用 free 获得的指标
  - Buffers 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值。
    - Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据
  - Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与SReclaimable 之和。
    - Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。
  - Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。

  



## 内存中的Buffer和Cache

- Buffer 既可以用作“将要写入磁盘数据的缓存”，也可以用作“从磁盘读取数据的缓存”。

- Cache 既可以用作“从文件读取数据的页缓存”，也可以用作“写文件的页缓存”。

- dd 来模拟磁盘和文件的 I/O

  - > if=文件名：输入文件名，缺省为标准输入。即指定源文件。
    >
    > of=文件名：输出文件名，缺省为标准输出。即指定目的文件。
    >
    > ibs=bytes：一次读入bytes个字节，即指定一个块大小为bytes个字节。
    >
    > obs=bytes：一次输出bytes个字节，即指定一个块大小为bytes个字节。
    >
    > bs=bytes：同时设置读入/输出的块大小为bytes个字节。
    >
    > count=blocks：仅拷贝blocks个块，块大小等于ibs指定的字节数。

  - 文件写

    - > dd if=/dev/urandom of=/tmp/file bs=1M count=500

  - 文件读

    - > dd if=/tmp/file of=/dev/null

  - 磁盘写

    - > dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048

  - 磁盘读

    - > dd if=/dev/sda1 of=/dev/null bs=1M count=1024

- 关于磁盘和文件的区别

  - 磁盘是一个块设备，可以划分为不同的分区；在分区之上再创建文件系统，挂载到某个目录，之后才可以在这个目录中读写文件。
  - Linux 中“一切皆文件”，而文中提到的“文件”是普通文件，磁盘是块设备文件，可以执行 "ls -l <路径>" 查看它们的区别
  - 在读写普通文件时，会经过文件系统，由文件系统负责与磁盘交互
  - 而读写磁盘或者分区时，就会跳过文件系统，也就是所谓的“裸I/O“
  - 这两种读写方式所使用的缓存是不同的，也就是文中所讲的 Cache 和 Buffer 区别。





## 系统缓存

- 概述

  - Buffer 和Cache 的设计目的，是为了提升系统的 I/O 性能
  - 它们利用内存，充当起慢速磁盘与快速CPU 之间的桥梁，可以加速 I/O 的访问速度。
  - 为了方便你理解，Buffer 和 Cache 我仍然用英文表示，避免跟“缓存”一词混淆。而文中的“缓存”，通指数据在内存中的临时存储。

- **缓存命中率**

  - cachestat 提供了整个操作系统缓存的读写命中情况。
  - cachetop 提供了每个进程的缓存命中情况。
    - cachetop 工具并不把直接 I/O 算进来。

- 使用 pcstat 这个工具，来查看文件在内存中的缓存大小以及缓存比例

  - > pcstat /bin/ls
    >
    > 它展示了 /bin/ls 这个文件的缓存情况

  - 如果你执行一下 ls 命令，再运行相同的命令来查看的话，就会发现 /bin/ls 都在缓存中了

- **要判断应用程序是否用了直接 I/O，最简单的方法当然是观察它的系统调用**，查找应用程序在调用它们时的选项。使用什么工具来观察系统调用呢？自然还是 strace。

  - 1024 次缓存全部命中，读的命中率是 100%，看起来全部的读请求都经过了系统缓存。但是问题又来了，如果真的都是缓存 I/O，读取速度不应该这么慢。
    - 内存以页为单位进行管理，而每个页的大小是 4KB。所以，在 5 秒的时间间隔里，命中的缓存为 1024*4K/1024 = 4MB，再除以 5 秒，可以得到每秒读的缓存是0.8MB，显然跟案例应用的 32 MB/s 相差太多。
  - 直接从磁盘读写的速度，自然远慢于对缓存的读写。这也是缓存存在的最大意义了。
    - **直接IO是跳过Buffer，裸IO是跳过文件系统（还是有buffer的）**
    - 修改源代码，删除O_DIRECT 选项，让应用程序使用缓存 I/O ，而不是直接 I/O，就可以加速磁盘读取速度
    - 读的命中率还是 100%，HITS （即命中数）却变成了 40960，同样的方法计算一下，换算成每秒字节数正好是 32 MB（即40960*4k/5/1024=32M）。







## 内存泄漏

- 当进程通过 malloc() 申请虚拟内存后，系统并不会立即为其分配物理内存，而是在首次访问时，才通过缺页异常陷入内核中分配内存。

- 管理内存的过程中，也很容易发生各种各样的“事故”

  - 没正确回收分配后的内存，导致了泄漏
  - 访问的是已分配内存边界外的地址，导致程序异常退出

- **内存的分配和回收**

  - 栈内存由系统自动分配和管理。一旦程序运行超出了这个局部变量的作用域，栈内存就会被系统自动回收，所以不会产生内存泄漏的问题。
  - 很多时候，我们事先并不知道数据大小，所以你就要用到标准库函数 malloc() __ 在程序中动态分配内存。这时候，系统就会从内存空间的堆中分配内存。
    - 堆内存由应用程序自己来分配和管理。除非程序退出，这些堆内存并不会被系统自动释放，而是需要应用程序明确调用库函数 free() 来释放它们。如果应用程序没有正确释放堆内存，就会造成内存泄漏
  - 内存映射段，包括动态链接库和共享内存，其中共享内存由程序动态分配和管理。所以，如果程序在分配后忘了回收，就会导致跟堆内存类似的泄漏问题
  - 只读段，包括程序的代码和常量，由于是只读的，不会再去分配新的内存，所以也不会产生内存泄漏。
  - 数据段，包括全局变量和静态变量，这些变量在定义时就已经确定了大小，所以也不会产生内存泄漏

- memleak

  - 专门用来检测内存泄漏的工具

  - memleak 可以跟踪系统或指定进程的内存分配、释放请求，然后定期输出一个未释放内存和相应调用栈的汇总情况（默认 5 秒）。

  - > 1 # -a 表示显示每个内存分配请求的大小以及地址 2 # -p 指定案例应用的 PID 号 
    >
    > 3 $ /usr/share/bcc/tools/memleak -a -p $(pidof app)





## Swap升高

- 当发生了内存泄漏时，或者运行了大内存的应用程序，导致系统的内存资源紧张时，会导致两种可能结果，内存回收和 OOM 杀死进程

  - 第一个可能的结果，内存回收，也就是系统释放掉可以回收的内存，比如缓存和缓冲区，就属于可回收内存。它们在内存管理中，通常被叫做文件页（Filebacked Page）
  - 应用程序动态分配的堆内存，也就是我们在内存管理中说到的匿名页是不是也可以回收呢
    - 如果这些内存在分配后很少被访问，似乎也是一种资源浪费。是不是可以把它们暂时先存在磁盘里，释放内存给其他更需要的进程
    - 其实，这正是 Linux 的 Swap 机制。Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了

- Swap 原理

  - Swap 其实是把系统的可用内存变大了。这样，即使服务器的内存不足，也可以运行大内存的应用程序。
  - Swap 说白了就是把一块磁盘空间或者一个本地文件当成内存来使用。它包括换出和换入两个过程。

- Linux 到底在什么时候需要回收内存呢

  - 一个最容易想到的场景就是，有新的大块内存分配请求，但是剩余内存不足。这个时候系统就需要回收一部分内存（比如前面提到的缓存），进而尽可能地满足新内存请求。这个过程通常被称为直接内存回收。
  - 除了直接内存回收，还有一个专门的内核线程用来定期回收内存，也就是kswapd0
    - 为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是页最小阈值（pages_min）、页低阈值（pages_low）和页高阈值（pages_high）。
    - kswapd0 定期扫描内存的使用情况，并根据剩余内存落在这三个阈值的空间位置，进行内存的回收操作。
      - 剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存。
      - 剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了。这时kswapd0 会执行内存回收，直到剩余内存大于页高阈值为止。
      - 剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求。

- NUMA 与 Swap

  - 很多情况下，你明明发现了 Swap 升高，可是在分析系统的内存使用时，却很可能发现，系统剩余内存还多着呢。为什么剩余内存很多的情况下，也会发生 Swap 呢？

  - 这正是处理器的 NUMA 架构导致的。在 NUMA 架构下，多个处理器被划分到不同 Node 上，且每个 Node 都拥有自己的本地内存空间。

  - > 通过 numactl 命令，来查看处理器在 Node 的分布情况，以及每个 Node 的内存使用情况 $ numactl --hardware

- kubernetes关闭swap

  - 一个是性能问题，开启swap会严重影响性能（包括内存和I/O）
  - 另一个是管理问题，开启swap后通过cgroups设置的内存上限就会失效

- 磁盘 I/O 的案例

  - > 1 # 间隔 1 秒输出一组数据 2 # -r 表示显示内存使用情况，-S 表示显示 Swap 使用情况 3 $ sar -r -S 1
    >
    > sar 的输出结果是两个表格，第一个表格表示内存的使用情况，第二个表格表示 Swap 的使用情况

  - 刚开始，剩余内存（kbmemfree）不断减少，而缓冲区（kbbuffers）则不断增大，由此可知，剩余内存不断分配给了缓冲区。

  - 为什么 Swap 也跟着升高了呢？直观来说，缓冲区占了系统绝大部分内存，还属于可回收内存，内存不够用时，不应该先回收缓冲区吗？

    - 这种情况，我们还得进一步通过 /proc/zoneinfo ，观察剩余内存、内存阈值以及匿名页和文件页的活跃情况

    - > -A 表示仅显示 Normal 行以及之后的 15 行输出 $ watch -d grep -A 15 'Normal' /proc/zoneinf

    - 你可以发现，剩余内存（pages_free）在一个小范围内不停地波动。当它小于页低阈值（pages_low) 时，又会突然增大到一个大于页高阈值（pages_high）的值

  - 一段时间后，剩余内存已经很小，而缓冲区占用了大部分内存。这时候，Swap 的使用开始逐渐增大，缓冲区和剩余内存则只在小范围内波动。

    - 当剩余内存小于页低阈值时，系统会回收一些缓存和匿名内存，使剩余内存增大。其中，缓存的回收导致 sar 中的缓冲区减小，而匿名内存的回收导致了 Swap 的使用增大。
    - 紧接着，由于 dd 还在继续，剩余内存又会重新分配给缓存，导致剩余内存减少，缓冲区增大。

- 用smem --sort swap命令可以直接将进程按照swap使用量排序显示

- 关闭 Swap	

  - swapoff -a

- 关闭 Swap 后再重新打开，也是一种常用的 Swap 空间清理方法	

  - swapoff -a && swapon -a

- 在内存资源紧张时，Linux 会通过 Swap ，把不常访问的匿名页换出到磁盘中，下次访问的时候再从磁盘换入到内存中来

- 当 Swap 变高时，你可以用 sar、/proc/zoneinfo、/proc/pid/status 等方法，查看系统和进程的内存使用情况，进而找出 Swap 升高的根源和受影响的进程。

- 反过来说，通常，降低 Swap 的使用，可以提高系统的整体性能





## 如何找到系统内存的问题

- 内存**性能指标**

  - 首先，你最容易想到的是**系统内存使用情况**，比如已用内存、剩余内存、共享内存、可用内存、缓存和缓冲区的用量等。
    - 已用内存和剩余内存很容易理解，就是已经使用和还未使用的内存。 
    - 共享内存是通过 tmpfs 实现的，所以它的大小也就是 tmpfs 使用的内存大小。tmpfs 其实也是一种特殊的缓存。 
    - 可用内存是新进程可以使用的最大内存，它包括剩余内存和可回收缓存。 
    - 缓存包括两部分，一部分是磁盘读取文件的页缓存，用来缓存从磁盘读取的数据，可以加快以后再次访问的速度。另一部分，则是 Slab 分配器中的可回收内存。 
    - 缓冲区是对原始磁盘块的临时存储，用来缓存将要写入磁盘的数据。这样，内核就可以把分散的写集中起来，统一优化磁盘写入。
  - 第二类很容易想到的，应该是**进程内存使用情况**，比如进程的虚拟内存、常驻内存、共享内存以及 Swap 内存等。
    - 虚拟内存，包括了进程代码段、数据段、共享内存、已经申请的堆内存和已经换出的内存等。这里要注意，已经申请的内存，即使还没有分配物理内存，也算作虚拟内存。 
    - 常驻内存是进程实际使用的物理内存，不过，它不包括 Swap 和共享内存。
    - 共享内存，既包括与其他进程共同使用的真实的共享内存，还包括了加载的动态链接库以及程序的代码段等。 
    - Swap 内存，是指通过 Swap 换出到磁盘的内存。 
  - 除了这些很容易想到的指标外，我还想再强调一下，缺页异常。
    - 系统调用内存分配请求后，并不会立刻为其分配物理内存，而是在请求首次访问时，通过缺页异常来分配
    - 缺页异常又分为下面两种场景
      - 可以直接从物理内存中分配时，被称为次缺页异常。
      - 需要磁盘 I/O 介入（比如 Swap）时，被称为主缺页异常		
      - 显然，主缺页异常升高，就意味着需要磁盘 I/O，那么内存访问也会慢很多。
  - 除了系统内存和进程内存，第三类重要指标就是 **Swap** 的使用情况，比如 Swap 的已用空间、剩余空间、换入速度和换出速度等

- **内存性能工具**

  - 所有的案例中都用到了 free。这是个最常用的内存工具，可以查看系统的整体内存和 Swap 使用情况
  - 通过 proc 文件系统，找到了内存指标的来源；并通过 vmstat，动态观察了内存的变化情况。与 free 相比，vmstat 除了可以动态查看内存变化，还可以区分缓存和缓冲区、Swap 换入和换出的内存大小。
  - 为了弄清楚缓存的命中情况，我们又用了 cachestat，查看整个系统缓存的读写命中情况，并用 cachetop 来观察每个进程缓存的读写命中情况
  - 我们用 vmstat，发现了内存使用在不断增长，又用memleak，确认发生了内存泄漏。通过 memleak 给出的内存分配栈，我们找到了内存泄漏的可疑位置。
  - 用 sar 发现了缓冲区和 Swap 升高的问题。通过cachetop，我们找到了缓冲区升高的根源；通过对比剩余内存跟 /proc/zoneinfo 的内存阈，我们发现 Swap 升高是内存回收导致的。案例最后，我们还通过 /proc 文件系统，找出了 Swap 所影响的进程。

- **内存性能工具**

  - ![img](https://upload-images.jianshu.io/upload_images/315466-0c91bdfb4819a104.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp)
  - ![img](https://upload-images.jianshu.io/upload_images/315466-3a25544b244b3f1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp)

  

- 如何迅速分析内存的**性能瓶颈**

  - 为了迅速定位内存问题，我通常会先运行几个覆盖面比较大的性能工具，比如free、top、vmstat、pidstat 等。
    - 具体的分析思路主要有这几步。 
      1. 先用 free 和 top，查看系统整体的内存使用情况。 
      2. 再用 vmstat 和 pidstat，查看一段时间的趋势，从而判断出内存问题的类型。 
      3. 最后进行详细分析，比如内存分配分析、缓存 / 缓冲区分析、具体进程的内存使用分析等。 
    - ![img](https://upload-images.jianshu.io/upload_images/315466-ae55df93100f4fb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp)
  - 当你通过 free，发现大部分内存都被缓存占用后，可以使用 vmstat 或者 sar观察一下缓存的变化趋势，确认缓存的使用是否还在继续增大
    - 如果继续增大，则说明导致缓存升高的进程还在运行，那你就能用缓存 / 缓冲区分析工具（比如 cachetop、slabtop 等），分析这些缓存到底被哪里占用。
  - 当你 free 一下，发现系统可用内存不足时，首先要确认内存是否被缓存 / 缓冲区占用。
    - 排除缓存 / 缓冲区后，你可以继续用 pidstat 或者 top，定位占用内存最多的进程
    - 找出进程后，再通过进程内存空间工具（比如 pmap），分析进程地址空间中内存的使用情况就可以了。
  - 当你通过 vmstat 或者 sar 发现内存在不断增长后，可以分析中是否存在内存泄漏的问题。
    - 比如你可以使用内存分配分析工具 memleak ，检查是否存在内存泄漏。如果存在内存泄漏问题，memleak 会为你输出内存泄漏的进程以及调用堆栈

- 在我看来，内存调优最重要的就是，保证应用程序的热点数据放到内存中，并尽量减少换页和交换。 常见的优化思路有这么几种

  1. **最好禁止 Swap**。如果必须开启 Swap，降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。
  2. **减少内存的动态分配**。比如，可以使用内存池、大页（HugePage）等。
  3. **尽量使用缓存和缓冲区来访问数据**。比如，可以使用堆栈明确声明内存空间，来存储需要缓存的数据；或者用 Redis 这类的外部缓存组件，优化数据的访问。
  4. **使用 cgroups 等方式限制进程的内存使用情况**。这样，可以确保系统内存不会被异常进程耗尽。
  5. **通过 /proc/pid/oom_adj ，调整核心应用的 oom_score**。这样，可以保证即使内存紧张，核心应用也不会被 OOM 杀死

  





## **怎么评估系统的网络性能**

- **性能指标回顾**

  - 首先，**带宽**，表示链路的最大传输速率，单位是 b/s（比特 / 秒）。在你为服务器选购网卡 时，带宽就是最核心的参考指标。常用的带宽有 1000M、10G、40G、100G 等。 
  - 第二，**吞吐量**，表示没有丢包时的最大数据传输速率，单位通常为 b/s （比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽的限制，吞吐量 / 带宽也就是该网络链路的使用率。 
  - 第三，**延时**，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。这个指标 在不同场景中可能会有不同的含义。它可以表示建立连接需要的时间（比如 TCP 握手延时），或者一个数据包往返所需时间（比如 RTT）。 
  - 最后，**PPS**，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速 率。PPS 通常用来评估网络的转发能力，而基于 Linux 服务器的转发，很容易受到网络包 大小的影响（交换机通常不会受到太大影响，即交换机可以线性转发）。 

- **网络基准测试**

  - 基于 HTTP 或者 HTTPS 的 Web 应用程序，显然属于应用层，需要我们测试 HTTP/HTTPS 的性能；
  - 而对大多数游戏服务器来说，为了支持更大的同时在线人数，通常会基于 TCP 或 UDP ，与客户端进行交互，这时就需要我们测试 TCP/UDP 的性能

- **各协议层的性能测试**

  - 转发性能

    - hping3 作为一个 SYN 攻击的工具来使用

    - Linux 内核自带的高性能网络测试工具 pktgen。 pktgen 支持丰富的自定义选项，方便你根据实际需要构造所需网络包，从而更准确地测试 出目标服务器的性能。

    - 不过，在 Linux 系统中，你并不能直接找到 pktgen 命令。因为 pktgen 作为一个内核线 程来运行，需要你加载 pktgen 内核模块后，再通过 /proc 文件系统来交互。

    - ```shell
      1 $ modprobe pktgen
      2 $ ps -ef | grep pktgen | grep -v grep
      
      5 $ ls /proc/net/pktgen/
      
      
      ```

    - pktgen 在每个 CPU 上启动一个内核线程，并可以通过 /proc/net/pktgen 下面的同名文 件，跟这些线程交互；而 pgctrl 则主要用来控制这次测试的开启和停止。

  - $ cat /proc/net/pktgen/eth0

    - 你可以看到，测试报告主要分为三个部分： 
      - 第一部分的 Params 是测试选项； 
      - 第二部分的 Current 是测试进度，其中， packts so far（pkts-sofar）表示已经发送了 100 万个包，也就表明测试已完成。 
      - 第三部分的 Result 是测试结果，包含测试所用时间、网络包数量和分片、PPS、吞吐量 以及错误数。
      - PPS 为 12 万
        - 作为对比，你可以计算一下千兆交换机的 PPS。交换机可以达到线速（满负载时，无差错 转发），它的 PPS 就是 1000Mbit 除以以太网帧的大小，即 1000Mbps/((64+20)*8bit) = 1.5 Mpps（其中，20B 为以太网帧前导和帧间距的大小）。 你看，即使是千兆交换机的 PPS，也可以达到 150 万 PPS，比我们测试得到的 12 万大多 了。

  - TCP/UDP 性能

    - 相应的测试工具，比如 iperf 或者 netperf

      - 特别是现在的云计算时代，在你刚拿到一批虚拟机时，首先要做的，应该就是用 iperf ，测 试一下网络性能是否符合预期。 iperf 和 netperf 都是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量。它们都以 客户端和服务器通信的方式，测试一段时间内的平均吞吐量。

    - ```shell
      1 # -s 表示启动服务端，-i 表示汇报间隔，-p 表示监听端口
      2 $ iperf3 -s -i 1 -p 10000
      ```

    - ```shell
      # 接着，在另一台机器上运行 iperf 客户端，运行测试
      1 # -c 表示启动客户端，192.168.0.30 为目标服务器的 IP
      2 # -b 表示目标带宽 (单位是 bits/s)
      3 # -t 表示测试时间
      4 # -P 表示并发数，-p 表示目标服务器监听端口
      5 $ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000
      ```

    - 稍等一会儿（15 秒）测试结束后，回到目标服务器，查看 iperf 的报告

      - 最后的 SUM 行就是测试的汇总结果，包括测试时间、数据传输量以及带宽等。按照发送和 接收，这一部分又分为了 sender 和 receiver 两行。 从测试结果你可以看到，这台机器 TCP 接收的带宽（吞吐量）为 860 Mb/s， 跟目标的 1Gb/s 相比，还是有些差距的。 

  - HTTP 性能

    - 从传输层再往上，到了应用层。有的应用程序，会直接基于 TCP 或 UDP 构建服务。当 然，也有大量的应用，基于应用层的协议来构建服务，HTTP 就是最常用的一个应用层协 议。比如，常用的 Apache、Nginx 等各种 Web 服务，都是基于 HTTP。

    - 要测试 HTTP 的性能，也有大量的工具可以使用，比如 ab、webbench 等，都是常用的 HTTP 压力测试工具。其中，ab 是 Apache 自带的 HTTP 压测工具，主要测试 HTTP 服务 的每秒请求数、请求延迟、吞吐量以及请求延迟的分布情况等。

      - ```shell
        1 # -c 表示并发请求数为 1000，-n 表示总的请求数为 10000
        2 $ ab -c 1000 -n 10000 http://192.168.0.30/
        ```

      - 可以看到，ab 的测试结果分为三个部分，分别是请求汇总、连接时间汇总还有请求延迟汇 总。以上面的结果为例，我们具体来看。

        - **Requests per second** 为 1074； 

          每个请求的延迟（Time per request）分为两行，第一行的 927 ms 表示平均延迟，包 括了线程运行的调度时间和网络请求响应时间，而下一行的 0.927ms ，则表示实际请求 的响应时间； 

        - **Transfer rate 表示吞吐量（BPS）**为 890 KB/s。 

          连接时间汇总部分，则是分别展示了建立连接、请求、等待以及汇总等的各类时间，包括最 小、最大、平均以及中值处理时间。 

        - 最后的**请求延迟汇总部分**，则给出了不同时间段内处理请求的百分比，比如， 90% 的请 求，都可以在 274ms 内完成。

  - 应用负载性能

    - 当你用 iperf 或者 ab 等测试工具，得到 TCP、HTTP 等的性能数据后，这些数据是否就能 表示应用程序的实际性能呢？我想，你的答案应该是否定的。

      - 比如，你的应用程序基于 HTTP 协议，为最终用户提供一个 Web 服务。这时，使用 ab 工 具，可以得到某个页面的访问性能，但这个结果跟用户的实际请求，很可能不一致。因为用户请求往往会附带着各种各种的负载（payload），而这些负载会影响 Web 应用程序内部 的处理逻辑，从而影响最终性能。

    - 幸运的是，我们还可以用 wrk、TCPCopy、Jmeter 或 者 LoadRunner 等实现这个目标。

      - wrk 的命令行参数比较简单。比如，我们可以用 wrk ，来重新测一下前面已经启动的 Nginx 的性能。

        - ```shell
          1 # -c 表示并发连接数 1000，-t 表示线程数为 2
          2 $ wrk -c 1000 -t 2 http://192.168.0.30/
          ```

        - 你可以看到，每秒请求 数为 9641，吞吐量为 7.82MB，平均延迟为 65ms，比前面 ab 的测试结果要好很多。

        



## 网络性能优化的几个思路

- **确定优化目标**

  - 实际上，虽然网络性能优化的整体目标，是降低网络延迟（如 RTT）和提高吞吐量（如 BPS 和 PPS），但具体到不同应用中，每个指标的优化标准可能会不同，优先级顺序也大 相径庭。 
    - NAT 网关来说，由于其直接影响整个数据中心的网络出入性能，所以 NAT 网关通常需要达到或接近线性转发，也就是说， PPS 是最主要的性能目标。 
    - 再如，对于数据库、缓存等系统，快速完成网络收发，即低延迟，是主要的性能目标。
    - 而对于我们经常访问的 Web 服务来说，则需要同时兼顾

- Linux 网络协议栈

  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105407329-759102076.png)
  - **首先是网络接口层和网络层**，它们主要负责网络包的封装、寻址、路由，以及发送和接收。 每秒可处理的网络包数 PPS，就是它们最重要的性能指标（特别是在小包的情况下）。你可以用内核自带的发包工具 pktgen ，来测试 PPS 的性能。
  - **再向上到传输层的 TCP 和 UDP**，它们主要负责网络传输。对它们而言，吞吐量（BPS）、 连接数以及延迟，就是最重要的性能指标。你可以用 iperf 或 netperf ，来测试传输层的性 能。 
    - 不过要注意，网络包的大小，会直接影响这些指标的值。所以，通常，你需要测试一系列不 同大小网络包的性能。
  - 最后，再往上到了应用层，最需要关注的是吞吐量（BPS）、每秒请求数以及延迟等指标。 你可以用 wrk、ab 等工具，来测试应用程序的性能

- 网络性能工具

  - 网络性能指标的工具
    - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105438150-732021018.png)
    - 性能工具
      - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190920105457114-2056652101.png)

- 应用程序

  - 从网络 I/O 的角度来说，主要有下面两种优化思路

    - 从网络 I/O 的角度来说，主要有下面两种优化思路。 第一种是最常用的 I/O 多路复用技术 epoll，主要用来取代 select 和 poll。这其实是解决 C10K 问题的关键，也是目前很多网络应用默认使用的机制。
    - 第二种是使用异步 I/O（Asynchronous I/O，AIO）。AIO 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。等到 I/O 完成后，系统会用事件通知的方式，告诉应 用程序结果。不过，AIO 的使用比较复杂，你需要小心处理很多边缘情况

  - 而从进程的工作模型来说，也有两种不同的模型用来优化

    - 第一种，主进程 + 多个 worker 子进程。其中，主进程负责管理网络连接，而子进程负责 实际的业务处理。这也是最常用的一种模型。 

      第二种，监听到相同端口的多进程模型。在这种模型下，所有进程都会监听相同接口，并且 开启 SO_REUSEPORT 选项，由内核负责，把请求负载均衡到这些监听进程中去。 

  - 应用层的网络协议优化

    - 使用长连接取代短连接，可以显著降低 TCP 建立连接的成本。在每秒请求次数较多时， 这样做的效果非常明显。 
    - 使用内存等方式，来缓存不常变化的数据，可以降低网络 I/O 次数，同时加快应用程序 的响应速度。 
    - 使用 Protocol Buffer 等序列化的方式，压缩网络 I/O 的数据量，可以提高应用程序的吞 吐。 
    - 使用 DNS 缓存、预取、HTTPDNS 等方式，减少 DNS 解析的延迟，也可以提升网络 I/O 的整体速度

- 套接字

  - 套接字可以屏蔽掉 Linux 内核中不同协议的差异，为应用程序提供统一的访问接口。每个 套接字，都有一个读写缓冲区。 

    - 读缓冲区，缓存了远端发过来的数据。如果读缓冲区已满，就不能再接收新的数据。 
    - 写缓冲区，缓存了要发出去的数据。如果写缓冲区已满，应用程序的写操作就会被阻塞。

  - > 增大每个套接字的缓冲区大小 net.core.optmem_max； 
    >
    > 增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 
    >
    > net.core.wmem_max； 
    >
    > 增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 
    >
    > net.ipv4.tcp_wmem。
    >
    > 
    >
    > 发送缓冲区大小，理想数值是吞吐量 * 延迟，这样才可以达到最大网络利用 率

- 传输层

  - 第一类，在请求数比较大的场景下，你可能会看到大量处于 TIME_WAIT 状态的连接，它 们会占用大量内存和端口资源。这时，我们可以优化与 TIME_WAIT 状态相关的内核选 项，比如采取下面几种措施。
    - 增大处于 TIME_WAIT 状态的连接数量 net.ipv4.tcp_max_tw_buckets ，并增大连接跟踪表的大小 net.netfilter.nf_conntrack_max。 
    - 减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源。 
    - 开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用 到新建的连接中。 
    - 增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整 体的并发能力。 
    - 增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和 
    - 系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数。
  - 第二类，为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，你可 以考虑优化与 SYN 状态相关的内核选项，比如采取下面几种措施。 
    - 增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项 不可同时使用）。 
    - 减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries。 
  - 第三类，在长连接的场景中，通常使用 Keepalive 来检测 TCP 连接的状态，以便对端连接 断开后，可以自动回收。但是，系统默认的 Keepalive 探测间隔和重试次数，一般都无法 满足应用程序的性能要求。所以，这时候你需要优化与 Keepalive 相关的内核选项，比 如
    - 缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time；缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl； 
    - 减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数net.ipv4.tcp_keepalive_probes。 
  - UDP 的优化
    - UDP 提供了面向数据报的网络协议，它不需要网络连接，也不提供可靠性保障。所以， UDP 优化，相对于 TCP 来说，要简单得多。这里我也总结了常见的几种优化方案。 
      - 跟上篇套接字部分提到的一样，增大套接字缓冲区大小以及 UDP 缓冲区范围； 
      - 跟前面 TCP 部分提到的一样，增大本地端口号的范围； 
      - 根据 MTU 大小，调整 UDP 数据包的大小，减少或者避免分片的发生。

- 网络层

  - 网络层，负责网络包的封装、寻址和路由，包括 IP、ICMP 等常见协议。在网络层，最主 要的优化，其实就是对路由、 IP 分片以及 ICMP 等进行调优。 
  - 第一种，从路由和转发的角度出发，你可以调整下面的内核选项。 
    - 在需要转发的服务器中，比如用作 NAT 网关的服务器或者使用 Docker 容器时，开启 IP 转发，即设置 net.ipv4.ip_forward = 1。 
    - 调整数据包的生存周期 TTL，比如设置 net.ipv4.ip_default_ttl = 64。注意，增大该值会 降低系统性能。 
    - 开启数据包的反向地址校验，比如设置 net.ipv4.conf.eth0.rp_filter = 1。这样可以防止 IP 欺骗，并减少伪造 IP 带来的 DDoS 问题。
  - 第二种，从分片的角度出发，最主要的是调整 MTU（Maximum Transmission Unit）的 大小。 
    - 通常，MTU 的大小应该根据以太网的标准来设置。以太网标准规定，一个网络帧最大为 1518B，那么去掉以太网头部的 18B 后，剩余的 1500 就是以太网 MTU 的大小。在使用 VXLAN、GRE 等叠加网络技术时，要注意，网络叠加会使原来的网络包变大，导致 MTU 也需要调整。 
    - 比如，就以 VXLAN 为例，它在原来报文的基础上，增加了 14B 的以太网头部、 8B 的 VXLAN 头部、8B 的 UDP 头部以及 20B 的 IP 头部。换句话说，每个包比原来增大了 50B。 
    - 所以，我们就需要把交换机、路由器等的 MTU，增大到 1550， 或者把 VXLAN 封包前 （比如虚拟化环境中的虚拟网卡）的 MTU 减小为 1450。
  - 第三种，从 ICMP 的角度出发，为了避免 ICMP 主机探测、ICMP Flood 等各种网络问 题，你可以通过内核选项，来限制 ICMP 的行为。 
    - 比如，你可以禁止 ICMP 协议，即设置 net.ipv4.icmp_echo_ignore_all = 1。这样，外 部主机就无法通过 ICMP 来探测主机。 
    - 或者，你还可以禁止广播 ICMP，即设置 net.ipv4.icmp_echo_ignore_broadcasts = 1。

## 服务吞吐量下降很厉害，怎么分析

- 测试一下，案例中 Nginx 的吞吐量。

  - ```shell
    1 # 默认测试时间为 10s，请求超时 2s
    2 $ wrk --latency -c 1000 http://192.168.0.30
    
     1910 requests in 10.10s, 573.56KB read
     Non-2xx or 3xx responses: 1910
    ```

  - 从 wrk 的结果中，你可以看到吞吐量（也就是每秒请求数）只有 189，并且所有 1910 个 请求收到的都是异常响应（非 2xx 或 3xx）。这些数据显然表明，吞吐量太低了，并且请 求处理都失败了。这是怎么回事呢？

  - 根据 wrk 输出的统计结果，我们可以看到，总共传输的数据量只有 573 KB，那就肯定不会 是带宽受限导致的。所以，我们应该从请求数的角度来分析。 分析请求数，特别是 HTTP 的请求数，有什么好思路吗？当然就要**从 TCP 连接数入手**。

- 连接数优化

  - 要查看 TCP 连接数的汇总情况，首选工具自然是 ss 命令。为了观察 wrk 测试时发生的问 题，我们在终端二中再次启动 wrk，并且把总的测试时间延长到 30 分钟

    - ```shell
      # 测试时间 30 分钟
      $ wrk --latency -c 1000 -d 1800 http://192.168.0.30
      
      #观察 TCP 连接数
      $ ss -s
      ```

    - 从这里看出，wrk 并发 1000 请求时，建立连接数只有 5，而 closed 和 timewait 状态的 连接则有 1100 多 。其实从这儿你就可以发现两个问题

      - 一个是建立连接数太少了； 
      - 另一个是 timewait 状态连接太多了

    - 分析问题，自然要先从相对简单的下手。我们先来看第二个关于 timewait 的问题。在之前 的 NAT 案例中，我已经提到过，内核中的连接跟踪模块，有可能会导致 timewait 问题。 我们今天的案例还是基于 Docker 运行，而 Docker 使用的 iptables ，就会使用**连接跟踪 模块**来管理 NAT。那么，怎么确认是不是连接跟踪导致的问题呢？

      - 其实，最简单的方法，就是通过 dmesg 查看系统日志，如果有连接跟踪出了问题，应该会 看到 nf_conntrack 相关的日志

        - $ dmesg | tail

        - 从日志中，你可以看到 nf_conntrack: table full, dropping packet 的错误日志。这说明， 正是连接跟踪导致的问题。 

        - 这种情况下，我们应该想起前面学过的两个内核选项——连接跟踪数的最大限制 nf_conntrack_max ，以及当前的连接跟踪数 nf_conntrack_count。

        - 这次的输出中，你可以看到最大的连接跟踪限制只有 200，并且全部被占用了。200 的限 制显然太小，不过相应的优化也很简单，调大就可以了。

        - ```shell
          # 我们执行下面的命令，将 nf_conntrack_max 增大：
          # 将连接跟踪限制增大到 1048576
           $ sysctl -w net.netfilter.nf_conntrack_max=1048576
          ```

      - 从 wrk 的输出中，你可以看到，连接跟踪的优化效果非常好，吞吐量已经从刚才的 189 增 大到了 5382。看起来性能提升了将近 30 倍

        - 别急，我们再来看看 wrk 汇报的其他数据。果然，10s 内的总请求数虽然增大到了 5 万， 但是有 4 万多响应异常，说白了，真正成功的只有 8000 多个（54221-45577=8644）

- 工作进程优化

  - 由于这些响应并非 Socket error，说明 Nginx 已经收到了请求，只不过，响应的状态码并 不是我们期望的 2xx （表示成功）或 3xx（表示重定向）。所以，这种情况下，搞清楚 Nginx 真正的响应就很重要了。 

    - ```shell
      $ docker logs nginx --tail 3
      
      ```

    - 从 Nginx 的日志中，我们可以看到，响应状态码为 499。499 并非标准的 HTTP 状态码，而是由 Nginx 扩展而来，表示服务器端还没来得及响应 时，客户端就已经关闭连接了。换句话说，问题在于服务器端处理太慢，客户端因为超时 （wrk 超时时间为 2s），主动断开了连接。

    - 既然问题出在了服务器端处理慢，而案例本身是 Nginx+PHP 的应用，那是不是可以猜 测，是因为 PHP 处理过慢呢

    - ```shell
      # 查询 PHP 容器日志
      $ docker logs phpfpm --tail 5
      
      ```

    - 从这个日志中，我们可以看到两条警告信息，server reached max_children setting (5)， 并建议增大 max_children。

      - 一般来说，每个 php-fpm 子进程可能会占用 20 MB 左右的内存。所以，你可以根据内存 和 CPU 个数，估算一个合理的值。这儿我把它设置成了 20，并将优化后的配置重新打包 成了 Docker 镜像。

      - ```shell
        # 停止旧的容器
        $ docker rm -f nginx phpfpm
        # 使用新镜像启动 Nginx 和 PHP
        $ docker run --name nginx --network host --privileged -itd feisky/nginx-tp:1
        $ docker run --name phpfpm --network host --privileged -itd feisky/php-fpm-tp:1
        
        ```

    - 从 wrk 的输出中，可以看到，虽然吞吐量只有 4683，比刚才的 5382 少了一些；但是测试 期间成功的请求数却多了不少，从原来的 8000，增长到了 15000（47210- 31692=15518）。 

- 套接字优化

  - 观察有没有发生套接字的丢包现象

    - ```shell
      1 # 只关注套接字统计
      2 $ netstat -s | grep socket
      
      ```

    - 根据两次统计结果中 socket overflowed 和 sockets dropped 的变化，你可以看到，有大 量的套接字丢包，并且丢包都是套接字队列溢出导致的。所以，接下来，我们应该分析连接 队列的大小是不是有异常。

  - 查看套接字的队列大小$ ss -ltnp

    - 这次可以看到，Nginx 和 php-fpm 的监听队列 （Send-Q）只有 10，而 nginx 的当前监 听队列长度 （Recv-Q）已经达到了最大值，php-fpm 也已经接近了最大值。很明显，套 接字监听队列的长度太小了，需要增大

  - 关于套接字监听队列长度的设置，既可以在应用程序中，通过套接字接口调整，也支持通过 内核选项来配置

    - ```shell
      1 # 查询 nginx 监听队列长度配置
      2 $ docker exec nginx cat /etc/nginx/nginx.conf | grep backlog
      3 listen 80 backlog=10;
      45 # 查询 php-fpm 监听队列长度
      6 $ docker exec phpfpm cat /opt/bitnami/php/etc/php-fpm.d/www.conf | grep backlog
      7 ; Set listen(2) backlog.
      8 ;listen.backlog = 511
      9
      10 # somaxconn 是系统级套接字监听队列上限
      11 $ sysctl net.core.somaxconn
      12 net.core.somaxconn = 10
      
      ```

    - 从输出中可以看到，Nginx 和 somaxconn 的配置都是 10，而 php-fpm 的配置也只有 511，显然都太小了。那么，优化方法就是增大这三个配置，比如，可以把 Nginx 和 phpfpm 的队列长度增大到 8192，而把 somaxconn 增大到 65536

    - 现在的吞吐量已经增大到了 6185，并且在测试的时候，如果你在终端一中重新执行 netstat -s | grep socket，还会发现，现在已经没有套接字丢包问题了

  - 不过，这次 Nginx 的响应，再一次全部失败了，都是 Non-2xx or 3xx

    - 你可以看到，Nginx 报出了无法连接 fastcgi 的错误，错误消息是 Connect 时， Cannot assign requested address。这个错误消息对应的错误代码为 EADDRNOTAVAIL，表示 IP 地址或者端口号不可用。 
    - 在这里，显然只能是端口号的问题

- 端口号优化

  - 我们执行下面的命令，就可以查询系统配置的临时端口号范围

    - ```shell
      1 $ sysctl net.ipv4.ip_local_port_range
      2 net.ipv4.ip_local_port_range=20000 20050
      
      ```

  - 你可以看到，临时端口的范围只有 50 个，显然太小了 。优化方法很容易想到，增大这个 范围就可以了。比如，你可以执行下面的命令，把端口号范围扩展为 “10000 65535”：

    - ```shell
      1 $ sysctl -w net.ipv4.ip_local_port_range="10000 65535"
      2 net.ipv4.ip_local_port_range = 10000 65535
      
      ```

  - 这次，异常的响应少多了 ，不过，吞吐量也下降到了 3208。并且，这次还出现了很多 Socket read errors。显然，还得进一步优化。

- 火焰图

  - 前面我们已经优化了很多配置。这些配置在优化网络的同时，却也会带来其他资源使用的上 升

  - 执行 top ，观察 CPU 和内存的使用

  - 从 top 的结果中可以看到，可用内存还是很充足的，但系统 CPU 使用率（sy）比较高，两 个 CPU 的系统 CPU 使用率都接近 50%，且空闲 CPU 使用率只有 2%。再看进程部分， CPU 主要被两个 Nginx 进程和两个 docker 相关的进程占用，使用率都是 30% 左右。

    - CPU 使用率上升了，该怎么进行分析呢？我想，你已经还记得我们多次用到的 perf，再配 合前两节讲过的火焰图，很容易就能找到系统中的热点函数。 

    - ```shell
      1 # 执行 perf 记录事件
      2 $ perf record -g
      34 # 切换到 FlameGraph 安装路径执行下面的命令生成火焰图
      
      ```

  - 根据我们讲过的火焰图原理，这个图应该从下往上、沿着调用栈中最宽的函数，来分析执行 次数最多的函数。 

  - 如果有大量连接 占用着端口，那么检查端口号可用的函数，不就会消耗更多的 CPU 吗

    - $ ss -s

    - 这回可以看到，有大量连接（这儿是 32768）处于 timewait 状态，而 timewait 状态的连 接，本身会继续占用端口号。如果这些端口号可以重用，那么自然就可以缩短 __init_check_established 的过程。而 Linux 内核中，恰好有一个 tcp_tw_reuse 选项，用 来控制端口号的重用。

      ```shell
      # 我们在终端一中，运行下面的命令，查询它的配置：
      
      1 $ sysctl net.ipv4.tcp_tw_reuse
      2 net.ipv4.tcp_tw_reuse = 0
      # 你可以看到，tcp_tw_reuse 是 0，也就是禁止状态。其实看到这里，我们就能理解，为什么临时端口号的分配会是系统运行的热点了。当然，优化方法也很容易，把它设置成 1 就可以开启了。
      
      ```

      



## **综合：分析性能问题的一般步骤**

- **系统资源瓶颈**
  - 使用率、饱和度以及错误数这三类指标来衡量。系统的资源，可以分为硬件资源和软件资源两 类。
    - 如 CPU、内存、磁盘和文件系统以及网络等，都是最常见的硬件资源。 
    - 而文件描述符数、连接跟踪数、套接字缓冲区大小等，则是典型的软件资源。
- **CPU** **性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925163759359-2129947612.png)
  - 比如说，当你收到系统的用户 CPU 使用率过高告警时，从监控系统中直接查询到，导致 CPU 使用率过高的进程；然后再登录到进程所在的 Linux 服务器中，分析该进程的行为。 
  - 你可以使用 strace，查看进程的系统调用汇总；也可以使用 perf 等工具，找出进程的热点 函数；甚至还可以使用动态追踪的方法，来观察进程的当前执行过程，直到确定瓶颈的根 源。
- **内存性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925164214302-575822803.png)
  - 比如说，当你收到内存不足的告警时，首先可以从监控系统中。找出占用内存最多的几个进 程。然后，再根据这些进程的内存占用历史，观察是否存在内存泄漏问题。
  - 确定出最可疑的 进程后，再登录到进程所在的 Linux 服务器中，分析该进程的内存空间或者内存分配，最 后弄清楚进程为什么会占用大量内存
- **磁盘和文件系统** **I/O** **性能分析**
  - ![img](https://img2018.cnblogs.com/blog/1075436/201909/1075436-20190925164238972-714799998.png)
  - 当你使用 iostat ，发现磁盘 I/O 存在性能瓶颈（比如 I/O 使用率过 高、响应时间过长或者等待队列长度突然增大等）后，可以再通过 pidstat、 vmstat 等， 确认 I/O 的来源。接着，再根据来源的不同，进一步分析文件系统和磁盘的使用率、缓存 以及进程的 I/O 等，从而揪出 I/O 问题的真凶
  - 比如说，当你发现某块磁盘的 I/O 使用率为 100% 时，首先可以从监控系统中，找出 I/O 最多的进程。然后，再登录到进程所在的 Linux 服务器中，借助 strace、lsof、perf 等工具，分析该进程的 I/O 行为。最后，再结合应用程序的原理，找出大量 I/O 的原因。 
- **网络性能分析**
  - 最后的网络性能，其实包含两类资源，即网络接口和内核资源
  - 而要分析网络的性能，自然也是要从这几个协议层入手，通过使用率、饱和度以及错误数这 几类性能指标，观察是否存在性能问题。比如 ： 
    - 在链路层，可以从网络接口的吞吐量、丢包、错误以及软中断和网络功能卸载等角度分 析； 
    - 在网络层，可以从路由、分片、叠加网络等角度进行分析； 
    - 在传输层，可以从 TCP、UDP 的协议原理出发，从连接数、吞吐量、延迟、重传等角度 进行分析；在应用层，可以从应用层协议（如 HTTP 和 DNS）、请求数（QPS）、套接字缓存等角 度进行分析。 
  - 比如，当你收到网络不通的告警时，就可以从监控系统中，查找各个协议层的丢包指标，确 认丢包所在的协议层。然后，从监控系统的数据中，确认网络带宽、缓冲区、连接跟踪数等 软硬件，是否存在性能瓶颈。最后，再登录到发生问题的 Linux 服务器中，借助 netstat、 tcpdump、bcc 等工具，分析网络的收发数据，并且结合内核中的网络选项以及 TCP 等网 络协议的原理，找出问题的来源
- **应用程序瓶颈**
  - 除了以上这些来自网络资源的瓶颈外，还有很多瓶颈，其实直接来自应用程序。比如，最典 型的应用程序性能问题，就是吞吐量（并发请求数）下降、错误率升高以及响应时间增大。 
  - 不过，在我看来，这些应用程序性能问题虽然各种各样，但就其本质来源，实际上只有三 种，也就是资源瓶颈、依赖服务瓶颈以及应用自身的瓶颈。 
    - 第一种**资源瓶颈**，其实还是指刚才提到的 CPU、内存、磁盘和文件系统 I/O、网络以及内 核资源等各类软硬件资源出现了瓶颈，从而导致应用程序的运行受限。对于这种情况，我们 就可以用前面系统资源瓶颈模块提到的各种方法来分析。 
    - 第二种**依赖服务的瓶颈**，也就是诸如数据库、分布式缓存、中间件等应用程序，直接或者间 接调用的服务出现了性能问题，从而导致应用程序的响应变慢，或者错误率升高。这说白了 就是跨应用的性能问题，使用全链路跟踪系统，就可以帮你快速定位这类问题的根源。 
    - 最后一种，**应用程序自身的性能问题**，包括了多线程处理不当、死锁、业务算法的复杂度过 高等等。对于这类问题，在我们前面讲过的应用程序指标监控以及日志监控中，观察关键环 节的耗时和内部执行过程中的错误，就可以帮你缩小问题的范围。 
  - 不过，由于这是应用程序内部的状态，外部通常不能直接获取详细的性能数据，所以就需要 应用程序在设计和开发时，就提供出这些指标，以便监控系统可以了解应用程序的内部运行状态。 
  - 如果这些手段过后还是无法找出瓶颈，你还可以用系统资源模块提到的各类进程分析工具， 来进行分析定位。比如： 
    - 你可以用 strace，观察系统调用； 
    - 使用 perf 和火焰图，分析热点函数； 
    - 甚至使用动态追踪技术，来分析进程的执行状态。
  - 当然，系统资源和应用程序本来就是相互影响、相辅相成的一个整体。实际上，很多资源瓶 颈，也是应用程序自身运行导致的。比如，进程的内存泄漏，会导致系统内存不足；进程过 多的 I/O 请求，会拖慢整个系统的 I/O 请求等。



## **综合：优化性能问题的一般方法**

- 我们可以从系统资源瓶颈和应用程序瓶颈，这两个角度来分析性能问题的根源

  - 从系统资源瓶颈的角度来说，USE 法是最为有效的方法，即从使用率、饱和度以及错误数 这三个方面，来分析 CPU、内存、磁盘和文件系统 I/O、网络以及内核资源限制等各类软硬件资源。
  - 从应用程序瓶颈的角度来说，可以把性能问题的来源，分为资源瓶颈、依赖服务瓶颈以及应 用自身的瓶颈这三类。 
  - 资源瓶颈的分析思路，跟系统资源瓶颈是一样的。 
  - 依赖服务的瓶颈，可以使用全链路跟踪系统，进行快速定位。 
  - 而应用自身的问题，则可以通过系统调用、热点函数，或者应用自身的指标和日志等，进 行分析定位。 

- **CPU** **优化**

  - CPU 性能优化的核心，在于排除所有不必要的工作、充分利用 CPU 缓存并减少进程调度对性能的*影响
  - 最典型 的三种优化方法。
  - 第一种，把进程绑定到一个或者多个 CPU 上，充分利用 CPU 缓存的本地性，并减少进 程间的相互影响。 
  - 第二种，为中断处理程序开启多 CPU 负载均衡，以便在发生大量中断时，可以充分利用多 CPU 的优势分摊负载。 
  - 第三种，使用 Cgroups 等方法，为进程设置资源限制，避免个别进程消耗过多的 CPU。 同时，为核心应用程序设置更高的优先级，减少低优先级任务的影响。 

- **内存优化**

  - 内存问题，比如可用内存不足、内存泄漏、 Swap 过多、缺页异常过多以及缓存过多等等。所以，说白了，内存性能的优化，也就是要 解决这些内存使用的问题。
  - 你可以通过以下几种方法
    - 第一种，除非有必要，Swap 应该禁止掉。这样就可以避免 Swap 的额外 I/O ，带来内 存访问变慢的问题。 
    - 第二种，使用 Cgroups 等方法，为进程设置内存限制。这样就可以避免个别进程消耗过多内存，而影响了其他进程。对于核心应用，还应该降低 oom_score，避免被 OOM 杀 死。 
    - 第三种，使用大页、内存池等方法，减少内存的动态分配，从而减少缺页异常。

- **磁盘和文件系统** **I/O** **优化**

  - 三种最典型的方 法。
    - 第一种，也是最简单的方法，通过 SSD 替代 HDD、或者使用 RAID 等方法，提升 I/O性能。 
    - 第二种，针对磁盘和应用程序 I/O 模式的特征，选择最适合的 I/O 调度算法。比如， SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法；而数据库应用，更推荐使用deadline 算法。 
    - 第三，优化文件系统和磁盘的缓存、缓冲区，比如优化脏页的刷新频率、脏页限额，以及 内核回收目录项缓存和索引节点缓存的倾向等等。
  - 除此之外，使用不同磁盘隔离不同应用的数据、优化文件系统的配置选项、优化磁盘预读、 增大磁盘队列长度等，也都是常用的优化思路。

- **网络优化**

  - 针对每个协议层的工作原理 进行优化。这里，我同样强调一下，最典型的几种网络优化方法。
    - 首先，从内核资源和网络协议的角度来说，我们可以对内核选项进行优化，比如： 
      - 你可以增大套接字缓冲区、连接跟踪表、最大半连接数、最大文件描述符数、本地端口范 围等内核资源配额； 
      - 也可以减少 TIMEOUT 超时时间、SYN+ACK 重传数、Keepalive 探测时间等异常处理 参数； 
      - 还可以开启端口复用、反向地址校验，并调整 MTU 大小等降低内核的负担。 
    - 其次，从网络接口的角度来说，我们可以考虑对网络接口的功能进行优化，比如： 
      - 你可以将原来 CPU 上执行的工作，卸载到网卡中执行，即开启网卡的 GRO、GSO、 RSS、VXLAN 等卸载功能； 
      - 也可以开启网络接口的多队列功能，这样，每个队列就可以用不同的中断号，调度到不同 CPU 上执行； 
      - 还可以增大网络接口的缓冲区大小以及队列长度等，提升网络传输的吞吐量。

- **应用程序优化**

  - 第一个例子，是系统 CPU 使用率（sys%）过高的问题。有时候出现问题，虽然表面现象是系统 CPU 使用率过高，但待你分析过后，很可能会发现，应用程序的不合理系统调用才是罪魁祸首。这种情况下，优化应用程序内部系统调用的逻辑，显然要比优化内核要简单也有 用得多。 
  - 再比如说，数据库的 CPU 使用率高、I/O 响应慢，也是最常见的一种性能问题。这种问 题，一般来说，并不是因为数据库本身性能不好，而是应用程序不合理的表结构或者 SQL查询语句导致的。这时候，优化应用程序中数据库表结构的逻辑或者 SQL 语句，显然要比优化数据库本身，能带来更大的收益。
  - 所以，在观察性能指标时，你应该先查看**应用程序的响应时间、吞吐量以及错误率**等指标，因为它们才是性能优化要解决的终极问题。以终为始，从这些角度出发，你一定能想到很多优化方法，而我比较推荐下面几种方法。
    - 第一，从 CPU 使用的角度来说，简化代码、优化算法、异步处理以及编译器优化等，都 是常用的降低 CPU 使用率的方法，这样可以利用有限的 CPU 处理更多的请求。 
    - 第二，从数据访问的角度来说，使用缓存、写时复制、增加 I/O 尺寸等，都是常用的减 少磁盘 I/O 的方法，这样可以获得更快的数据处理速度。
    - 第三，从内存管理的角度来说，使用大页、内存池等方法，可以预先分配内存，减少内存的动态分配，从而更好地内存访问性能。 
    - 第四，从网络的角度来说，使用 I/O 多路复用、长连接代替短连接、DNS 缓存等方法，可以优化网络 I/O 并减少网络请求数，从而减少网络延时带来的性能问题。 
    - 第五，从进程的工作模型来说，异步处理、多线程或多进程等，可以充分利用每一个 CPU 的处理能力，从而提高应用程序的吞吐能力。 
    - 除此之外，你还可以使用消息队列、CDN、负载均衡等各种方法，来优化应用程序的架构，将原来单机要承担的任务，调度到多台服务器中并行处理。这样也往往能获得更好的整体性能

  


